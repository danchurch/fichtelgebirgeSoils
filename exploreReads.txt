## let's take a look at Sulari and Arne's reads

### set up ssh key for this. ###
git remote set-url origin git@github.com:danchurch/fichtelgebirgeSoils.git

## might need to grab files quickly between computers. a template

laptopLoc="/home/daniel/Desktop"
nanoCompLoc="/media/vol1/daniel/sulariArne/illuminaReads/dadaFsSulariOnly.rda"
scp -r -i ~/.ssh/id_ed25519 test@132.180.112.115:$nanoCompLoc $laptopLoc

## or something like that


###########################################

## now, much of this analysis will likely have to be done in R,
## on a computer with sufficient memory.

## the goal would be to contain this analysis to R
## so we'll use dada2 in R

## for the moment, let's use the lab computer,

## let's look in our files for primers, etc:

conda activate 

conda create -n readQC -c bioconda cutadapt

readDir="/media/vol1/daniel/sulariArne/illuminaReads/goodReads"

## let's do this old-school:

cd $readDir

gunzip -k *

cat *fq > allSulariArneReads.fq

mkdir /media/vol1/daniel/sulariArne/illuminaReads/fastqcOut

## set variables
file="/media/vol1/daniel/sulariArne/illuminaReads/goodReads/allSulariArneReads.fq"
outdir="/media/vol1/daniel/sulariArne/illuminaReads/fastqcOut"
fastqc -t 10 -o  $outdir $file  &


path2put=/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/readReport/
path2get="/media/vol1/daniel/sulariArne/illuminaReads/fastqcOut/"
scp -r -i ~/.ssh/id_ed25519 test@132.180.112.115:$path2get $path2put

## quality looks great. 

## split by direction and check again

## on nanoComp

conda activate 

cd /media/vol1/daniel/sulariArne/illuminaReads

readDir="/media/vol1/daniel/sulariArne/illuminaReads/goodReads"

cat $readDir/*good_1.fq > R1_SulariArneReads.fq &
cat $readDir/*good_2.fq > R2_SulariArneReads.fq

r1=/media/vol1/daniel/sulariArne/illuminaReads/R1_SulariArneReads.fq
r2=/media/vol1/daniel/sulariArne/illuminaReads/R2_SulariArneReads.fq
outdir="/media/vol1/daniel/sulariArne/illuminaReads/fastqcOut"
fastqc -t 10 -o $outdir $r1 &
fastqc -t 10 -o $outdir $r2 &

## local machine:
path2put=/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils
path2get="/media/vol1/daniel/sulariArne/illuminaReads/Envt_Matrix.csv"
scp -r -i ~/.ssh/id_ed25519 test@132.180.112.115:$path2get $path2put

firefox *html

## all looks pretty amazing. For each (R1 and R2) We have 15510344/195 = 79540 reads per sample. 
## after pairing. 
## pretty much what they reported. 

## look for primers...the first ten or so basepairs are highly conserved in both
## are these remnants of the primers? 

## back on nanoComp
 
head R1_SulariArneReads.fq

wc -l R1_SulariArneReads.fq

## here are the primers used as reported by the company.
GTGYCAGCMGCCGCGGTAA 

len('GTGYCAGCMGCCGCGGTAA') ##19

GGACTACNVGGGTWTCTAAT

len('GGACTACNVGGGTWTCTAAT') ##20

## these look like the latest earth microbiome, parada primers

## 515 forward, parada, in R1?:
grep -c "^GTG.CAGC.GCCGCGGTAA" R1_SulariArneReads.fq ## 14903512 reads
grep -c GTG.CAGC.GCCGCGGTAA R1_SulariArneReads.fq ## again 14903512 reads, out of 
grep -c "@A01720" R1_SulariArneReads.fq ## 15510344
wc -l R1_SulariArneReads.fq ## 15510344
## 806R, in R1 reads?:
grep -c "GGACTAC..GGGT.TCTAAT" R1_SulariArneReads.fq ## 0 reverse primers, that's good
## reverse complement:
grep -c "ATTAGA.ACCC.NGTAGTCC" R1_SulariArneReads.fq ## 0 reverse RC primers, that's good

## 515 forward, parada, in R2?:
grep -c "^GTG.CAGC.GCCGCGGTAA" R2_SulariArneReads.fq ## 0, good

## 806R, in R1 reads?:
grep -c "GGACTAC..GGGT.TCTAAT" R2_SulariArneReads.fq ## 15170340, out of 15510344
## reverse complement:
grep -c "ATTAGA.ACCC.NGTAGTCC" R2_SulariArneReads.fq ## 0, good.

## this looks like really good data. 

## let's clip these primers, and get on to dada2

## make a directories of uncompressed, separated R1/R2 

readDir="/media/vol1/daniel/sulariArne/illuminaReads/goodReads"
allReadsPrimersClippedDir="/media/vol1/daniel/sulariArne/illuminaReads/goodReads_primerClipped"
#mkdir $allReadsPrimersClippedDir
readsPrimersClippedDir="/media/vol1/daniel/sulariArne/illuminaReads/goodReads_primerClipped"

conda activate readQC

cd $readDir

cd $allReadsPrimersClippedDir

## something like this?

for iR1 in *good_1.fq; do
  iR2=${iR1/_1\.fq/_2\.fq}
  echo $iR1
  outR1=${iR1/1\.fq/1_trimmed\.fq}
  outR2=${iR2/2\.fq/2_trimmed\.fq}
  cutadapt -g GTGYCAGCMGCCGCGGTAA -G GGACTACNVGGGTWTCTAAT -o $outR1 -p $outR2 $iR1 $iR2 
done


## did that work? 

cd /media/vol1/daniel/sulariArne/illuminaReads/uncompressedReads

head -n4 Bacteria_BM663-01M0087_good_1.fq

head -n4 Bacteria_BM663-01M0087_good_1_trimmed.fq

clear
tail -n4 Bacteria_BM663-01M0087_good_1.fq
tail -n4 Bacteria_BM663-01M0087_good_1_trimmed.fq

tail -n4 Bacteria_BM663-01M0087_good_2.fq
tail -n4 Bacteria_BM663-01M0087_good_2_trimmed.fq

grep "^GTG.CAGC.GCCGCGGTAA" Bacteria_BM663-01M0087_good_1.fq ## lots
grep "^GTG.CAGC.GCCGCGGTAA" Bacteria_BM663-01M0087_good_1_trimmed.fq ## nada

## looks pretty good, move them to their own directory:
#mv *trimmed* /media/vol1/daniel/sulariArne/illuminaReads/goodReads_primerClipped/

cd /media/vol1/daniel/sulariArne/illuminaReads/goodReads_primerClipped

######################

## with a specialized environment for dada2.
## I remember R with conda package management being kind of 
## a pain

conda activate 

conda create -n dada2 -c r r-base

## to old
conda remove -n dada2 --all

## can we force a newer version?

conda create -n dada2 -c r r-base>=4.3.0

conda update -n base -c defaults conda

conda activate dada2


conda install -c bioconda bioconductor-dada2

## fails
## try internal installations:


## in R
install.packages("BiocManager")



library("BiocManager")
BiocManager::install("dada2")

BiocManager::install("phyloseq")

library("phyloseq")


## that takes forever.

## following these tutorials from the dada2 site:
http://benjjneb.github.io/dada2/bigdata_paired.html
http://benjjneb.github.io/dada2/bigdata.html

## but start with this:
http://benjjneb.github.io/dada2/tutorial.html


#############

## try to get Arne and Sulari to divide up their read directories, to reduce 
## memory usage?

nanoComp

conda activate dada2
R
library(dada2)
library(phyloseq)

packageVersion("dada2")

## will dada2 take our data as is?


## on lab comp
setwd("/media/vol1/daniel/sulariArne/illuminaReads/")

## on personal comp:
#setwd("/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/soilExtractions")

## for clipped: 
#path <- "/media/vol1/daniel/sulariArne/illuminaReads/goodReads_primerClipped"

## sample names are here:


sampleNames <- read.csv('sampleName_clientId.txt', 
                        sep='\t', 
                        col.names= c('sampleName', 'clientId'))

## on my personal computer:
#sampleNames <- read.csv('/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/readReport/sampleName_clientId.txt',
#                        sep='\t', 
#                        col.names= c('sampleName', 'clientId'))

getwd()



head(sampleNames)

## for raw reads: 
path <- "/media/vol1/daniel/sulariArne/illuminaReads/uncompressedReads"

## on personal computer: ##
#path <- "/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/readsUncompressed"

list.files(path)


# make two sorted lists, for R1 and R2
fnFs <- sort(list.files(path, pattern="_1.fq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_2.fq", full.names = TRUE))


plotQualityProfile(fnFs[1:2]) ## kicks error. is this because of the lab computer setup?
## yes, works fine on my local setup

## does the order of our sampleNames DF fit the order of our file name vectors?

head(sampleNames)

fnFs[1:6]

tail(sampleNames)

fnFs[190:195]

## looks good
## so this should be the vector of sample names:

sample.names <- sampleNames$clientId
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names


#### can we subset to just Sulari's reads? ###

## use grep to get just samples that start with "S":
onlySulari <- grep("S", filtFs)
controls <- grep("C[1-2]", filtFs)
sulAndCon <- c(onlySulari, controls)
justSulariSamplesF <- filtFs[sulAndCon]


## repeat with reverse files
onlySulari <- grep("S", filtRs)
sulAndCon <- c(onlySulari, controls)
controls <- grep("C[1-2]", filtRs)
justSulariSamplesR <- filtRs[sulAndCon]

length(justSulariSamplesF)
head(justSulariSamplesF)
tail(justSulariSamplesF)

length(justSulariSamplesR)
head(justSulariSamplesR)
tail(justSulariSamplesR)

#################################################


out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft=c(19,20), 
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE, 
                     compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE

## that uses pretty much all of my resources on my laptop
## run it on the lab computer.

## for me, I import them here:

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft=c(19,20), 

head(out) ## gives you a report of the results.

## to 
## now dada2 needs to model the errors from the sequencing run:

## now this may be to much for laptops:
errF <- learnErrors(filtFs, multithread=TRUE)

## interruptions, so save this to be recovered for tomorrow
#save(errF, file='errF.rda')

load('errF.rda')


errR <- learnErrors(filtRs, multithread=TRUE)
#save(errR, file='errR.rda')

load('errR.rda')

getwd()


## if your computer is overrun at this point (out of RAM)
## try script below this one

plotErrors(errF, nominalQ=TRUE)

## to run dada2 on ALL the reads (Arne, Sulari, Controls)
dadaFs <- dada(filtFs, err=errR, multithread=TRUE)
#save(dadaFs, file='dadaFs.rda')
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)

## can we do this with just Sulari's reads (and controls)?
dadaFs <- dada(justSulariSamplesF, err=errF, multithread=TRUE)
#save(dadaFs, file="dadaFsSulariOnly.rda")

dadaRs <- dada(justSulariSamplesR, err=errR, multithread=TRUE)
#save(dadaRs, file="dadaRsSulariOnly.rda")

load("dadaFsSulariOnly.rda")
load("dadaRsSulariOnly.rda")


## the object is here:
dadaFs[[1]]
dadaRs[[1]]


## for just sulari
mergers <- mergePairs(dadaFs, justSulariSamplesF, dadaRs, justSulariSamplesR, verbose=TRUE)

#save(mergers, file="sulariMergers.rda")

load("sulariMergers.rda")


## for all samples
#mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)


# Inspect the merger data.frame from the first sample

head(mergers[[1]])

## so now we should have fully denoised, merged sequences
## this means we can construct ASVs, or zero-radius-OTUs (zOTUs)

## after merging, keep following the same tutorial, here:

## http://benjjneb.github.io/dada2/tutorial.html

seqtab <- makeSequenceTable(mergers)

?makeSequenceTable

dim(seqtab)

# Inspect distribution of sequence lengths

table(nchar(getSequences(seqtab))

## get rid of chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)

#save(seqtab.nochim, file ="sulariOnlyseqtab.nochim")

load("sulariOnlyseqtab.nochim")

dim(seqtab.nochim)

## how many were chimeras?

sum(seqtab.nochim)/sum(seqtab) ## 99%, good stuff

## let's get the latest tax training dataset:

wget https://zenodo.org/record/4587955/files/silva_nr99_v138.1_train_set.fa.gz

taxa <- assignTaxonomy(seqtab.nochim, "silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)

#save(taxa, file ="sulariOnlyTaxa.rda")

load("sulariOnlyTaxa.rda")


head(taxa)

taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)




## checking mock communities:

## first mock community

unqs.mock <- seqtab.nochim['S53',]

unqs.mock <- seqtab.nochim['C1.1',]

unqs.mock <- seqtab.nochim['C1.2',]

unqs.mock <- seqtab.nochim['C2.1',]

unqs.mock <- seqtab.nochim['C2.2',]

unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community.\n")

dim(seqtab.nochim)

seqtab.nochim['C1.1',2:10]


tail(seqtab.nochim)[,1:2]


## try these out on phyloseq!

library(phyloseq)

ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               tax_table(taxa))

########## phyloseq ###############

## on lab computer
conda activate dada2

R
library(dada2)
library(phyloseq)


## try  making a phyloseq object out of just our  
## otu table and our taxonomic object:

setwd("/media/vol1/daniel/sulariArne/illuminaReads/")


## we can move directly from the code 
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
#save(ps, file="sulariPhyloseqObject.rda")

## or if we are coming back from a break  we can start here:

load("sulariPhyloseqObject.rda")

taxa_names(ps)

## let's start with this introductory tutorial,

http://joey711.github.io/phyloseq-demo/phyloseq-demo.html

## start with the section "Basic Interaction with phyloseq Data"

ntaxa(ps)

nsamples(ps)

sample_names(ps)[1:10]

taxa_names(ps)[1]





## sulari's environmental data should be here:

## get the latest:
#download.file(
#    "https://raw.githubusercontent.com/danchurch/fichtelgebirgeSoils/main/Envt_Matrix.csv",
#    destfile = "Envt_Matrix.csv", 
#    method="wget")

env_data = read.csv('Envt_Matrix.csv', row.names=1)


sample_data(ps) <- env_data





tail(env_data)



## we also have geographic data we can add to our 
## environmental matrix:

download.file(
    "https://raw.githubusercontent.com/danchurch/fichtelgebirgeSoils/main/carb4D_cleaned.csv", 
    destfile = "carb4D_cleaned.csv",
    method="wget") ## for windows users, don't add method, use default

geoDat <- read.csv("carb4D_cleaned.csv")

## now get corresponding physical locations of each sample:

## do plotIDs match up?


env_data[1:8,1:3]

head(geoDat, n=3)

tail(geoDat, n=3)

geoDat[geoDat$plotID == "P0104C",]

geoDat[geoDat$plotID == "P0104",]




###### this is how dan got lat/lon data from betty's spreadsheet ####

env_data <- read.csv('Envt Matrix.csv')
geoDat <- read.csv("carb4D_cleaned.csv")

getLat <- function(samp){
bb <- substring(samp, 1, 5)
cc <- grep(bb, geoDat$plotID, value=TRUE)
lat <- as.numeric(geoDat[geoDat$plotID == cc,]["lat"])
return(lat)
}
getLon <- function(samp){
bb <- substring(samp, 1, 5)
cc <- grep(bb, geoDat$plotID, value=TRUE)
lon <- as.numeric(geoDat[geoDat$plotID == cc,]["lon"])
return(lon)
}
## 
lats <- vector()
for (i in env_data$Plot.ID){
  lats <- c(lats,getLat( i))
}
## add the control NAs
lats <- c(lats, c(NA,NA,NA,NA))
env_data["Latitude"] <- lats
lons <- vector()
for (i in env_data$Plot.ID){
  lons <- c(lons,getLon( i))
}
## add the control NAs
lons <- c(lons, c(NA,NA,NA,NA))
env_data["Longitude"] <- lons


## get rid of old, empty lattitude and longitude:
env_data <- env_data[,-9:-10]

head(env_data)

tail(env_data)



write.csv(env_data, file='Envt_Matrix.csv')

## use this now as our latest environmental data
## from now on, let's avoid spaces in file names

###################################################




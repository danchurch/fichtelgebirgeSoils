## git our local copy of the repo in order for office comp.
## working off the work tower, so need to get git synced up now to
## avoid confusion.

## we need RSA with SHA-2 signature algorithm

man ssh-keygen
ssh-keygen -t rsa -f fuj2git

## now, what do we need to get the push functionality...

git clone https://github.com/danchurch/fichtelgebirgeSoils.git

## test

touch thisIsNotReal.txt

## and of course can't push

git config --global user.email "danchurchthomas@gmail.com"
git config --global user.name "danchurch"

git remote add origin https://github.com/danchurch/fichtelgebirgeSoils.git
git branch -M main
git remote set-url origin git@github.com:danchurch/fichtelgebirgeSoils.git
git push -u origin main

## and we're in business with github

## maybe let's get a conda environment going for this.

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash

conda config --set auto_activate_base false

## get the mamba solver:

conda update -n base conda
conda install -n base conda-libmamba-solver
conda config --set solver libmamba

## get the standard channels
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

## this new conda env comes with python3.12

## let's see if this works for our spatial analysis


conda activate
conda create -n "spatialDirt" 
 
## let's think about spatial turnover in Sulari's community data

## first step would be to get a map. 

## we want to see where we sample, and visualize respiration 
## values across the landscape

conda deactivate

conda remove -n spatialDirt --all

conda create -n "spatialDirt" 

conda activate "spatialDirt" 
conda config --env --add channels conda-forge
conda config --env --set channel_priority strict

conda install python=3 geopandas

conda activate spatialDirt 

pip install rasterio

## we also need to be R up to speed...
## maybe do this outside of conda

conda deactivate

sudo R 

install.packages("BiocManager")
BiocManager::install("phyloseq")


## I think that took care of most of the complex installs

## oh wait, let's get the jupyter notebook setup going...

## how do we make sure that the jupyter behaves, stays in the 
## right python?

conda activate spatialDirt 
pip install notebook 

which jupyter ## looks like that work. Gets easier every year.

## and it looks like it is even keeping the R kernel from 
## my general environment. 

## to get a bash kernel on there? https://github.com/takluyver/bash_kernel

pip install bash_kernel
python -m bash_kernel.install

##### bayesian setup #####

## last time we worked with pymc3 we needed
## a separate conda env. Let's see if 
## things have gotten better. 

## first back up the env, just in case

conda activate spatialDirt

conda env export > spatialDirt.yml

## installing bambi should also install pymc, so
## try the bambi conda installs as per: 
https://github.com/bambinos/bambi#quickstart

pip install bambi

pip install "preliz[full,lab]"

## get the data from the newest martin book.

## put outside our repo

cd /home/daniel/Documents/manualsBooks/bayesian
git clone https://github.com/aloctavodia/BAP3.git

## seems like that worked in our spatialDirt environment

## try everything out for a bit, then update yaml backup for the repo


conda activate spatialDirt 

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
from matplotlib_scalebar.scalebar import ScaleBar
import pymc as pm


spatDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
## ^different on laptop
os.chdir(spatDir)

## we just want a map of points right now:

## on officeComp
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"
## on laptop
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"

envData = pd.read_csv(sulariEnvCSV)
## get rid of spaces
envData.rename({"Sample ID":"SampleID"}, axis="columns", inplace=True)
## we need to clean up the plot.ID. Sulari recorded season in the plot IDs,
## using letter codes. Also she has one double sampling, with an underscore
envData['Plot.ID'] = envData['Plot.ID'].str.slice(0,5)

## let's get rid of decimals in the names, weird for python work:
envData.rename({"Plot.ID":"PlotID", 
       "soil.respiration":"soil_respiration",
              "Land.type":"Land_type"}, 
                axis="columns", inplace=True)

envData.head()

## so we don't have to repeat:
envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index=False)

envData.head()


envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
 

## if we need to drop controls
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

## her otu table is really large:
##### R ###
library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")
logMin50ps
comdat <- as.data.frame(otu_table(logMin50ps))
write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")
############

## back in python

comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", index_col=0)

comData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

comData.head()

## the map below looks funny...looks like every forest plot has 
## a grassland under it...

dupsFilter = envData.Latitude.duplicated()
dupped = envData[dupsFilter].sort_values(by="Latitude")
dupped.groupby('Plot.ID').nunique()
envData.iloc[0:5,0:8]

## nope...looks okay

## we want a geodf...

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

## to convert to UTM? looks like we are in zone 33
## looks like:
## EPSG:32633

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

sulariPlot_utm.tail()

## can we import our georeferenced tif of the study area?
## lat/lon
#fichtelMap = rasterio.open("studyAreaClipped_modified.tif")
## UTM

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color code our points according to land type?
cdik={
"Arableland":"b",
"Grassland":"y",
"Forest":"g",
}
sulariPlotsDF['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]
sulariPlot_utm['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]

fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    facecolor=sulariPlotsDF['landColors'],
    markersize=400) 


grassPatch = Patch(color='y', label='grassland',)
forestPatch = Patch(color='g', label='forest')
farmPatch = Patch(color='b', label='arable land')
ax.legend(handles=[grassPatch, forestPatch, farmPatch], 
          loc="lower left",
          fontsize=15,
)

## if we want to compare just grassland and forest

plt.close('all')
onlyGrassForest = sulariPlot_utm[sulariPlot_utm['Land.type'].apply(lambda x: x in ["Forest", "Grassland"])]
fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
onlyGrassForest.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    linewidths=2,
    facecolor=onlyGrassForest['landColors'],
    markersize=200) 
ax.ticklabel_format(style='plain', axis='y', useOffset=False)
grassPatch = Patch(color='y', label='grassland',)
forestPatch = Patch(color='g', label='forest')
ax.legend(handles=[grassPatch, forestPatch], loc='lower left')
ax.add_artist(ScaleBar(1, location='lower right')) 
ax.set_xlim([265500, 286930])
ax.set_ylim([5547227, 5570000])
plt.savefig('forestVsGrasslandMapUTM.png', dpi=600, format='png')

## Look at the turnover data:

## lat/long
aa = pd.DataFrame({'xx':envData.Longitude, 'yy':envData.Latitude})
physDist = sp.distance.pdist(aa, metric='euclidean')
bcDist = sp.distance.pdist(comData, metric='brayCurtis')
fig, ax = plt.subplots()
ax.scatter(physDist, bcDist)
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title(label="Turnover with Lat/Lon", loc='center')

## utms
aa = pd.DataFrame({'xx':sulariPlot_utm.geometry.x, 'yy':sulariPlot_utm.geometry.y})
physDist = sp.distance.pdist(aa, metric='euclidean')
bcDist = sp.distance.pdist(comData, metric='brayCurtis')
fig, ax = plt.subplots()
ax.scatter(physDist, bcDist)
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title(label="Turnover with UTM", loc='center')

plt.close('all')
## subset by landtype
for lt in [ "Arableland" ,"Grassland" ,"Forest"]: 
    print(lt)
    edf = envData[envData['Land.type'] == lt]
    cdf = comData.loc[edf.index]
    aa = pd.DataFrame({'xx':edf.Longitude, 'yy':edf.Latitude})
    aa = aa.iloc[0:120,:]
    physDist = sp.distance.pdist(aa, metric='euclidean')
    bcDist = sp.distance.pdist(cdf, metric='brayCurtis')
    fig, ax = plt.subplots()
    ax.scatter(physDist, bcDist)
    ax.set_title(lt)
    ax.set_title(label= (lt + " in degrees"), loc='center')
    X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
    ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')

## well that looks pretty much like I hypothesized
## good stuff.

sulariPlot_utm.head()

plt.close('all')
plt.rc('ytick', labelsize=15)
plt.rc('xtick', labelsize=15)
lts = [ "Arableland" ,"Grassland" ,"Forest"]
#lts = [ "Grassland" ,"Forest"]
fig, axes = plt.subplots(nrows=1, ncols=len(lts), sharey=True)
axes = axes.flatten()
for nu,lt in enumerate(lts):
    edf = sulariPlot_utm[sulariPlot_utm['Land.type'] == lt]
    cdf = comData.loc[edf.index]
    aa = pd.DataFrame({'xx':edf.geometry.x, 'yy':edf.geometry.y})
    physDist = sp.distance.pdist(aa, metric='euclidean')
    bcDist = sp.distance.pdist(cdf, metric='brayCurtis')
    axes[nu].scatter(physDist, bcDist)
    X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
    linMod =  LinearRegression().fit(X, Y)
    axes[nu].plot( X, linMod.predict(X), c='k')
    axes[nu].set_title(label=lt, size=20, loc='center')
    axes[nu].set_xlabel('meters', size=20)
    print(lt, stats.linregress(physDist,bcDist))

fig.suptitle("Turnover in prokaryotic community", size=40)
axes[0].set_ylabel('Bray-Curtis dissimilarity', size=20)
axes[1].tick_params(left=False, labelleft=False, right=True, labelright=True, color='red', axis='y')
plt.subplots_adjust(wspace = 0)


###################################
##
## outputs from stats.regress:
##
## Arable Land
## slope=2.810886008879358e-06
## intercept=0.5742255266887248
## rvalue=0.07906122967379002
## pvalue=0.02033328808278514
## stderr=1.209265073980233e-06
## intercept_stderr=0.013364930035836518
## 
## Grassland
## slope=4.135586933082137e-07
## intercept=0.5590568736859212
## rvalue=0.012708447690178782
## pvalue=0.7298157896065798
## stderr=1.1969812747713313e-06
## intercept_stderr=0.013504959623407586
## 
## Forest
## slope=5.843245351182221e-06
## intercept=0.6024952517397398
## rvalue=0.20458396890349276
## pvalue=1.918646699083231e-08
## stderr=1.0284330215725841e-06
## intercept_stderr=0.01259259776931045
######################################

## add in the correlation coefficients and pvalues to grant app graphic. 

###### SAC curves ##########

## we have to stop avoiding gamma diversity calculations...

## do this in vegan? why not.

R

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
setwd(spatDir)


library(vegan)
library(phyloseq)

comM <- read.csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv', 
                    row.names=1)

## why is this so big, btw?

sum(colSums(comM) > 0) ## 4363. Why do we have a bunch of empty colums? I think these were low abundance ASVs, below our cutoffs.

## get rid of them to save memory:

library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")

logMin50ps

comdat <- as.data.frame(otu_table(logMin50ps))
comdat = comdat[,colSums(comdat) > 0] 

write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")


## get rid of controls
notControls=!(row.names(comM) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comM = comM[notControls,]

comM[1:4,1:4]


sp1 <- specaccum(comM)

plot(sp1, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue")

specpool(comM)

sp2 <- specaccum(comM, "random")

summary(sp2)

plot(sp2, ci.type="poly", col="red", lwd=2, ci.lty=0, ci.col="pink")


data(BCI)

sp1 <- specaccum(BCI)


sp2 <- specaccum(BCI, "random")

sp2

summary(sp2)

plot(sp1, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue")
boxplot(sp2, col="yellow", add=TRUE, pch="+")
## Fit Lomolino model to the exact accumulation
mod1 <- fitspecaccum(sp1, "lomolino")
coef(mod1)
fitted(mod1)
plot(sp1)

aa <- specaccum(comM, method = "exact")

?specaccum

anaSAC <- data.frame(aa$richness, aa$sd)
colnames(anaSAC) <- c('richness', 'sd')
anaSpeciesEstimators = specpool(comM)
print(anaSpeciesEstimators)
 
## okay, but we need to separate out by land types.
## wish we were in python...

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
setwd(spatDir)
library(vegan)
library(phyloseq)

comData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", row.names=1)
envData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv", row.names=1)

all(row.names(comData) == row.names(envData))

lt <- 'Forest'
justThisLandtype=row.names(envData[envData['Land_type'] == lt,])
aa <- comData[ justThisLandtype,]
sp1 <- specaccum(aa)
plot(sp1, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue")


## so loop this:

for (lt in c('Arableland','Grassland', 'Forest')){
    print(lt)
    justThisLandtype=row.names(envData[envData['Land_type'] == lt,])
    comm.i <- comData[ justThisLandtype,]
    specAccum.i <- specaccum(comm.i)
    SACdf.i <- data.frame(specAccum.i$richness, specAccum.i$sd)
    colnames(SACdf.i) <- c('richness', 'sd')
    speciesEstimators.i = specpool(comm.i)
    print(speciesEstimators.i)
    write.csv(SACdf.i, file=paste(lt, "SAC.csv", sep="_"))
    write.csv(speciesEstimators.i, file=paste(lt, "specEst.csv", sep="_"))
}

## interesting, this is pretty much exactly what Brendan found
## in the amazon. Despite lower alpha diversity, higher beta 
## diversity in forest soils. 
## and this equates to a higher total diversity across the
## survey (gamma). 

## take over to python for plotting

## we have an old function for this, wonder if it still works:


os.chdir("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis")

sacs = [ i for i in os.listdir() if "_SAC.csv" in i  ]

def plotSACs(habtype, color='black', ax=None):
    if ax is None: fig, ax = plt.subplots()
    sacs = [ i for i in os.listdir() if "_SAC.csv" in i  ]
    sacName = (habtype +'_SAC.csv')
    assert( (habtype +'_SAC.csv') in sacs)
    specEstName = (habtype + "_specEst.csv")
    sac_i = pd.read_csv(sacName, index_col=0)
    specEst_i = pd.read_csv(specEstName, index_col=0).loc['All']
    specEst_i.index = specEst_i.index.str.replace(".","_")
    X = sac_i.index
    ax.plot(X, sac_i['richness'], color=color)
    ax.fill_between(x=X,
                     y1=sac_i.richness - sac_i.sd,
                     y2=sac_i.richness + sac_i.sd,
                    alpha=0.4,
                    color=color,
                    )

plt.close('all')
fig, ax = plt.subplots(figsize=(10,10))
plotSACs('Arableland', ax=ax, color='#862d2d')
plotSACs('Forest', ax=ax, color='#006600')
plotSACs('Grassland', ax=ax, color='#FF7F00')

Arableland_patch = Patch(color='#862d2d', label='Arableland', alpha=0.4)
Forest_patch = Patch(color='#006600', label='Forest', alpha=0.4)
Grassland_patch = Patch(color='#FF7F00', label='Grassland', alpha=0.4)

ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])
ax.set_title('Species accumulution curves by\nland-use/Habitat')
ax.set_xlabel('Sites sampled')
ax.set_ylabel('Prokaryotic ASVs')


[Arableland_patch, Forest_patch, Grassland_patch]

## repeat alpha diversity
## using our >50 reads OTU table, can we calculate alpha diveristy by land type?
## back to old fashioned vegan/R

## we just want species richness. So we need to rarefy and compare 
## forest v. farm v. grassland data

notControls=!(row.names(comData) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comData = comData[notControls,]
envData = envData[notControls,]

data(BCI)

S <- specnumber(BCI) # observed number of species

S <- specnumber(comData) # observed number of species

## pretty much same as:
aa <- comData
aa[aa > 0] <- 1
rowSums(aa)

S <- specnumber(comData) # observed number of species

(raremax <- min(rowSums(BCI)))

Srare <- rarefy(BCI, raremax)

plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")

abline(0, 1)

rarecurve(BCI, step = 20, sample = raremax, col = "blue", cex = 0.6)


## so we are looking for a rarified species richness for each site.

## from this we will generate 3 mean +/- error values of species richness 
## one for each land use.

## this kind of analysis is really ok for count data.
## we've done all kinds of transformations, to try 
## to reduce sequencer error. 

## so I think we need to back up to phyloseq, to use our 
## sequencing depth information

library(phyloseq)


## transformed:
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")

## not transformed:
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

logMin50ps

(p = plot_richness(ps, x = "Land.type"))

estimate_richness(ps)

estimate_richness

## but I am thinking about this incorrectly. 
## these richness estimates are way high, 
## because of PCR, sequencer error, etc. 

## we attempting to reign in these errors 
## a bit through our transformations, let's
## honor this. 

## so back and use our communty matrix, 

savehistory("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/latelyInR.txt")

## to make it into "count data", multiply 
## to get rid of decimals:


sum(comData < .001 & comData > 0) ## 508 observations smaller than .001.
sum(comData < .0001 & comData > 0) ## 0 observations, so let's multiply by 10000
comDataFakeCounts = ceiling(comData * 10000) 
min(comDataFakeCounts[ comDataFakeCounts > 0 ]) ## our smallest non-zero observation is 8 

S <- specnumber(comDataFakeCounts) # observed number of species

(raremax <- min(rowSums(comDataFakeCounts))) ## 10001

rowSums(comDataFakeCounts)

Srare <- rarefy(comDataFakeCounts, raremax) ## this is what we need. 

## this is an estimate of how many species are present in each 
## sample, after coming down to a minimum abundance


## kind of interesting but not useful. Shows we sequenced deeply enough:
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")

## can we do all that without transforming to "counts"?
S2 <- specnumber(comData) # observed number of species, same as fake counts

(raremax2 <- min(rowSums(comData))) ## .9896

rowSums(comData)



## compare to:

bb <- specnumber(comData) # observed number of species
all(bb == Srare) ## yes. the same
## So I guess I am brilliant, I just reinvented their command. 
## big waste of time. 

## maybe also shows there isn't really a need to rarefy, at least
## on the transformed data

## not sure, but now let's trust these numbers.

## now, subset by land type, and get means?

head(envData)

all(row.names(envData) == row.names(comData))

## I'd guess we need a vector of group names (by land_type):
hist(Srare, 20) ## looks more or less normal.
mean(Srare) ## 301.3833
sd(Srare) ## 26.4

print("mean alpha diversity of all sites = ", mean(Srare))

print(paste("mean alpha diversity of all sites =", mean(Srare), "ASVs"))

cat(paste("mean alpha diversity of all sites =", mean(Srare), "ASVs"))

cat(paste("mean alpha diversity of all sites =", mean(Srare), "+/-", round(sd(Srare)), "ASVs"))


all(names(Srare) == row.names(envData))

## break this down by groups:

tapply(Srare, envData$Land_type, mean)
tapply(Srare, envData$Land_type, sd)

boxplot(Srare ~ envData$Land_type)

## anova
res.aov <- aov(Srare ~ envData$Land_type)
summary(res.aov) ## F= 2.85, p = 0.0618

## so maybe differences in species richness due to land type,
## maybe not.  Not a large effect, anyway. 
## update notebook, give it a break.

## t-test for difference between forest and grassland?

## to remove the farm samples 

noFarms <- envData$Land_type != "Arableland"

envData$Land_type[noFarms]
Srare[noFarms]


head(envData)

t.test(Srare[noFarms] ~ envData$Land_type[noFarms])

t_test(weight ~ group)

## we should probably do this in a bayesian way...
## get the ordinations done, then order the new book, 
## PCNMs can also be started without tests.

## but generally not sure how to handle the multivariate
## tests in a bayesian way. 

## two possibilities:
## BetaBayes: https://doi.org/10.3390/d14100858, somehow related to GDS
## BERA: https://doi.org/10.1080/00273171.2019.1598837
## BERA is apparently related to RDA. More reading is in order on both. 

## in the meantime...re-run the ordinations:

## the usual pipeline
## let's get ordinations with vegan and plot with python 

library(vegan)

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
setwd(spatDir)

comData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", row.names=1)
envData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv", row.names=1)
## get rid of controls
notControls=!(row.names(comData) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comData <- comData[notControls,]
envData <- envData[notControls,]

comData[1:4,1:4]

comNMS <- metaMDS(comData, try=40)

write.csv(comNMS$points, file='comNMS.csv')

## check this out in python:

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import matplotlib.colors 
import os
import scipy.spatial as sp
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
import pymc as pm
import preliz as pz
import arviz as az
import rasterio.plot

## data
nmsPts = pd.read_csv("comNMS.csv", index_col=0)
sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 
controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
envData.drop(controls, axis='rows', inplace=True)
comData.drop(controls, axis='rows', inplace=True)

## need some colors for land type
colorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
landCols = [ colorDict[i] for i in envData['Land_type'] ]
plt.close('all')
fig, ax = plt.subplots()
ax.scatter(x=nmsPts["MDS1"],
           y=nmsPts["MDS2"], 
           c=landCols,
          )
Arableland_patch = Patch(color='#862d2d', label='Arableland')
Forest_patch = Patch(color='#006600', label='Forest')
Grassland_patch = Patch(color='#FF7F00', label='Grassland')
ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])

## try markers for seasonality?
seasonDict = {
     'S': "o", 
    'SP': "v", 
    'W1': "D", 
    'W2': "P", 
     'A': "s",
}

## we might also check pH, and seasonality, and microbial biomass

## seasonality:
seasonShapes = [ seasonDict[i] for i in envData['season'] ]

## matplot lib doesn't change markers on the fly...
## if we want to change markers for each season:

plt.close('all')
fig, ax = plt.subplots()
for i in envData.season.unique():
  print(i)
  env_i = envData[envData['season'] == i]
  plots_i = env_i.index.to_list()
  nmsPts_i = nmsPts.loc[plots_i]
  cols_i = [ colorDict[i] for i in env_i['Land_type'] ]
  ax.scatter(x=nmsPts_i["MDS1"],
             y=nmsPts_i["MDS2"], 
             c=cols_i,
        marker=seasonDict[i],
            )

## I don't see any evidence of seasonality affecting these
## community structures

## ph ordinations

## color by pH, land by symbol, respiration by size

landTypeShapesDict = {
'Arableland': "o", 
'Forest'    : "v", 
'Grassland' : "s", 
}

pHmin = envData['pH'].min() ## 3.647
pHmax = envData['pH'].max() ## 7.312
norm=matplotlib.colors.Normalize(pHmin, pHmax)
plt.close('all')
fig, ax = plt.subplots()
for i in envData.Land_type.unique():
  print(i)
  env_i = envData[envData['Land_type'] == i]
  plots_i = env_i.index.to_list()
  nmsPts_i = nmsPts.loc[plots_i]
  sizes = env_i['soil_respiration']*50
  ax.scatter(x=nmsPts_i["MDS1"],
             y=nmsPts_i["MDS2"], 
             #s=140,
             s=sizes,
             c=env_i['pH'],
             cmap='Spectral',
             edgecolors='black',
             marker=landTypeShapesDict[i],
             norm=norm,
            )

fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.Normalize(pHmin, pHmax), cmap='Spectral'),
             ax=ax, orientation='vertical', label='pH')


## this is a good graph. needs a legend. 

## as usual, legends are beyond my ability. Do them manually later if we want the figure.


### test out bayesian setup, try comparison of two groups ###

dist = pz.Beta()
pz.maxent(dist, 0.1, 0.7, 0.9)

## test out the pymc3 setup:
np.random.seed(123)
trials = 4
theta_real = 0.35
data = pz.Binomial(n=1, p=theta_real).rvs(trials)

np.random.seed(123)
trials = 4
theta_real = 0.35
data = stats.bernoulli.rvs(p=theta_real, size=trials)

with pm.Model() as our_first_model:
    θ = pm.Beta('θ', alpha=1., beta=1.)
    γ = pm.Bernoulli('γ', p=θ, observed=data)
    trace = pm.sample(1000, random_seed=123)

## works. I can hardly remember how to do this bayesian stuff,
## but we can work from old examples....

## let's redo our only statistical model/test so far, the comparison 
## of alpha diversity between the land types

## start with oswaldo's tips example:

tips = pd.read_csv("/home/daniel/Documents/manualsBooks/bayesian/BAP3/code/data/tips.csv")

tips.tail()

categories = np.array(["Thur", "Fri", "Sat", "Sun"])
tip = tips["tip"].values
idx = pd.Categorical(tips["day"], categories=categories).codes

## arviz has cool plotting capabilities I have not even begun to learn:
az.plot_forest(tips.pivot(columns="day", values="tip").to_dict("list"),
               kind="ridgeplot",
               hdi_prob=1,
               colors="C1",
               figsize=(12, 4))

## for indexing with Arviz:

coords = {"days": categories, "days_flat":categories[idx]}

with pm.Model(coords=coords) as comparing_groups:
    μ = pm.HalfNormal("μ", sigma=5, dims="days")
    σ = pm.HalfNormal("σ", sigma=1, dims="days")
    y = pm.Gamma("y", mu=μ[idx], sigma=σ[idx], observed=tip, dims="days_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))


_, axes = plt.subplots(2, 2, figsize=(10, 5), sharex=True, sharey=True)

az.plot_ppc(idata_cg, num_pp_samples=100,
            colors=["C1", "C0", "C0"],
            coords={"days_flat":[categories]}, flatten=[], ax=axes)

az.plot_trace(idata_cg)

plt.show()

## seems fine
az.summary(idata_cg, kind="stats").round(2)



## okay, how do we adapt this to our data? we want to compare 
## alpha diversity of three groups - crop, forest, and grassland.

sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 
controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
envData.drop(controls, axis='rows', inplace=True)
comData.drop(controls, axis='rows', inplace=True)

## we observed above that we can trust the raw species 
## richness counts from our community matrix, no need
## to rarify back or anything. 

## so how to get this in pandas/python?

aa = comData.copy()
aa[aa > 0] = 1
specRich = aa.sum(axis="columns")

comData.head()

aa.head()

aa.sum(axis="columns")

aa.sum(axis="columns").loc['S14']

aa.sum(axis="columns").loc['S102']


specRich = aa.sum(axis="columns")



pd.to_numeric(specRich, downcast='integer')

## maybe a df with all the info we need:

specRichLT = (pd.concat([pd.to_numeric(specRich, downcast='integer'), envData['Land_type']], axis='columns')
                     .rename({0:"spRich"},axis="columns"))

## looks right. so this should be our species richness. Might need this later:
specRichLT.to_csv("specRich.csv")

specRichLT.head()

## we want to get a distribution for the mean values of alpha diversity for
## each group 
 
specRichLT.head()

## can we visualize this with arviz first?:

az.plot_forest(specRichLT.pivot(columns="Land_type", values="spRich").to_dict("list"),
               kind="ridgeplot",
               hdi_prob=1,
               colors="C1",
               figsize=(12, 4))

## oswaldo's confusing code for creating an index, adapted for our data:
categories = np.array(["Arableland", "Grassland", "Forest"])
spr = specRichLT["spRich"].values
idx = pd.Categorical(specRichLT["Land_type"], categories=categories).codes
coords = {"Land_type": categories, "land_type_flat":categories[idx]}

with pm.Model(coords=coords) as comparing_groups:
    μ = pm.Normal("μ", mu=300, sigma=50, dims="Land_type")
    σ = pm.HalfNormal("σ", sigma=10, dims="Land_type")
    y = pm.Normal("Species richness", mu=μ[idx], sigma= σ[idx], observed=spr, dims="land_type_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))

    #y = pm.Gamma("y", mu=μ[idx], sigma=σ[idx], observed=spr, dims="land_type_flat") ## better for outliers?

plt.close('all')


with pm.Model(coords=coords) as comparing_groups:
    μ = pm.Normal("μ", mu=300, sigma=50, dims="Land_type")
    σ = pm.HalfNormal("σ", sigma=20, dims="Land_type")
    y = pm.Normal("Species richness", mu=μ[idx], sigma= σ[idx], observed=spr, dims="land_type_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))

plt.close('all')
fig, axes = plt.subplots(3, 1, sharex=True)
az.plot_ppc(idata_cg, num_pp_samples=100, coords={"land_type_flat":[categories]}, flatten=[], ax=axes)
fig.tight_layout()

## works, but the outliers are forcing a lot of variation into the posterior

## can we do this with cauchy?

with pm.Model(coords=coords) as model_t:
    μ = pm.Normal("μ", mu=300, sigma=50, dims="Land_type")
    σ = pm.HalfNormal("σ", sigma=20, dims="Land_type")
    ν = pm.Exponential('ν', 0.1, dims="Land_type") ## exponential gets flatter with lower values, mean gets pulled away from zero
    y = pm.StudentT('Species richness', nu=ν[idx], mu=μ[idx], sigma= σ[idx], observed=spr, dims="land_type_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))

## in the notebook, they give two models for this
## the second uses the gamma distribution.
## not run - why gamma? not sure. 

plt.close('all')

fig, axes = plt.subplots(3, 1, sharex=True)
az.plot_ppc(idata_cg, num_pp_samples=100, coords={"land_type_flat":[categories]}, flatten=[], ax=axes)
axes[0].set_xlim(200,400)
axes[1].get_legend().remove()
axes[2].get_legend().remove()
fig.tight_layout()


#### map of respiration values ####

## we still don't have a general analysis strategy,
## but it always helps to look at a map at the trait 
## of interest.

## we want to see rates of respiration across the 
## study. 

## it would be great to gave a vectorized land use map...
## does this exist somewhere?

## first, plot the respiration values:

envData.soil_respiration

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )
sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color edges by landtype:

landColorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
landCols = [ landColorDict[i] for i in sulariPlot_utm['Land_type'] ]
## 
plt.close('all')
fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
ax.ticklabel_format(useOffset=False, style='plain')
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    cmap='YlOrRd',
    #cmap='RdPu',
    column='soil_respiration',
    edgecolors=landCols,
    linewidth=2,
    markersize=sulariPlot_utm['soil_respiration']*50,
     )

ax.set_ylim(5547500, 5570000)
ax.set_xlim(265000, 287000)
respMin = envData['soil_respiration'].min() ## 0.488057256
respMax = envData['soil_respiration'].max() ## 13.70117879
fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.Normalize(respMin, respMax), cmap='YlOrRd'),
             ax=ax, orientation='vertical', label='resp')
Arableland_patch = Patch(color='#862d2d', label='Arableland')
Forest_patch = Patch(color='#006600', label='Forest')
Grassland_patch = Patch(color='#FF7F00', label='Grassland')
ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])

#### find species that are associated with high respiration (and low resp?) ##

## try deseq or bayesian equivalent. 
## or indicator species?
## cooccurrence networks?

## in general, we want species that are associated with high resp
## even in forests and croplands, which are the land types with 
## lower respiration rates generally. 

## we could cluster the communities and see if a cluster not related to
## to land type emerges. It is possible that a "highly respiring" community
## exists. But seems unlikely, across all land types. 

## take the highest respiring sites from each land type, and look for 
## species that occur only in them? or run deseq on each land 
## type alone and take the species positively associated with respiration

## see if there are any common species. We'll start with deseq, but 
## I think for publication I'd like to build some bayesian models for
## differential abundance


##### deseq ##### 

## need deseq. Let's install it in the overall environment, not just spatialDirt env


if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("DESeq2")

## we need raw abundances

R

library('phyloseq')
library('DESeq2')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

psLandCont <- ps ## make a duplicate phyloseq obj to play with

sample_data(psLandCont)[c('C1.1','C1.2','C2.1','C2.2'),'Land.type']  <- "control" ## add info

tail(sample_data(psLandCont)) ## looks okay

## okay, now following the tutorial above:

diagdds = phyloseq_to_deseq2(psLandCont, ~ Land.type)
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")

resultsNames(diagdds) ## Forest vs. Grassland not mentioned, but still possible

## let's see how the results look:
res <- results(diagdds, contrast=c("Land.type","Forest","Arableland"))

## edit down to highly significant results:

alpha = 0.1 ## significance cutoff
res <- res[complete.cases(res),] ## necessary because not all ASVs in all landtypes? 
## but then are those ASVs that are present only in one or two land types excluded here?
## these would be among the most important, I would think...think about this later
sigtab = res[(res$padj < alpha), ] ## cut off below the alpha using adjusted pvalues
## add taxonomy:
sigtab = cbind(as(sigtab, "data.frame"), as(tax_table(psLandCont)[rownames(sigtab), ], "matrix"))

## to check ASVs associated with respiration?

R

library('phyloseq')
library('DESeq2')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

## just like before make a duplicate phyloseq obj to play with
## and get rid of controls while we're at it
psNoControl = prune_samples(!(rownames(sample_data(ps)) %in% c("C1.1","C1.2","C2.1","C2.2")), ps)

## get rid of NAs in basalResp
basalRespNotNA <- !is.na(sample_data(psNoControl)$soil.respiration)
psNoControl = prune_samples(basalRespNotNA, psNoControl)
## okay, same old code as before:
diagdds = phyloseq_to_deseq2(psNoControl, ~ soil.respiration) ## set "treatment" of interest
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")
resultsNames(diagdds)
res <- results(diagdds)

alpha = 0.01 ## significance cutoff
res <- res[complete.cases(res),] ## necessary because not all ASVs in all landtypes
sigtab = res[(res$padj < alpha), ] ## cut off below the alpha using adjusted pvalues

sigtab$log2FoldChange

View(sigtab)



write.csv(sigtab, file="respDiffSeq.csv")

## these are all the strongly positively associated ASVs with resp:

highResp <- tax_table(ps)[c('ASV5', 'ASV13', 'ASV85', 'ASV110', 'ASV371', 'ASV462', 'ASV621',
       'ASV1089', 'ASV1419', 'ASV1795', 'ASV1831', 'ASV1905', 'ASV2058',
       'ASV2184', 'ASV2773')]

write.csv(highResp, file="hiRespTax.csv")


## there is a geobacter (ASV371) in there, maybe two (ASV2058). Weird.

ASV1831

## let's look at the rep sequence of these weird ones when we have time.


## since our "gene length" is exactly the same for all reads, I think we 
## can interpret baseMean as abundance 

## is this true? for instance, ASV2 has baseMean of 590. Seems like this 
## would be the abundance of ASV among the samples of lowest respiration values.

 
sample_data(psNoControl)$Basal.respiration

sample_variables(psNoControl)

## find some of these ASVs associated with increasing respiration - are they co-occurring? Are they where we expect them to be?

##### diffseq with land type as covariate #####


## if we want to check using covariate of land type, something like this?:

#diagdds = phyloseq_to_deseq2(psNoControl, ~ soil.respiration + Land.type) ## set "treatment" of interest
## ^ I think order matters here, covariate of interest should be last? 
## as per https://support.bioconductor.org/p/100828/

diagdds = phyloseq_to_deseq2(psNoControl, ~ Land.type + soil.respiration) ## set "treatment" of interest
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")
resultsNames(diagdds)
res <- results(diagdds)

alpha = 0.01 ## significance cutoff
res <- res[complete.cases(res),] ## necessary because not all ASVs in all landtypes
sigtab = res[(res$padj < alpha), ] ## cut off below the alpha using adjusted pvalues
sigtab$log2FoldChange

## these should ASVs that are changing with respiration 
## regardless of land type...

aa <- sigtab[ sigtab$log2FoldChange > 0, ]
bb <- aa[ aa$baseMean > 1, ]

tax_table(ps)[row.names(bb)]

View(sigtab)

## with this approach, the asvs that come across as most important here are:

ASV1164 Chloroflexi
ASV1734 acidobacteriota, Bryobacter
ASV2626 alphaprot, Esterales 
ASV4902 Actinbacteriota, Solirubrobacteraceae
ASV6938 Alphaproteo, Sphingomonas

## what habitat do these microbes prefer?
## plotted below, seems like only ASV1164 is useful new info

## to be sure, maybe subset (split) by land type and run? We lose
## statistical power, but could be interesting.

## we need a way to rapidly map ASVs.

## we also need to figure out if these are cooccurring

## we have a bunch of old, crude tools...start with these, and think about 
## a proper bayesian model for finding which species are responding

## the bracod paper seems very promising, even though it has zero citations...
## hasn't been updated in a year but could be worse. Interesting, it 
## focuses on the species as predictors of a host trait. That fits our 
## setup just fine, with species as predictors of respiration. Can't
## tell what the backend is. 


## the other option seems to be BORAL. This has been cited a bunch, in nature 
## etc. Looks like a good package to know about. More generalized, etc.
## but also looks like we do 
## uses old gibbs samplers, I think. Weird. 

## let's give bracod a run tomorrow, see if it is easy to set up and use

## in the meantime, how do we map the observations of an ASV? 

## should be do-able in python with the data we have:


sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 
controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
envData.drop(controls, axis='rows', inplace=True)
comData.drop(controls, axis='rows', inplace=True)
plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )
sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration',
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)
sulariPlotsDF.to_crs('EPSG:32633', inplace=True)
landColorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

fig,ax = plt.subplots()

## how can we generalize this so it can handle multiple ASVs? 

def mapOneASV(asv, ax=None, color="b", jitter=0, showLand=False):
    if ax is None: ax = plt.gca()
    jitX = sulariPlotsDF['geometry'].x.apply(lambda x: x + np.random.normal(loc=0, scale=jitter))
    jitY = sulariPlotsDF['geometry'].y.apply(lambda x: x + np.random.normal(loc=0, scale=jitter))
    asvPlotPoints = gpd.points_from_xy( jitX, jitY, crs="EPSG:32633" )
    asvGEO = gpd.GeoDataFrame(pd.concat([comData[asv], sulariPlotsDF['Land_type']], axis=1), geometry=asvPlotPoints, crs="EPSG:32633")
    if showLand: 
        asvGEO['landCols'] = [ landColorDict[i] for i in asvGEO['Land_type'] ]
    else: 
        asvGEO['landCols'] = "k"
    rasterio.plot.show(fichtelMap, ax=ax)
    asvGEO.plot(
        marker="o",
        ax=ax,
        linewidth=1,
        edgecolor=asvGEO['landCols'],
        facecolor=color,
        markersize=asvGEO[asv]*10000)
    return(asvGEO)

plt.close('all')

## then we can make maps as we see fit, with 0-inf otus.

mapOneASV("ASV2", color="b", jitter=0, showLand=True)

asv1 = mapOneASV("ASV1", color="r", jitter=0)

fig,ax = plt.subplots()

mapOneASV("ASV2", color="b", jitter=0)

mapOneASV("ASV1", color="r", jitter=0)

mapOneASV("ASV3", color="k", jitter=0)

## side note, ASV1 seems to be in a lot of plots...
## I thought this was E. coli from our mock community. Is this index bleed?

## how to check...

## this would mean diving back into the phyloseq pipeline and tracking 
## the prevalence of these MC otus...ugh, what a pain. 

## anyway, fairly low levels. If we are sticking to community level
## questions, probably not important.

## let's map some of the ASVs that look important for 

ASV1831 "Gemmataceae"         "Zavarzinella"
ASV371  "Geobacteraceae"      "Geobacter"

aa = mapOneASV("ASV1831", color="b", jitter=0)

aa = mapOneASV("ASV371", color="y", jitter=100)

aa[aa["ASV371"] > 0].index.values

envData.loc(bb)

envData.loc["S36"]
envData.loc["S70"]

## let's map the five most plentiful 
respASV = pd.read_csv("respDiffSeq.csv", index_col=0)

respASVtax = pd.read_csv("hiRespTax.csv", index_col=0)


respASV.head()

respASV.index


respASV.query("log2FoldChange > 1" ).index


## these are not so rare:

respASV.query("log2FoldChange > 1 & baseMean > 7")

## look at udeabacter


udea = mapOneASV("ASV621", color="b", jitter=0, showLand=True)

## interesting, but are any of these non-grassland species?
## subset to just forest sites, then run the deseq
## check tomorrow, have to work on some other things...

## here are some species that still appeared sensitive to 
## respiration, after accounting for land type, using deseq2 above

## ASV1164 Chloroflexi  forest
## ASV1734 acidobacteriota, Bryobacter
## ASV2626 alphaprot, Esterales 
## ASV4902 Actinbacteriota, Solirubrobacteraceae
## ASV6938 Alphaproteo, Sphingomonas

fig,ax = plt.subplots()

plt.close('all')

mapOneASV("ASV1164", color="b", jitter=0, showLand=True) ## definitely forest associated

aa = mapOneASV("ASV1734", color="b", jitter=0, showLand=True) ## definitely forest associated
aa = mapOneASV("ASV2626", color="b", jitter=0, showLand=True) ## only found in one plot above thresholds? So this probably disappears after our transformations
aa = mapOneASV("ASV4902", color="b", jitter=0, showLand=True) ## same: only found in one plot above thresholds? So this probably disappears after our transformations
aa = mapOneASV("ASV6938", color="b", jitter=0, showLand=True) ## also only one point. Oh jeez. 

(aa.iloc[:,0] > 0).sum()

## so the only the Chloroflexi from forests seems informative here.
## let's try subsetting by landtype:

## I think we need some multivariate approaches here. Networks and bayesian lms, like maybe bradco

## not sure if it will work with our current conda env?:

pip install BRACoD ## nope

## try a new env:

conda deactivate

conda create -n BRACoD python=3.6

conda activate BRACoD

pip install BRACoD

## theano not working...needs this?
conda install mkl-service

## that seems to have installed okay...test it out with sample data

## following https://github.com/ajverster/BRACoD

python

import BRACoD
import numpy as np

sim_counts, sim_y, contributions = BRACoD.simulate_microbiome_counts(BRACoD.df_counts_obesity)

sim_y ## our response variable 

contributions.shape

sim_relab = BRACoD.scale_counts(sim_counts)

help(BRACoD.run_bracod)

trace = BRACoD.run_bracod(sim_relab, sim_y, n_sample = 1000, n_burn=1000, njobs=4)

## too many cores? try defaults?
trace = BRACoD.run_bracod(sim_relab, sim_y, n_sample = 1000, n_burn=1000)


## whoah, this is slow...why?
## not working, freezes up. No real use of memory/cores


BRACoD.convergence_tests(trace, sim_relab)

conda env export > my_bracod_env.yaml


## that appears to be dead. 

## okay, that rules out bracod for the moment. Let's pick a cooccurence network 
## analysis method, try an install:

install.packages("devtools")

install.packages("BiocManager")

BiocManager::install("limma")

# Install NetCoMi
devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))


## try some sample features:

library(NetCoMi)

data("amgut1.filt")

amgut1.filt[0:10,0:10]


data("amgut2.filt.phy")

amgut2.filt.phy ## phyloseq obj


net_spring <- netConstruct(amgut1.filt,
                           filtTax = "highestFreq",
                           filtTaxPar = list(highestFreq = 50),
                           filtSamp = "totalReads",
                           filtSampPar = list(totalReads = 1000),
                           measure = "spring",
                           measurePar = list(nlambda=10, 
                                             rep.num=10,
                                             Rmethod = "approx"),
                           normMethod = "none", 
                           zeroMethod = "none",
                           sparsMethod = "none", 
                           dissFunc = "signed",
                           verbose = 2,
                           seed = 123456)



props_spring <- netAnalyze(net_spring, 
                           centrLCC = TRUE,
                           clustMethod = "cluster_fast_greedy",
                           hubPar = "eigenvector",
                           weightDeg = FALSE, normDeg = FALSE)


#?summary.microNetProps


summary(props_spring, numbNodes = 5L)

p <- plot(props_spring, 
          nodeColor = "cluster", 
          nodeSize = "eigenvector",
          title1 = "Network on OTU level with SPRING associations", 
          showTitle = TRUE,
          cexTitle = 2.3)

legend(0.7, 1.1, cex = 2.2, title = "estimated association:",
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"), 
       bty = "n", horiz = TRUE)

## or with pearson:

net_pears <- netConstruct(amgut2.filt.phy,  
                          measure = "pearson",
                          normMethod = "clr",
                          zeroMethod = "multRepl",
                          sparsMethod = "threshold",
                          thresh = 0.3,
                          verbose = 3)

## let's try it on our data.

## we can use a centered log transformation, pearson correlation as per the examples 
## on the NetCoMi github

load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

net_pears <- netConstruct(ps,  
                          measure = "pearson",
                          normMethod = "clr",
                          zeroMethod = "multRepl", ## don't understand totally...
                          sparsMethod = "threshold", ## don't understand totally...
                          thresh = 0.3,
                          verbose = 3)

## killed. To much memory required?


props_pears <- netAnalyze(net_pears, 
                          clustMethod = "cluster_fast_greedy")

plot(props_pears, 
     nodeColor = "cluster", 
     nodeSize = "eigenvector",
     title1 = "Network on OTU level with Pearson correlations", 
     showTitle = TRUE,
     cexTitle = 2.3)

legend(0.7, 1.1, cex = 2.2, title = "estimated correlation:", 
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"), 
       bty = "n", horiz = TRUE)


## can we port this over to the lab computer?

nanoComp

## let's try using the house R install:

## make a working directory:

cd /media/vol1/daniel/sulariArne/soilAnalysis


#library('DESeq2')

if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install('phyloseq')

library('phyloseq')

setwd("/media/vol1/daniel/sulariArne/soilAnalysis")

download.file("https://github.com/danchurch/fichtelgebirgeSoils/raw/main/sulariData/sulariPhyloseqObject.rda", destfile="sulariPhyloseqObject.rda")

load("sulariPhyloseqObject.rda")

ps ## looks ok


BiocManager::install("limma")

BiocManager::install("zCompositions")

install.packages("devtools") 

devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))


## dev.tools failed. 
## the following packages failed. Work on it tomorrow. 

install.packages("systemfonts")
## which needs...
sudo apt install libfontconfig1-dev

install.packages(       "xml2")
## which needs...
sudo apt install libxml2-dev

install.packages("textshaping")
## which needs...
sudo apt install libharfbuzz-dev libfribidi-dev

install.packages(  "rversions") ## easy
install.packages( "urlchecker") ## easy

install.packages(    "openssl")
## which needs
sudo apt install libssl-dev

install.packages(       "ragg")
## which needs
sudo apt install libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev

install.packages("credentials") ## easy
install.packages(      "httr2") ## easy
install.packages(       "httr") ## easy
install.packages(       "gert") ## easy
install.packages(         "gh") ## easy
install.packages(    "usethis") ## easy
install.packages(    "pkgdown") ## easy
install.packages(   "roxygen2") ## easy

## and...
install.packages(   "devtools")

devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))

## failed again. Lots of failed dependencies:
‘SpiecEasi’, ‘mixedCCA’, ‘qgraph’, ‘SPRING’, ‘WGCNA’ are not available for package ‘NetCoMi’


library(devtools)
install_github("zdk123/SpiecEasi")

## which needs:

## which needs fortran ??
sudo apt install gfortran
## and the following are not there:
ld -llapack --verbose
ld -lblas --verbose
sudo apt install liblapack-dev libopenblas-dev

install.packages("mixedCCA")
install.packages("qgraph") 
install.packages("SPRING")
install.packages("WGCNA")

## try again 
devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))

library('NetCoMi') ## no errors...finally...

R

library('phyloseq')
library('NetCoMi')
setwd("/media/vol1/daniel/sulariArne/soilAnalysis")

## does the plotter work on x11 forwarding?

plot(1) ## looks okay, for base plotter

#download.file("https://github.com/danchurch/fichtelgebirgeSoils/raw/main/sulariData/sulariPhyloseqObject.rda", destfile="sulariPhyloseqObject.rda")

load("sulariPhyloseqObject.rda")

## to run our data, we want a pretty rigorous minimum abundance threshold. 
## for all of our vegan stuff, we use a minimum abundance of 50 reads 
## per observation. 

## to be clear, we want all sample counts below 50 to be come zero

dropLow <- function(x) {
                           if (x < 0) {x = 0}
                          }

dropLow <- function(x) {
                           x <- x - 50
                           if (x < 0) {x = 0}
                           x
                          }

dropLow(100)
ps.atLeast50 <- transform_sample_counts(ps.filter.fam, dropLow)

## nope, can't handle if statements. Let' pull out the matrix and work 
## on it directly:


setwd("/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis")

load("../sulariData/sulariPhyloseqObject.rda")
library('vegan')
library('phyloseq')
library('NetCoMi')

ps.atLeast50 <- ps
aa <- otu_table(ps)
bb <- aa - 50 
bb[bb < 0] <- 0
otu_table(ps.atLeast50) <- bb
## can we trim out empty otus now? 
ps.atLeast50 <- prune_taxa( taxa_sums(ps.atLeast50) > 0, ps.atLeast50 )

otu_table(ps)[0:10,0:10]

otu_table(ps.atLeast50)[0:10,0:10]

dim(otu_table(ps)) 
dim(otu_table(ps.atLeast50)) ## down to just 4280 taxa, out of 36140

## ok, looks right
## now pass that to network software:

## just curious, can it handle the full ps? want to see how this changes the network.

#net_pears_fullPS <- netConstruct(ps,  
#                          measure = "pearson",
#                          normMethod = "clr",
#                          zeroMethod = "multRepl", ## don't understand totally...
#                          sparsMethod = "threshold", ## don't understand totally...
#                          thresh = 0.3,
#                          verbose = 3)

## nope, dies

## try smaller object:

net_pears_PSatleast50 <- netConstruct(ps.atLeast50,  
                          measure = "pearson",
                          normMethod = "clr",
                          zeroMethod = "multRepl", ## don't understand totally...
                          sparsMethod = "threshold", ## don't understand totally...
                          thresh = 0.3,
                          verbose = 3)

## that was computationally expensive:
save(net_pears_PSatleast50, file = "net_pears_PSatleast50.rda") ## 139 mb, pretty darn big..

props_pears <- netAnalyze(net_pears_PSatleast50, 
                          clustMethod = "cluster_fast_greedy")


?netConstruct

## huh, we can use a custom count matrix

## which means we can add columns. 
## so let's make a matrix that includes land-type, and 

?netAnalyze

## not run
plot(props_pears, 
     nodeColor = "cluster", 
     nodeSize = "eigenvector",
     title1 = "Network on OTU level with Pearson correlations", 
     showTitle = TRUE,
     cexTitle = 2.3)

legend(0.7, 1.1, cex = 2.2, title = "estimated correlation:", 
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"), 
       bty = "n", horiz = TRUE)

## plan for the day:

## rerun network analysis with environment variables land-use and respiration

## if it works, update notebook with it:



### on another note, we need to run jupyter on the lab computer...

## means 
## - install jupyter
## - clone repo to lab comp
##  maybe best to do this as we did on my own computer, within a conda env

## can we use our current spatialDirt yaml for this?

conda env export > spatialDirt_15.4.24.yaml

## update repo, clone onto lab
## on the lab comp, try to use it to make a new environment:

cd /media/vol1/daniel/sulariArne/soilAnalysis

wget https://raw.githubusercontent.com/danchurch/fichtelgebirgeSoils/main/spatialDirt_15.4.24.yaml 

conda update -n base conda

conda install -n base conda-libmamba-solver

conda config --set solver libmamba

## get the standard channels

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

conda env create --name spatialDirt --file=spatialDirt_15.4.24.yaml

## lots of errors...let's see...installing kernels as above

## is the R kernel on there?

## try: 
## https://stackoverflow.com/questions/44056164/jupyter-client-has-to-be-installed-but-jupyter-kernelspec-version-exited-wit

## in a sudo R sesh on lab comp:

install.packages('IRkernel')

system.file('kernelspec', package = 'IRkernel')

## gives us:
"/usr/local/lib/R/site-library/IRkernel/kernelspec"


## which we can give to our notebook
jupyter kernelspec install "/usr/local/lib/R/site-library/IRkernel/kernelspec" \
  --name "R" \
  --user 

jupyter kernelspec list

cd /media/vol1/daniel/miniconda3/envs/spatialDirt



## as per this site:
https://stackoverflow.com/questions/69244218/how-to-run-a-jupyter-notebook-through-a-remote-server-on-local-machine

jupyter notebook --no-browser --port=8080

ssh -L 8080:localhost:8080 test@132.180.112.115

## almost works. wants passwords and tokens and stuff:
https://jupyter-server.readthedocs.io/en/latest/operators/public-server.html

## for now use tokens...


## we need to synch up with git

## on the nanocomp

## as per above, the nanocomp computer repo needs 
## to be given permissions, etc.

cd /media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis

git config --global user.email "danchurchthomas@gmail.com"

git config --global user.name "danchurch"

git remote add origin https://github.com/danchurch/fichtelgebirgeSoils.git
git branch -M main
git remote set-url origin git@github.com:danchurch/fichtelgebirgeSoils.git

git push -u origin main

## nope, don't think I put keys for lab comp on there

cd ~/.ssh
ssh-keygen -t rsa -f nanoComp2git

## and as per here:
https://stackoverflow.com/questions/13509293/git-fatal-could-not-read-from-remote-repository
## the ssh-agent had to be started up and informed about the new key:

eval `ssh-agent -s`
ssh-add ~/.ssh/nanoComp2git

## and this had to be added to the bashrc. weird. never had to do that before...

## anyway, github seems synched. 
## and does that mean we can carefully work remotely with the jupyter notebook now?

## when we actually get comfortable with this, we can get back to editing this
## text file local. For now, both vim and jupyter are on the nanocomp computer:


cd /media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis

conda activate spatialDirt 

jupyter notebook --no-browser --port=8080

## run this to activate the tcp forwarding
ssh -L 8080:localhost:8080 test@132.180.112.115

## then open browser to: 
http://localhost:8080/notebooks/spatialAnalysisSulariData.ipynb

## it's going to be confusing working on both computers. 
## just slow it down, save/pull/push.

## calculating the network statistics takes for ever with netcommi

## can we multithread? its only using one core...


## I think we need to revisit the network software 

## we need some how a vector of the ASVs most responsive to 
## respiration, added to a cooccurrence matrix.

## we need to mkae our own adjacency matrix, I guess. 

## anyway, to get the candidates for responsiveness to 
## respiration...seems like we can't avoid a full multivariate
## treatment of the community matrix?

## let's take a look at the BORAL tool...

##### boral install and test ####

## boral github repo is here:
https://github.com/emitanaka/boral

## try the install on local machine:

install.packages('boral') 

install.packages('R2jags') 

install.packages('rjags') 

sudo apt install jags ## and work back up

library(boral)

## and pretty much zero documentation...

#### HMSC install and test #####

## let's try HMSC package:

install.packages("devtools") # if not yet installed
install.packages("usethis") # if not yet installed

library(devtools)

install_github("hmsc-r/HMSC")

## cran could also work, for stable version. 

## there is a book. Maybe we order it? 

## but for the moment, try to find some online examples?

## they have a series of vignettes they want you to work through

https://cran.r-project.org/web/packages/Hmsc/index.html

## start with the first:

https://cran.r-project.org/web/packages/Hmsc/vignettes/vignette_1_univariate.pdf


library(Hmsc)

set.seed(1)

## make a simple linear model:
## using a maximum likelihood fit:

n = 50
x = rnorm(n)
alpha = 0
beta = 1
sigma = 1
L = alpha + beta*x
y = L + rnorm(n, sd = sigma)

plot(x, y, las=1)


df = data.frame(x,y)
m.lm = lm(y ~ x, data=df)
summary(m.lm)

## compare this to HMSC

Y = as.matrix(y)

XData = data.frame(x = x)

m = Hmsc(Y = Y, XData = XData, XFormula = ~x)

nChains = 2
thin = 5
samples = 1000
transient = 500*thin
verbose = 500*thin

?sampleMcmc

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
nChains = nChains, verbose = verbose)

mpost = convertToCodaObject(m)

summary(mpost$Beta)

preds = computePredictedValues(m)

?computePredictedValues

evaluateModelFit(hM=m, predY=preds)

plot(mpost$Beta)

effectiveSize(mpost$Beta)

## if these indicators are close to one, the 
## chains behaved similarly during sampling
gelman.diag(mpost$Beta,multivariate=FALSE)$psrf


## evaluating at a maxlikelihood linear model:

nres.lm = rstandard(m.lm)
preds.lm = fitted.values(m.lm)
par(mfrow=c(1,2))
hist(nres.lm, las = 1)
plot(preds.lm,nres.lm, las = 1)
abline(a=0,b=0)

plot(m.lm)

## checking out a bayesian lm:

preds.mean = apply(preds, FUN=mean, MARGIN=1)

nres = scale(y-preds.mean)

par(mfrow=c(1,2))

hist(nres, las = 1)
plot(preds.mean,nres, las = 1)

abline(a=0,b=0)

## generalized linear models (link function, different dists) possible, just a couple
## for univariate models:

y = 1*(L+ rnorm(n, sd = 1)>0)

plot(x,y, las = 1)

### example hierachical model

## 100 samples, 10 plots, one response variable, checking for plot effects
## additive effects, in other words looking for different intercepts
## due to the different plots:

## make fake data:
n = 100
x = rnorm(n)
alpha = 0
beta = 1
sigma = 1
L = alpha + beta*x
np = 10
sigma.plot = 1
sample.id = 1:n
plot.id = sample(1:np, n, replace = TRUE)
ap = rnorm(np, sd = sigma.plot)
a = ap[plot.id]
y = L + a + rnorm(n, sd = sigma)
plot.id = as.factor(plot.id)
plot(x,y,col = plot.id, las = 1)
XData = data.frame(x = x)
Y = as.matrix(y)
studyDesign = data.frame(sample = as.factor(sample.id), plot = as.factor(plot.id))
rL = HmscRandomLevel(units = studyDesign$plot)
m = Hmsc(Y=Y, XData=XData, XFormula=~x,
       studyDesign=studyDesign, ranLevels=list("plot"=rL))
m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
                nChains = nChains, nParallel = nChains, verbose = verbose)
preds = computePredictedValues(m)
MF = evaluateModelFit(hM=m, predY=preds)
MF$R2 ## ~.7, keeps changing
## that is classical R2, I guess, all data used to make the model
## and all data used to evaluate

## they offer rapid cross-validation predictive R2 
partition = createPartition(m, nfolds = 2, column = "sample")

partition

## not sure how this works, but model is refit using the likelihood from half 
## the data I guess:
preds = computePredictedValues(m, partition = partition, nParallel = nChains)

## seems like this would create two diff posteriors? no way, too weird. I just don't
## understand

MF = evaluateModelFit(hM = m, predY = preds)
MF$R2 ## ~0.7 still pretty good



## spatial autorrelation can be incorporated as an additional variable

## make a random dataset with y-variable autocorrelation that does
## not correspond with autocorrelation in x:

sigma.spatial = 2
alpha.spatial = 0.5
sample.id = rep(NA,n)
for (i in 1:n){
sample.id[i] = paste0("location_",as.character(i))
}
sample.id = as.factor(sample.id)
xycoords = matrix(runif(2*n), ncol=2)
rownames(xycoords) = sample.id
colnames(xycoords) = c("x-coordinate","y-coordinate")
a = MASS::mvrnorm(mu=rep(0,n),
Sigma = sigma.spatial^2*exp(-as.matrix(dist(xycoords))/alpha.spatial))
y = L + a + rnorm(n, sd = sigma)
Y=as.matrix(y)
colfunc = colorRampPalette(c("cyan", "red"))
ncols = 100
cols = colfunc(100)
par(mfrow=c(1,2))
for (i in 1:2){
if (i==1) value = x
if (i==2) value = y
value = value-min(value)
value = 1+(ncols-1)*value/max(value)
plot(xycoords[,1],xycoords[,2],col=cols[value],pch=16,main=c("x","y")[i], asp=1)
}

studyDesign = data.frame(sample = sample.id)

## tell the model we are expecting a spatial level to our model with sData setting?
rL = HmscRandomLevel(sData = xycoords)

m = Hmsc(Y=Y, XData=XData, XFormula=~x,
         studyDesign=studyDesign, ranLevels=list("sample"=rL))

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
         nChains = nChains, nParallel = nChains, verbose = verbose)


## not sure how they hande the estimation of the spatial effects,
## need to check under the hood a bit. But also need to keep moving with this
## analysis. 

## the second tutorial: simple multivariate data:

https://cran.r-project.org/web/packages/Hmsc/vignettes/vignette_2_multivariate_low.pdf

library(Hmsc)
library(corrplot)

set.seed(1) 

## five species example:

n = 100
x1 = rnorm(n)
x2 = rnorm(n)
XData = data.frame(x1=x1,x2=x2)
alpha = c(0,0,0,0,0)
beta1 = c(1,1,-1,-1,0)
beta2 = c(1,-1,1,-1,0)
sigma = c(1,1,1,1,1)

L = matrix(NA,nrow=n,ncol=5)

Y = matrix(NA,nrow=n,ncol=5)

for (j in 1:5){
L[,j] = alpha[j] + beta1[j]*x1 + beta2[j]*x2
Y[,j] = L[,j] + rnorm(n, sd = sigma[j])
}

## build model

m = Hmsc(Y = Y, XData = XData, XFormula = ~x1+x2)

nChains = 2
thin = 10
samples = 1000
transient = 500*thin
verbose = 0

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
               nChains = nChains, nParallel = nChains, verbose = verbose)

mpost = convertToCodaObject(m)

## check for convergence 
effectiveSize(mpost$Beta)

gelman.diag(mpost$Beta, multivariate=FALSE)$psrf
## sample sizes should 

par(mfrow=c(1,2))
hist(effectiveSize(mpost$Beta), main="ess(beta)")
hist(gelman.diag(mpost$Beta, multivariate=FALSE)$psrf, main="psrf(beta)")

preds = computePredictedValues(m)

evaluateModelFit(hM = m, predY = preds)

partition = createPartition(m, nfolds = 2)

preds = computePredictedValues(m, partition = partition, nParallel = nChains)

evaluateModelFit(hM = m, predY = preds)

postBeta = getPostEstimate(m, parName = "Beta")

plotBeta(m, post = postBeta, param = "Support", supportLevel = 0.95)

## so we can pick out the individual species response to environmental parameter
## in a statistically sound way, no need for pvalue correction, etc.
## this is a great improvement...

### including species-species interations 

## biotic interactions are included as latent variables in model, using 
## a species-species covariance matrix

studyDesign = data.frame(sample = as.factor(1:n))

rL = HmscRandomLevel(units = studyDesign$sample)

m = Hmsc(Y = Y, XData = XData, XFormula = ~x1+x2,
      studyDesign = studyDesign, ranLevels = list(sample = rL))

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
       nChains = nChains, nParallel = nChains, verbose = verbose)

mpost = convertToCodaObject(m)

par(mfrow=c(2,2))
hist(effectiveSize(mpost$Beta), main="ess(beta)")
hist(gelman.diag(mpost$Beta, multivariate=FALSE)$psrf, main="psrf(beta)")
hist(effectiveSize(mpost$Omega[[1]]), main="ess(omega)")
hist(gelman.diag(mpost$Omega[[1]], multivariate=FALSE)$psrf, main="psrf(omega)")

postBeta = getPostEstimate(m, parName="Beta")
plotBeta(m, post=postBeta, param="Support", supportLevel = 0.95)
## you would expect these to be the same as above

## and but now you also have a posterior for a latent variable 
## that is describing species-species interactions

OmegaCor = computeAssociations(m)
supportLevel = 0.95
toPlot = ((OmegaCor[[1]]$support>supportLevel)
   + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean

corrplot(toPlot, method = "color",
         col = colorRampPalette(c("blue","white","red"))(200),
         title = paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))


## and there is no correlation between species abundances,
## because the data was not generated with any covariances
## among the species abundances.

## make a simpler model, drop one covariate...what happens?

m = Hmsc(Y=Y, XData=XData, XFormula=~x1,
         studyDesign=studyDesign, ranLevels=list(sample=rL))

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
               nChains = nChains, nParallel = nChains, verbose = verbose)

postBeta = getPostEstimate(m, parName="Beta")

plotBeta(m, post=postBeta, param="Support", supportLevel = 0.95)

OmegaCor = computeAssociations(m)

?computeAssociations

supportLevel = 0.95

toPlot = ((OmegaCor[[1]]$support>supportLevel)
         + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean

corrplot(toPlot, method = "color",
         col=colorRampPalette(c("blue","white","red"))(200),
         title=paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))

## the resulting checkerboard shows what happens when an important 
## predictor is left out. The residual covariance after that is left
## after the remaining environmental predictor is incorporated into 
## the latent variable that is intended to represent biotic interactions.
## So some species are covarying, but only because they both "like" 
## environmental conditions that weren't in the model, not because 
## they are in symbioses etc. This shows the limitations of this kind of analysis. 

## explanatory R2:
preds = computePredictedValues(m)

evaluateModelFit(hM = m, predY = preds)

## the latent variable is actually doing a good job of 

## predictive/cross-validated R2
## making up the missing predictor. 
oo




## but this breaks down when you split the data
## for training/prediction. 
## They don't describe this in the tutorial, but
## assume this because the latent variables 
## are given uninformative priors and the 
## likelihood isn't weighted as much with the 
## smaller amound of data/evidenc.

preds = computePredictedValues(m, partition = partition, nParallel = nChains)

evaluateModelFit(hM = m, predY = preds)

## possible to do predictions of a species, especially
## if you have data from the other species. So cross validation
## leaving one species out at a time also possible:

preds = computePredictedValues(m, partition=partition,
      partition.sp=c(1,2,3,4,5), mcmcStep=10, nParallel = nChains)


## this is mostly for when you have a small species matrix

##### Ordinations ####

rL$nfMin=2
rL$nfMax=2

m = Hmsc(Y=Y, XData=XData, XFormula=~1,
    studyDesign=studyDesign, ranLevels=list(sample=rL))
m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
    nChains = nChains, nParallel = nChains, verbose = verbose)

etaPost=getPostEstimate(m, "Eta")
lambdaPost=getPostEstimate(m, "Lambda")

biPlot(m, etaPost = etaPost, lambdaPost = lambdaPost, factors = c(1,2), "x2")

## interesting, but my results don't match the tutorial, so not sure how to 
## interpret

## essentially, the idea is similar to eigen-decomposition/PCA type 
## analysis, but the method is different. 

## they just introduced two vaguely defined variables the explain the 
## general mass of the data, without any previous hypotheses
## stated, but what are the priors and likelihoods on these? don't 
## understand. Some sort of gaussian or dirichilet process I guess. Beyond me. 

## then these two latent variables act pretty much like when we create 
## a composite variable using PCAs

## we subtract the variance we can explain from environmental conditions
## and biotic interactions by giving these to the model, and the remaining
## residual variation only is explained by these 

## you can mix up the link functions and residual models:

nChains = 2
thin = 10
samples = 1000
transient = 500*thin
verbose = 0
set.seed(2)
n = 100
x1 = rnorm(n)
x2 = rnorm(n)
alpha = c(0,0,0,0)
beta1 = c(1,1,-1,-1)
beta2 = c(1,-1,1,-1)
sigma = c(1,NA,NA,1)
XData = data.frame(x1=x1,x2=x2)
L = matrix(NA,nrow=n,ncol=4)
Y = matrix(NA,nrow=n,ncol=4)
for (j in 1:4){
  L[,j] = alpha[j] + beta1[j]*x1 + beta2[j]*x2
  }
Y[,1] = L[,1] + rnorm(n, sd = sigma[1])
Y[,2] = 1*((L[,2] + rnorm(n, sd = 1))>0)
Y[,3] = rpois(n, lambda = exp(L[,3]))
Y[,4] = rpois(n, lambda = exp(L[,4] + rnorm(n, sd = sigma[4])))

m = Hmsc(Y = Y, XData = XData, XFormula = ~x1+x2,
       distr = c("normal","probit","poisson","lognormal poisson"))

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
    nChains = nChains, nParallel = nChains, verbose = verbose)



mpost = convertToCodaObject(m)
effectiveSize(mpost$Beta)
gelman.diag(mpost$Beta, multivariate=FALSE)$psrf

preds = computePredictedValues(m, expected = FALSE)

evaluateModelFit(hM = m, predY = preds)

postBeta = getPostEstimate(m, parName="Beta")

plotBeta(m, post=postBeta, param="Support", supportLevel = 0.95)

## cool, works. But not sure how we would assign priors/likelihood models for
## all of the thousand species we're going to examine...
## maybe in the next tutorial:

## HSCMC tutorial #3:
## https://cran.r-project.org/web/packages/Hmsc/vignettes/vignette_3_multivariate_high.pdf

library(Hmsc)
library(corrplot)
library(ape)
library(MASS)
library(fields) ## for image.plot?
library(knitr)
set.seed(1)

ns = 50
## this will incorporate phylogeny, which we will randomly make up:
phy = ape::rcoal(n=ns, tip.label = sprintf('species_%.3d',1:ns), br = "coalescent")
plot(phy, show.tip.label = FALSE, no.margin = TRUE)

## model two traits - forest preference and thermal optimum
## assume there is a phylogenetic effect - more closely related 
## species are more likely to have the same preference for forest
## or more similar thermal optimum:

## make up the data so, this hurts my poor little brain:

C = vcv(phy, model = "Brownian", corr = TRUE) ## ape function for traits that are evolving
spnames = colnames(C)
traits = matrix(NA,ncol =2,nrow = ns)
## fill in the traits,  mvrnorm from MASS package
for (i in 1:2){
    traits[,i] = matrix(mvrnorm(n = 1, mu = rep(0,ns), Sigma=C))
    }
rownames(traits) = spnames
colnames(traits) = c("habitat.use","thermal.optimum")
traits = as.data.frame(traits)
par(fig = c(0,0.6,0,0.8), mar=c(6,0,2,0))
plot(phy, show.tip.label = FALSE)
par(fig = c(0.6,0.9,0.025,0.775), mar=c(6,0,2,0), new=T)
plot.new()
image.plot(t(traits),axes=FALSE,legend.width = 3,legend.shrink=1,
#imagePlot(t(traits),axes=FALSE,legend.width = 3,legend.shrink=1,
col = colorRampPalette(c("blue","white","red"))(200))
text(x=1.1, y=0.72, srt = 90, "H", cex=0.9, pos = 4)
text(x=1.4, y=0.72, srt = 90, "T", cex=0.9, pos = 4)

## neat. maybe do something similar with respiration and forest/grassland

## make some environmental and community data:

n = 200
habitat = factor(sample(x = c("forest","open"), size = n, replace=TRUE))
climate = rnorm(n)
nc = 4
mu = matrix(0,nrow=nc,ncol=ns)

#expected niche of each species related to the "covariate" intercept
mu[1, ] = -traits$thermal.optimum^2/4-traits$habitat.use
#expected niche of each species related to the covariate forest
#(open area as reference level, so included in intercept)
mu[2, ] = 2*traits$habitat.use
#expected niche of each species related to the covariate climate
mu[3, ] = traits$thermal.optimum/2
#expected niche of each species related to the covariate climate*climate
mu[4, ] = -1/4
beta = mu + 0.25*matrix(rnorm(n = ns*nc), ncol=ns)
X = cbind(rep(1,ns), as.numeric(habitat=="forest"), climate, climate*climate)
L = X%*%beta
Y = L + mvrnorm(n=n, mu=rep(0,ns), Sigma=diag(ns))
colnames(Y) = spnames

## didn't really understand all of the code, but the goal 
## is to create a community matrix of species that 
## affected by the two traits. 

Y[1:5,1:5] ## species are columns

## build a model that includes:
## the climate and habitat data, 
## a climate^2 term (allows for unimodal, non-linear climate niche curve)
## also a term for the phylogenetic signal

XData = data.frame(climate = climate, habitat = habitat)
XFormula = ~habitat + poly(climate,degree = 2,raw = TRUE)
TrFormula = ~habitat.use + thermal.optimum
studyDesign = data.frame(sample = sprintf('sample_%.3d',1:n), stringsAsFactors=TRUE)
rL = HmscRandomLevel(units = studyDesign$sample)
rL$nfMax = 15
m = Hmsc(
                Y = Y, 
            XData = XData, 
         XFormula = XFormula,
           TrData = traits, 
        TrFormula = TrFormula,
        phyloTree = phy,
      studyDesign = studyDesign, 
        ranLevels = list(sample = rL))


## not run - apparently this will take 2 hours.
## lets set up the labcomputer for this.

m = Hmsc(Y = Y, XData = XData, XFormula = XFormula,
         TrData = traits, TrFormula = TrFormula,
         phyloTree = phy,
         studyDesign = studyDesign, ranLevels = list(sample = rL))

nChains = 2
thin = 10
samples = 1000
transient = 500
verbose = 0

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
               nChains = nChains, nParallel = nChains, verbose = verbose)

## that actually ran for like ten minutes

## so now we have posterior probability for the community

## they look for four parameters:

## species niches (environmental predictors of species abundance)
## influence of traits on species niches (organismal traits predicting their response to environment)
## species-species interactions 
## effect of phylogeny

## the omega (s x s) matrix is 50 x 50 x 2500 elements. The 2500 is from the traces of the posterior, I think
## but I thought that part of the point of latent variables was to avoid building this massive matrix?

## anyway...figure out later. They subset to one 100 random species pairs

## check convergences

mpost = convertToCodaObject(m)

par(mfrow=c(3,2))
ess.beta = effectiveSize(mpost$Beta)
psrf.beta = gelman.diag(mpost$Beta, multivariate=FALSE)$psrf
hist(ess.beta)
hist(psrf.beta)
ess.gamma = effectiveSize(mpost$Gamma)
psrf.gamma = gelman.diag(mpost$Gamma, multivariate=FALSE)$psrf
hist(ess.gamma)
hist(psrf.gamma)
sppairs = matrix(sample(x = 1:ns^2, size = 100))
tmp = mpost$Omega[[1]]

for (chain in 1:length(tmp)){
tmp[[chain]] = tmp[[chain]][,sppairs]
}

ess.omega = effectiveSize(tmp)
psrf.omega = gelman.diag(tmp, multivariate=FALSE)$psrf
hist(ess.omega)
hist(psrf.omega)

print("ess.rho:")
effectiveSize(mpost$Rho)

print("psrf.rho:")
gelman.diag(mpost$Rho)$psrf

## convergence diagnostics look fine

## overall performance of the model, explanatory.
## for this, check the R2 on average for all 
## species predictions:

preds = computePredictedValues(m)
MF = evaluateModelFit(hM=m, predY=preds)

hist(MF$R2, xlim = c(0,1), main=paste0("Mean = ", round(mean(MF$R2),2)))

## hovering around .70  not bad

m$X

head(m$X)


## we can use our model to do Variance Partitioning:

VP = computeVariancePartitioning(m, group = c(1,1,2,2), groupnames = c("habitat","climate"))

?computeVariancePartitioning

plotVariancePartitioning(m, VP = VP)

kable(VP$R2T$Beta)

## we can get back to the environmental predictors, on a per-species basis: 
postBeta = getPostEstimate(m, parName = "Beta")

plotBeta(m, post = postBeta, param = "Support",
plotTree = TRUE, supportLevel = 0.95, split=.4, spNamesNumbers = c(F,F))

postGamma = getPostEstimate(m, parName = "Gamma")

plotGamma(m, post=postGamma, param="Support", supportLevel = 0.95)

OmegaCor = computeAssociations(m)

## what are these these objects?

str(OmegaCor)

str(OmegaCor[[1]][1])

## significance is judged by a "support" metric, not sure what this is
str(OmegaCor[[1]][2])

supportLevel = 0.95 ## what is this? some sort of credible interval?

toPlot = ((OmegaCor[[1]]$support>supportLevel)
        + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean

corrplot(toPlot, method = "color",
         col=colorRampPalette(c("blue","white","red"))(200),
         tl.cex=.6, tl.col="black",
         title=paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))

## no associations, because synthetic data, not included

summary(mpost$Rho)

## we can predict what a community might do over an environmental gradient:

Gradient = constructGradient(m,focalVariable = "climate",
                   non.focalVariables = list("habitat"=list(3,"open")))

Gradient$XDataNew

predY = predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew,
                ranLevels=Gradient$rLNew, expected=TRUE)

plotGradient(m, Gradient, pred=predY, measure="S", showData = TRUE)


## individual species response can be modeled:

par(mfrow=c(1,2))
plotGradient(m, Gradient, pred=predY, measure="S", index = 1, showData = TRUE)
plotGradient(m, Gradient, pred=predY, measure="Y", index = 2, showData = TRUE)

## trait values of the group changing with a gradient:
plotGradient(m, Gradient, pred=predY, measure="T", index = 3, showData = TRUE)


## "gradients" can be constructed for categorical variables?:

Gradient = constructGradient(m,focalVariable = "habitat",
                             non.focalVariables = list("climate"=list(1)))

Gradient$XDataNew

## ah, I get it. This is like a parameter sweep, where we hold everything else
## constant and just change the variable of interest.

## so in this case we have a trait for "preference for forest". We can select the species most 
## responsive to habitat and watch its response to being in either a forest or meadow:

predY = predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew,
                ranLevels=Gradient$rLNew, expected=TRUE)

plotGradient(m, Gradient, pred=predY, measure="Y", index=which.max(m$TrData$habitat.use),
             showData = TRUE, jigger = 0.2)

## predicted to drop, make sense.

## and we can see how the community mean for this trait will drop with a habitat-change:

plotGradient(m, Gradient, pred=predY, measure="T", index=2, showData = TRUE, jigger = 0.2)

## same story. The number of microbes with the "forest-preference trait" will increase
## when the community is subject to forests.

## to get closer to reality, they give the example of trying to model the simulated 
## community without all the predictors that were used to make the data
## so it's like real life, when we are hypothesizing what is driving the community
## dynamics, but don't really know.

## for instance, trying to explain the community composition with climate only:

XFormula.1 = ~poly(climate, degree = 2, raw = TRUE)
ma50 = Hmsc(Y=Y, XData=XData, XFormula = XFormula.1,
            TrData = traits, TrFormula = TrFormula,
            phyloTree = phy,
            studyDesign=studyDesign, ranLevels=list(sample=rL))

ma50 = sampleMcmc(ma50, thin = thin, samples = samples, transient = transient,
            nChains = nChains, nParallel = nChains, verbose = verbose)

print ("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! done !!!!!!!!!!!!!!!!")
print ("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! done !!!!!!!!!!!!!!!!")
print ("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! done !!!!!!!!!!!!!!!!")

## and so we have a lot more variance from randome effects

VP = computeVariancePartitioning(ma50, group = c(1,1,1), groupnames=c("climate"))
plotVariancePartitioning(ma50, VP = VP)

## which is now ascribed partially due to species associations, because 

OmegaCor = computeAssociations(ma50)

supportLevel = 0.95
toPlot = ((OmegaCor[[1]]$support>supportLevel)
  + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean

corrplot(toPlot, method = "color",
col=colorRampPalette(c("blue","white","red"))(200),
tl.cex=.6, tl.col="black",
title=paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))


## and other stuff about traits, controlling shrinkage on the hierarchical model 
## etc, etc. But generally, got the idea. 

## there is another tutorial, about spatial datasets. Seems pertinent:

library(Hmsc)
library(MASS)

set.seed(6)

## 100 sites, 5 species:

n = 100
ns = 5
beta1 = c(-2,-1,0,1,2)
alpha = rep(0,ns)
beta = cbind(alpha,beta1)
x = cbind(rep(1,n),rnorm(n))
Lf = x%*%t(beta)
xycoords = matrix(runif(2*n),ncol=2)
colnames(xycoords) = c("x-coordinate","y-coordinate")
rownames(xycoords) = 1:n

## exponentially decreasing autocorrelation model:
sigma.spatial = c(2)
alpha.spatial = c(0.35)
Sigma = sigma.spatial^2*exp(-as.matrix(dist(xycoords))/alpha.spatial)
eta1 = mvrnorm(mu=rep(0,n), Sigma=Sigma)
lambda1 = c(1,2,-2,-1,0) ## species spatial residuals
Lr = eta1%*%t(lambda1) ##  
L = Lf + Lr ## linear function of effects of environmental and spatial function
y = as.matrix(L + matrix(rnorm(n*ns),ncol=ns))
yprob = 1*((L +matrix(rnorm(n*ns),ncol=ns))>0)
XData = data.frame(x1=x[,2])

rbPal = colorRampPalette(c('cyan','red'))
par(mfrow=c(2,3))
Col = rbPal(10)[as.numeric(cut(x[,2],breaks = 10))]
plot(xycoords[,2],xycoords[,1],pch = 20,col = Col,main=paste('x'), asp=1)
for(s in 1:ns){
    Col = rbPal(10)[as.numeric(cut(y[,s],breaks = 10))]
    plot(xycoords[,2],xycoords[,1],pch = 20,col = Col,main=paste('Species',s), asp=1)
    }

## this diagram is useful for showing that autocorrelation is hard to pick up by eye 
## sometimes

####

## make a model spatial by adding a random effect with a spatial argument
 
studyDesign = data.frame(sample = as.factor(1:n))
rL.spatial = HmscRandomLevel(sData = xycoords) ## here
rL.spatial = setPriors(rL.spatial,nfMin=1,nfMax=1) #We limit the model to one latent variables for visualization
m.spatial = Hmsc(Y=yprob, XData=XData, XFormula=~x1,
                 studyDesign=studyDesign, ranLevels=list("sample"=rL.spatial),distr="probit")

## sample:
nChains = 2
thin = 10
samples = 1000
transient = 1000
verbose = 1

m.spatial = sampleMcmc(m.spatial, thin = thin, samples = samples, transient = transient,
                       nChains = nChains, nParallel = nChains, verbose = verbose,
                       updater=list(GammaEta=FALSE))

## they skip checking the convergences

#Explanatory power

preds.spatial = computePredictedValues(m.spatial)

MF.spatial = evaluateModelFit(hM=m.spatial, predY=preds.spatial)

?evaluateModelFit

MF.spatial

partition = createPartition(m.spatial, nfolds = 2, column = "sample")

cvpreds.spatial = computePredictedValues(m.spatial, partition=partition,
                          nParallel = nChains, updater=list(GammaEta=FALSE))

mpost.spatial = convertToCodaObject(m.spatial)

plot(mpost.spatial$Alpha[[1]])

mpost.spatial = convertToCodaObject(m.spatial)

plot(mpost.spatial$Alpha[[1]])

summary(mpost.spatial$Alpha[[1]])

## try the model without the spatial component (so no random effects)
m = Hmsc(Y=yprob, XData=XData, XFormula=~x1, studyDesign = studyDesign, distr="probit")

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
                nChains = nChains, nParallel = nChains, verbose = verbose)


preds = computePredictedValues(m)

MF = evaluateModelFit(hM=m, predY=preds)

MF

partition = createPartition(m, nfolds = 2, column = "sample")
preds = computePredictedValues(m, partition=partition, nParallel = nChains)

MF = evaluateModelFit(hM=m, predY=preds)

MF



### ok great...anything else we need to know before we dive into the real data?

## don't think so. 

## first step, what format do we want our data in?

## should use the >50, log-transformed data. 

## we have a lot of predictors. Model might get too
## complex. 

## maybe start without spatial effects, find best predictors,
## then include the spatial effects

####### try out HMSC on sulari data ##############

## still on lab comp

R

library(phyloseq)
library(vegan)

library(Hmsc)
library(corrplot)
library(ape)
library(MASS)
library(fields) 
library(knitr)

## sulari  community data:
comData <- read.csv("../sulariData/comdat.csv", row.names=1)
envData <- read.csv("sulariEnv.csv", row.names=1)

tail(comData)[,1:5]
tail(envData)[,1:5]

## let's just start working through the high-dim tutorial, using our 
## data. I think we need a 16s tree of our sequences...

## we need to get the reduced ~5000 ASV sequences out into fasta form:


load("../sulariData/sularilogMin50ps.rda")
## get rid of zero columns
logMin50ps = prune_taxa( taxa_sums(logMin50ps) > 0, logMin50ps )

## export the sequences as a fasta file for the aligner:

?Biostrings::writeXStringSet

Biostrings::writeXStringSet(refseq(logMin50ps), "sulariAbundantASV16s.fna", append=FALSE,
                                  compress=FALSE, compression_level=NA, format="fasta")

## align them with ssu-aligner:

## try it on the local desktop

conda activate spatialDirt

#conda install bioconda::ssu-align

## align these with SSU align
## as per https://www.biostars.org/p/11377/, 
## to produce a single tree from both archea and bact,
## we need to designate a single model:

ssu-align -n bacteria sulariAbundantASV16s.fna sulariAbundantASV16s_ali
## then a strict mask, don't trust any ambiguous calls
ssu-mask --pf 0.9999 --pt 0.9999 sulariAbundantASV16s_ali

## to get a fasta output of the alignment (what we probably need)
ssu-mask --stk2afa sulariAbundantASV16s_ali

ssu-mask --stk2afa sulariAbundantASV16s_ali

grep ">" sulariAbundantASV16s_ali/sulariAbundantASV16s_ali.bacteria.afa | wc -l ## 4300 sequences
grep ">" sulariAbundantASV16s.fna | wc -l ## 4370 sequences

## we lost 70 sequences. Probably all of our archea. 
## not sure 
## not sure if the HMSC can handle missing asvs
## we can try anyway. 

## use fastree? phyml?

## start with phyml:

## but our alignment 

phyml -i allSeqsForGrahamTree_trimmed.phy -b 100

ssu-mask -h

sudo apt-get install libboost-program-options

sudo apt-get install libboost-program-options-dev

conda activate spatialDirt

conda install -c bioconda sina

sina -h

sudo ln -s /usr/lib/x86_64-linux-gnu/libboost_program_options.so.1.71.0 /usr/lib/x86_64-linux-gnu/libboost_program_options.so.1.54.0
sudo ln -s /usr/lib/x86_64-linux-gnu/libboost_thread.so.1.71.0 /usr/lib/x86_64-linux-gnu/libboost_thread.so.1.54.0
sudo ln -s /usr/lib/x86_64-linux-gnu/libboost_system.so.1.71.0 /usr/lib/x86_64-linux-gnu/libboost_system.so.1.54.0

sina -h

ls /opt/ont/dorado/lib/libnvToolsExt.so.1

## tree building... 

conda remove sina

phy = ape::rcoal(n=50, tip.label = sprintf('species_%.3d',1:50), br = "coalescent")


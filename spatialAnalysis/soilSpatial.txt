## git our local copy of the repo in order for office comp.
## working off the work tower, so need to get git synced up now to
## avoid confusion.

## we need RSA with SHA-2 signature algorithm

man ssh-keygen
ssh-keygen -t rsa -f fuj2git

## now, what do we need to get the push functionality...

git clone https://github.com/danchurch/fichtelgebirgeSoils.git

## test

touch thisIsNotReal.txt

## and of course can't push

git config --global user.email "danchurchthomas@gmail.com"
git config --global user.name "danchurch"

git remote add origin https://github.com/danchurch/fichtelgebirgeSoils.git
git branch -M main
git remote set-url origin git@github.com:danchurch/fichtelgebirgeSoils.git
git push -u origin main

## and we're in business with github

## maybe let's get a conda environment going for this.

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash

conda config --set auto_activate_base false

## get the mamba solver:
conda update -n base conda
conda install -n base conda-libmamba-solver
conda config --set solver libmamba

## get the standard channels
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

## this new conda env comes with python3.12

## let's see if this works for our spatial analysis


conda activate
conda create -n "spatialDirt" 
 
## let's think about spatial turnover in Sulari's community data

## first step would be to get a map. 

## we want to see where we sample, and visualize respiration 
## values across the landscape

conda deactivate

conda remove -n spatialDirt --all

conda create -n "spatialDirt" 

conda activate "spatialDirt" 

conda config --env --add channels conda-forge
conda config --env --set channel_priority strict

conda install python=3 geopandas

pip install rasterio

## we also need to be R up to speed...
## maybe do this outside of conda

conda deactivate

sudo R 

install.packages("BiocManager")
BiocManager::install("phyloseq")


## I think that took care of most of the complex installs

## oh wait, let's get the jupyter notebook setup going...

## how do we make sure that the jupyter behaves, stays in the 
## right python?

conda activate spatialDirt 

pip install notebook 

which jupyter ## looks like that work. Gets easier every year.

## and it looks like it is even keeping the R kernel from 
## my general environment. 

## to get a bash kernel on there? https://github.com/takluyver/bash_kernel

pip install bash_kernel
python -m bash_kernel.install

##### bayesian setup #####

## last time we worked with pymc3 we needed
## a separate conda env. Let's see if 
## things have gotten better. 

## first back up the env, just in case

conda activate spatialDirt

conda env export > spatialDirt.yml

## installing bambi should also install pymc, so
## try the bambi conda installs as per: 
https://github.com/bambinos/bambi#quickstart

pip install bambi

pip install "preliz[full,lab]"

## get the data from the newest martin book.

## put outside our repo

cd /home/daniel/Documents/manualsBooks/bayesian
git clone https://github.com/aloctavodia/BAP3.git

## seems like that worked in our spatialDirt environment

## try everything out for a bit, then update yaml backup for the repo


conda activate spatialDirt 

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
from matplotlib_scalebar.scalebar import ScaleBar
import pymc as pm


spatDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
## ^different on laptop
os.chdir(spatDir)

## we just want a map of points right now:

## on officeComp
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"
## on laptop
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"

envData = pd.read_csv(sulariEnvCSV)
## get rid of spaces
envData.rename({"Sample ID":"SampleID"}, axis="columns", inplace=True)
## we need to clean up the plot.ID. Sulari recorded season in the plot IDs,
## using letter codes. Also she has one double sampling, with an underscore
envData['Plot.ID'] = envData['Plot.ID'].str.slice(0,5)

## let's get rid of decimals in the names, weird for python work:
envData.rename({"Plot.ID":"PlotID", 
       "soil.respiration":"soil_respiration",
              "Land.type":"Land_type"}, 
                axis="columns", inplace=True)

envData.head()

## so we don't have to repeat:
envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index=False)

envData.head()


envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
 

## if we need to drop controls
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

## her otu table is really large:
##### R ###
library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")
logMin50ps
comdat <- as.data.frame(otu_table(logMin50ps))
write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")
############

## back in python

comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", index_col=0)

comData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

comData.head()

## the map below looks funny...looks like every forest plot has 
## a grassland under it...

dupsFilter = envData.Latitude.duplicated()
dupped = envData[dupsFilter].sort_values(by="Latitude")

dupped.groupby('Plot.ID').nunique()

envData.iloc[0:5,0:8]

## nope...looks okay

## we want a geodf...

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

## to convert to UTM? looks like we are in zone 33
## looks like:
## EPSG:32633

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

sulariPlot_utm.tail()

## can we import our georeferenced tif of the study area?
## lat/lon
#fichtelMap = rasterio.open("studyAreaClipped_modified.tif")
## UTM

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color code our points according to land type?
cdik={
"Arableland":"b",
"Grassland":"y",
"Forest":"g",
}
sulariPlotsDF['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]
sulariPlot_utm['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]

fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    facecolor=sulariPlotsDF['landColors'],
    markersize=400) 


grassPatch = Patch(color='y', label='grassland',)
forestPatch = Patch(color='g', label='forest')
farmPatch = Patch(color='b', label='arable land')
ax.legend(handles=[grassPatch, forestPatch, farmPatch], 
          loc="lower left",
          fontsize=15,
)

## if we want to compare just grassland and forest

plt.close('all')
onlyGrassForest = sulariPlot_utm[sulariPlot_utm['Land.type'].apply(lambda x: x in ["Forest", "Grassland"])]
fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
onlyGrassForest.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    linewidths=2,
    facecolor=onlyGrassForest['landColors'],
    markersize=200) 
ax.ticklabel_format(style='plain', axis='y', useOffset=False)
grassPatch = Patch(color='y', label='grassland',)
forestPatch = Patch(color='g', label='forest')
ax.legend(handles=[grassPatch, forestPatch], loc='lower left')
ax.add_artist(ScaleBar(1, location='lower right')) 
ax.set_xlim([265500, 286930])
ax.set_ylim([5547227, 5570000])
plt.savefig('forestVsGrasslandMapUTM.png', dpi=600, format='png')

## Look at the turnover data:

## lat/long
aa = pd.DataFrame({'xx':envData.Longitude, 'yy':envData.Latitude})
physDist = sp.distance.pdist(aa, metric='euclidean')
bcDist = sp.distance.pdist(comData, metric='brayCurtis')
fig, ax = plt.subplots()
ax.scatter(physDist, bcDist)
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title(label="Turnover with Lat/Lon", loc='center')

## utms
aa = pd.DataFrame({'xx':sulariPlot_utm.geometry.x, 'yy':sulariPlot_utm.geometry.y})
physDist = sp.distance.pdist(aa, metric='euclidean')
bcDist = sp.distance.pdist(comData, metric='brayCurtis')
fig, ax = plt.subplots()
ax.scatter(physDist, bcDist)
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title(label="Turnover with UTM", loc='center')

plt.close('all')
## subset by landtype
for lt in [ "Arableland" ,"Grassland" ,"Forest"]: 
    print(lt)
    edf = envData[envData['Land.type'] == lt]
    cdf = comData.loc[edf.index]
    aa = pd.DataFrame({'xx':edf.Longitude, 'yy':edf.Latitude})
    aa = aa.iloc[0:120,:]
    physDist = sp.distance.pdist(aa, metric='euclidean')
    bcDist = sp.distance.pdist(cdf, metric='brayCurtis')
    fig, ax = plt.subplots()
    ax.scatter(physDist, bcDist)
    ax.set_title(lt)
    ax.set_title(label= (lt + " in degrees"), loc='center')
    X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
    ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')

## well that looks pretty much like I hypothesized
## good stuff.

sulariPlot_utm.head()

plt.close('all')
plt.rc('ytick', labelsize=15)
plt.rc('xtick', labelsize=15)
lts = [ "Arableland" ,"Grassland" ,"Forest"]
#lts = [ "Grassland" ,"Forest"]
fig, axes = plt.subplots(nrows=1, ncols=len(lts), sharey=True)
axes = axes.flatten()
for nu,lt in enumerate(lts):
    edf = sulariPlot_utm[sulariPlot_utm['Land.type'] == lt]
    cdf = comData.loc[edf.index]
    aa = pd.DataFrame({'xx':edf.geometry.x, 'yy':edf.geometry.y})
    physDist = sp.distance.pdist(aa, metric='euclidean')
    bcDist = sp.distance.pdist(cdf, metric='brayCurtis')
    axes[nu].scatter(physDist, bcDist)
    X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
    linMod =  LinearRegression().fit(X, Y)
    axes[nu].plot( X, linMod.predict(X), c='k')
    axes[nu].set_title(label=lt, size=20, loc='center')
    axes[nu].set_xlabel('meters', size=20)
    print(lt, stats.linregress(physDist,bcDist))

fig.suptitle("Turnover in prokaryotic community", size=40)
axes[0].set_ylabel('Bray-Curtis dissimilarity', size=20)
axes[1].tick_params(left=False, labelleft=False, right=True, labelright=True, color='red', axis='y')
plt.subplots_adjust(wspace = 0)


###################################
##
## outputs from stats.regress:
##
## Arable Land
## slope=2.810886008879358e-06
## intercept=0.5742255266887248
## rvalue=0.07906122967379002
## pvalue=0.02033328808278514
## stderr=1.209265073980233e-06
## intercept_stderr=0.013364930035836518
## 
## Grassland
## slope=4.135586933082137e-07
## intercept=0.5590568736859212
## rvalue=0.012708447690178782
## pvalue=0.7298157896065798
## stderr=1.1969812747713313e-06
## intercept_stderr=0.013504959623407586
## 
## Forest
## slope=5.843245351182221e-06
## intercept=0.6024952517397398
## rvalue=0.20458396890349276
## pvalue=1.918646699083231e-08
## stderr=1.0284330215725841e-06
## intercept_stderr=0.01259259776931045
######################################

## add in the correlation coefficients and pvalues to grant app graphic. 

###### SAC curves ##########

## we have to stop avoiding gamma diversity calculations...

## do this in vegan? why not.

R

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
setwd(spatDir)


library(vegan)
library(phyloseq)

comM <- read.csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv', 
                    row.names=1)

## why is this so big, btw?

sum(colSums(comM) > 0) ## 4363. Why do we have a bunch of empty colums? I think these were low abundance ASVs, below our cutoffs.

## get rid of them to save memory:

library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")

logMin50ps

comdat <- as.data.frame(otu_table(logMin50ps))
comdat = comdat[,colSums(comdat) > 0] 

write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")


## get rid of controls
notControls=!(row.names(comM) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comM = comM[notControls,]

comM[1:4,1:4]


sp1 <- specaccum(comM)

plot(sp1, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue")

specpool(comM)

sp2 <- specaccum(comM, "random")

summary(sp2)

plot(sp2, ci.type="poly", col="red", lwd=2, ci.lty=0, ci.col="pink")


data(BCI)

sp1 <- specaccum(BCI)

sp2 <- specaccum(BCI, "random")

sp2

summary(sp2)

plot(sp1, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue")
boxplot(sp2, col="yellow", add=TRUE, pch="+")
## Fit Lomolino model to the exact accumulation
mod1 <- fitspecaccum(sp1, "lomolino")
coef(mod1)
fitted(mod1)
plot(sp1)

aa <- specaccum(comM, method = "exact")

?specaccum

anaSAC <- data.frame(aa$richness, aa$sd)
colnames(anaSAC) <- c('richness', 'sd')
anaSpeciesEstimators = specpool(comM)
print(anaSpeciesEstimators)
 
## okay, but we need to separate out by land types.
## wish we were in python...

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
setwd(spatDir)
library(vegan)
library(phyloseq)

comData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", row.names=1)
envData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv", row.names=1)

all(row.names(comData) == row.names(envData))

lt <- 'Forest'
justThisLandtype=row.names(envData[envData['Land_type'] == lt,])
aa <- comData[ justThisLandtype,]
sp1 <- specaccum(aa)
plot(sp1, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue")


## so loop this:

for (lt in c('Arableland','Grassland', 'Forest')){
    print(lt)
    justThisLandtype=row.names(envData[envData['Land_type'] == lt,])
    comm.i <- comData[ justThisLandtype,]
    specAccum.i <- specaccum(comm.i)
    SACdf.i <- data.frame(specAccum.i$richness, specAccum.i$sd)
    colnames(SACdf.i) <- c('richness', 'sd')
    speciesEstimators.i = specpool(comm.i)
    print(speciesEstimators.i)
    write.csv(SACdf.i, file=paste(lt, "SAC.csv", sep="_"))
    write.csv(speciesEstimators.i, file=paste(lt, "specEst.csv", sep="_"))
}

## interesting, this is pretty much exactly what Brendan found
## in the amazon. Despite lower alpha diversity, higher beta 
## diversity in forest soils. 
## and this equates to a higher total diversity across the
## survey (gamma). 

## take over to python for plotting

## we have an old function for this, wonder if it still works:


os.chdir("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis")

sacs = [ i for i in os.listdir() if "_SAC.csv" in i  ]

def plotSACs(habtype, color='black', ax=None):
    if ax is None: fig, ax = plt.subplots()
    sacs = [ i for i in os.listdir() if "_SAC.csv" in i  ]
    sacName = (habtype +'_SAC.csv')
    assert( (habtype +'_SAC.csv') in sacs)
    specEstName = (habtype + "_specEst.csv")
    sac_i = pd.read_csv(sacName, index_col=0)
    specEst_i = pd.read_csv(specEstName, index_col=0).loc['All']
    specEst_i.index = specEst_i.index.str.replace(".","_")
    X = sac_i.index
    ax.plot(X, sac_i['richness'], color=color)
    ax.fill_between(x=X,
                     y1=sac_i.richness - sac_i.sd,
                     y2=sac_i.richness + sac_i.sd,
                    alpha=0.4,
                    color=color,
                    )

plt.close('all')
fig, ax = plt.subplots(figsize=(10,10))
plotSACs('Arableland', ax=ax, color='#862d2d')
plotSACs('Forest', ax=ax, color='#006600')
plotSACs('Grassland', ax=ax, color='#FF7F00')

Arableland_patch = Patch(color='#862d2d', label='Arableland', alpha=0.4)
Forest_patch = Patch(color='#006600', label='Forest', alpha=0.4)
Grassland_patch = Patch(color='#FF7F00', label='Grassland', alpha=0.4)

ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])
ax.set_title('Species accumulution curves by\nland-use/Habitat')
ax.set_xlabel('Sites sampled')
ax.set_ylabel('Prokaryotic ASVs')


[Arableland_patch, Forest_patch, Grassland_patch]

## repeat alpha diversity
## using our >50 reads OTU table, can we calculate alpha diveristy by land type?
## back to old fashioned vegan/R

## we just want species richness. So we need to rarefy and compare 
## forest v. farm v. grassland data

notControls=!(row.names(comData) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comData = comData[notControls,]
envData = envData[notControls,]

data(BCI)

S <- specnumber(BCI) # observed number of species

S <- specnumber(comData) # observed number of species

## pretty much same as:
aa <- comData
aa[aa > 0] <- 1
rowSums(aa)

S <- specnumber(comData) # observed number of species

(raremax <- min(rowSums(BCI)))

Srare <- rarefy(BCI, raremax)

plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")

abline(0, 1)

rarecurve(BCI, step = 20, sample = raremax, col = "blue", cex = 0.6)


## so we are looking for a rarified species richness for each site.

## from this we will generate 3 mean +/- error values of species richness 
## one for each land use.

## this kind of analysis is really ok for count data.
## we've done all kinds of transformations, to try 
## to reduce sequencer error. 

## so I think we need to back up to phyloseq, to use our 
## sequencing depth information

library(phyloseq)


## transformed:
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")

## not transformed:
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

logMin50ps

(p = plot_richness(ps, x = "Land.type"))

estimate_richness(ps)

estimate_richness

## but I am thinking about this incorrectly. 
## these richness estimates are way high, 
## because of PCR, sequencer error, etc. 

## we attempting to reign in these errors 
## a bit through our transformations, let's
## honor this. 

## so back and use our communty matrix, 

savehistory("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/latelyInR.txt")

## to make it into "count data", multiply 
## to get rid of decimals:


sum(comData < .001 & comData > 0) ## 508 observations smaller than .001.
sum(comData < .0001 & comData > 0) ## 0 observations, so let's multiply by 10000
comDataFakeCounts = ceiling(comData * 10000) 
min(comDataFakeCounts[ comDataFakeCounts > 0 ]) ## our smallest non-zero observation is 8 

S <- specnumber(comDataFakeCounts) # observed number of species

(raremax <- min(rowSums(comDataFakeCounts))) ## 10001

rowSums(comDataFakeCounts)

Srare <- rarefy(comDataFakeCounts, raremax) ## this is what we need. 

## this is an estimate of how many species are present in each 
## sample, after coming down to a minimum abundance


## kind of interesting but not useful. Shows we sequenced deeply enough:
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")

## can we do all that without transforming to "counts"?
S2 <- specnumber(comData) # observed number of species, same as fake counts

(raremax2 <- min(rowSums(comData))) ## .9896

rowSums(comData)



## compare to:

bb <- specnumber(comData) # observed number of species
all(bb == Srare) ## yes. the same
## So I guess I am brilliant, I just reinvented their command. 
## big waste of time. 

## maybe also shows there isn't really a need to rarefy, at least
## on the transformed data

## not sure, but now let's trust these numbers.

## now, subset by land type, and get means?

head(envData)

all(row.names(envData) == row.names(comData))

## I'd guess we need a vector of group names (by land_type):
hist(Srare, 20) ## looks more or less normal.
mean(Srare) ## 301.3833
sd(Srare) ## 26.4

print("mean alpha diversity of all sites = ", mean(Srare))

print(paste("mean alpha diversity of all sites =", mean(Srare), "ASVs"))

cat(paste("mean alpha diversity of all sites =", mean(Srare), "ASVs"))

cat(paste("mean alpha diversity of all sites =", mean(Srare), "+/-", round(sd(Srare)), "ASVs"))


all(names(Srare) == row.names(envData))

## break this down by groups:

tapply(Srare, envData$Land_type, mean)
tapply(Srare, envData$Land_type, sd)

boxplot(Srare ~ envData$Land_type)

## anova
res.aov <- aov(Srare ~ envData$Land_type)
summary(res.aov) ## F= 2.85, p = 0.0618

## so maybe differences in species richness due to land type,
## maybe not.  Not a large effect, anyway. 
## update notebook, give it a break.

## t-test for difference between forest and grassland?

## to remove the farm samples 

noFarms <- envData$Land_type != "Arableland"

envData$Land_type[noFarms]
Srare[noFarms]


head(envData)

t.test(Srare[noFarms] ~ envData$Land_type[noFarms])

t_test(weight ~ group)

## we should probably do this in a bayesian way...
## get the ordinations done, then order the new book, 
## PCNMs can also be started without tests.

## but generally not sure how to handle the multivariate
## tests in a bayesian way. 

## two possibilities:
## BetaBayes: https://doi.org/10.3390/d14100858, somehow related to GDS
## BERA: https://doi.org/10.1080/00273171.2019.1598837
## BERA is apparently related to RDA. More reading is in order on both. 

## in the meantime...re-run the ordinations:

## the usual pipeline
## let's get ordinations with vegan and plot with python 

library(vegan)

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
setwd(spatDir)

comData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", row.names=1)
envData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv", row.names=1)
## get rid of controls
notControls=!(row.names(comData) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comData <- comData[notControls,]
envData <- envData[notControls,]

comData[1:4,1:4]

comNMS <- metaMDS(comData, try=40)

write.csv(comNMS$points, file='comNMS.csv')

## check this out in python:

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import matplotlib.colors 
import os
import scipy.spatial as sp
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
import pymc as pm
import preliz as pz
import arviz as az
import rasterio.plot

## data
nmsPts = pd.read_csv("comNMS.csv", index_col=0)
sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 
controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
envData.drop(controls, axis='rows', inplace=True)
comData.drop(controls, axis='rows', inplace=True)

## need some colors for land type
colorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
landCols = [ colorDict[i] for i in envData['Land_type'] ]
plt.close('all')
fig, ax = plt.subplots()
ax.scatter(x=nmsPts["MDS1"],
           y=nmsPts["MDS2"], 
           c=landCols,
          )
Arableland_patch = Patch(color='#862d2d', label='Arableland')
Forest_patch = Patch(color='#006600', label='Forest')
Grassland_patch = Patch(color='#FF7F00', label='Grassland')
ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])

## try markers for seasonality?
seasonDict = {
     'S': "o", 
    'SP': "v", 
    'W1': "D", 
    'W2': "P", 
     'A': "s",
}

## we might also check pH, and seasonality, and microbial biomass

## seasonality:
seasonShapes = [ seasonDict[i] for i in envData['season'] ]

## matplot lib doesn't change markers on the fly...
## if we want to change markers for each season:

plt.close('all')
fig, ax = plt.subplots()
for i in envData.season.unique():
  print(i)
  env_i = envData[envData['season'] == i]
  plots_i = env_i.index.to_list()
  nmsPts_i = nmsPts.loc[plots_i]
  cols_i = [ colorDict[i] for i in env_i['Land_type'] ]
  ax.scatter(x=nmsPts_i["MDS1"],
             y=nmsPts_i["MDS2"], 
             c=cols_i,
        marker=seasonDict[i],
            )

## I don't see any evidence of seasonality affecting these
## community structures

## ph ordinations

## color by pH, land by symbol, respiration by size

landTypeShapesDict = {
'Arableland': "o", 
'Forest'    : "v", 
'Grassland' : "s", 
}

pHmin = envData['pH'].min() ## 3.647
pHmax = envData['pH'].max() ## 7.312
norm=matplotlib.colors.Normalize(pHmin, pHmax)
plt.close('all')
fig, ax = plt.subplots()
for i in envData.Land_type.unique():
  print(i)
  env_i = envData[envData['Land_type'] == i]
  plots_i = env_i.index.to_list()
  nmsPts_i = nmsPts.loc[plots_i]
  sizes = env_i['soil_respiration']*50
  ax.scatter(x=nmsPts_i["MDS1"],
             y=nmsPts_i["MDS2"], 
             #s=140,
             s=sizes,
             c=env_i['pH'],
             cmap='Spectral',
             edgecolors='black',
             marker=landTypeShapesDict[i],
             norm=norm,
            )

fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.Normalize(pHmin, pHmax), cmap='Spectral'),
             ax=ax, orientation='vertical', label='pH')


## this is a good graph. needs a legend. 

## as usual, legends are beyond my ability. Do them manually later if we want the figure.


### test out bayesian setup, try comparison of two groups ###

dist = pz.Beta()
pz.maxent(dist, 0.1, 0.7, 0.9)

## test out the pymc3 setup:
np.random.seed(123)
trials = 4
theta_real = 0.35
data = pz.Binomial(n=1, p=theta_real).rvs(trials)

np.random.seed(123)
trials = 4
theta_real = 0.35
data = stats.bernoulli.rvs(p=theta_real, size=trials)

with pm.Model() as our_first_model:
    θ = pm.Beta('θ', alpha=1., beta=1.)
    γ = pm.Bernoulli('γ', p=θ, observed=data)
    trace = pm.sample(1000, random_seed=123)

## works. I can hardly remember how to do this bayesian stuff,
## but we can work from old examples....

## let's redo our only statistical model/test so far, the comparison 
## of alpha diversity between the land types

## start with oswaldo's tips example:

tips = pd.read_csv("/home/daniel/Documents/manualsBooks/bayesian/BAP3/code/data/tips.csv")

tips.tail()

categories = np.array(["Thur", "Fri", "Sat", "Sun"])
tip = tips["tip"].values
idx = pd.Categorical(tips["day"], categories=categories).codes

## arviz has cool plotting capabilities I have not even begun to learn:
az.plot_forest(tips.pivot(columns="day", values="tip").to_dict("list"),
               kind="ridgeplot",
               hdi_prob=1,
               colors="C1",
               figsize=(12, 4))

## for indexing with Arviz:

coords = {"days": categories, "days_flat":categories[idx]}

with pm.Model(coords=coords) as comparing_groups:
    μ = pm.HalfNormal("μ", sigma=5, dims="days")
    σ = pm.HalfNormal("σ", sigma=1, dims="days")
    y = pm.Gamma("y", mu=μ[idx], sigma=σ[idx], observed=tip, dims="days_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))


_, axes = plt.subplots(2, 2, figsize=(10, 5), sharex=True, sharey=True)

az.plot_ppc(idata_cg, num_pp_samples=100,
            colors=["C1", "C0", "C0"],
            coords={"days_flat":[categories]}, flatten=[], ax=axes)

az.plot_trace(idata_cg)

plt.show()

## seems fine
az.summary(idata_cg, kind="stats").round(2)



## okay, how do we adapt this to our data? we want to compare 
## alpha diversity of three groups - crop, forest, and grassland.

sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 
controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
envData.drop(controls, axis='rows', inplace=True)
comData.drop(controls, axis='rows', inplace=True)

## we observed above that we can trust the raw species 
## richness counts from our community matrix, no need
## to rarify back or anything. 

## so how to get this in pandas/python?

aa = comData.copy()
aa[aa > 0] = 1
specRich = aa.sum(axis="columns")

comData.head()

aa.head()

aa.sum(axis="columns")

aa.sum(axis="columns").loc['S14']

aa.sum(axis="columns").loc['S102']


specRich = aa.sum(axis="columns")



pd.to_numeric(specRich, downcast='integer')

## maybe a df with all the info we need:

specRichLT = (pd.concat([pd.to_numeric(specRich, downcast='integer'), envData['Land_type']], axis='columns')
                     .rename({0:"spRich"},axis="columns"))

## looks right. so this should be our species richness. Might need this later:
specRichLT.to_csv("specRich.csv")

specRichLT.head()

## we want to get a distribution for the mean values of alpha diversity for
## each group 
 
specRichLT.head()

## can we visualize this with arviz first?:

az.plot_forest(specRichLT.pivot(columns="Land_type", values="spRich").to_dict("list"),
               kind="ridgeplot",
               hdi_prob=1,
               colors="C1",
               figsize=(12, 4))

## oswaldo's confusing code for creating an index, adapted for our data:
categories = np.array(["Arableland", "Grassland", "Forest"])
spr = specRichLT["spRich"].values
idx = pd.Categorical(specRichLT["Land_type"], categories=categories).codes
coords = {"Land_type": categories, "land_type_flat":categories[idx]}

with pm.Model(coords=coords) as comparing_groups:
    μ = pm.Normal("μ", mu=300, sigma=50, dims="Land_type")
    σ = pm.HalfNormal("σ", sigma=10, dims="Land_type")
    y = pm.Normal("Species richness", mu=μ[idx], sigma= σ[idx], observed=spr, dims="land_type_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))

    #y = pm.Gamma("y", mu=μ[idx], sigma=σ[idx], observed=spr, dims="land_type_flat") ## better for outliers?

plt.close('all')


with pm.Model(coords=coords) as comparing_groups:
    μ = pm.Normal("μ", mu=300, sigma=50, dims="Land_type")
    σ = pm.HalfNormal("σ", sigma=20, dims="Land_type")
    y = pm.Normal("Species richness", mu=μ[idx], sigma= σ[idx], observed=spr, dims="land_type_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))

plt.close('all')
fig, axes = plt.subplots(3, 1, sharex=True)
az.plot_ppc(idata_cg, num_pp_samples=100, coords={"land_type_flat":[categories]}, flatten=[], ax=axes)
fig.tight_layout()

## works, but the outliers are forcing a lot of variation into the posterior

## can we do this with cauchy?

with pm.Model(coords=coords) as model_t:
    μ = pm.Normal("μ", mu=300, sigma=50, dims="Land_type")
    σ = pm.HalfNormal("σ", sigma=20, dims="Land_type")
    ν = pm.Exponential('ν', 0.1, dims="Land_type") ## exponential gets flatter with lower values, mean gets pulled away from zero
    y = pm.StudentT('Species richness', nu=ν[idx], mu=μ[idx], sigma= σ[idx], observed=spr, dims="land_type_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))

## in the notebook, they give two models for this
## the second uses the gamma distribution.
## not run - why gamma? not sure. 

plt.close('all')

fig, axes = plt.subplots(3, 1, sharex=True)
az.plot_ppc(idata_cg, num_pp_samples=100, coords={"land_type_flat":[categories]}, flatten=[], ax=axes)
axes[0].set_xlim(200,400)
axes[1].get_legend().remove()
axes[2].get_legend().remove()
fig.tight_layout()


#### map of respiration values ####

## we still don't have a general analysis strategy,
## but it always helps to look at a map at the trait 
## of interest.

## we want to see rates of respiration across the 
## study. 

## it would be great to gave a vectorized land use map...
## does this exist somewhere?

## first, plot the respiration values:

envData.soil_respiration

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )
sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color edges by landtype:

landColorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
landCols = [ landColorDict[i] for i in sulariPlot_utm['Land_type'] ]
## 
plt.close('all')
fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
ax.ticklabel_format(useOffset=False, style='plain')
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    cmap='YlOrRd',
    #cmap='RdPu',
    column='soil_respiration',
    edgecolors=landCols,
    linewidth=2,
    markersize=sulariPlot_utm['soil_respiration']*50,
     )

ax.set_ylim(5547500, 5570000)
ax.set_xlim(265000, 287000)
respMin = envData['soil_respiration'].min() ## 0.488057256
respMax = envData['soil_respiration'].max() ## 13.70117879
fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.Normalize(respMin, respMax), cmap='YlOrRd'),
             ax=ax, orientation='vertical', label='resp')
Arableland_patch = Patch(color='#862d2d', label='Arableland')
Forest_patch = Patch(color='#006600', label='Forest')
Grassland_patch = Patch(color='#FF7F00', label='Grassland')
ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])

#### find species that are associated with high respiration (and low resp?) ##

## try deseq or bayesian equivalent. 
## or indicator species?
## cooccurrence networks?

## in general, we want species that are associated with high resp
## even in forests and croplands, which are the land types with 
## lower respiration rates generally. 

## we could cluster the communities and see if a cluster not related to
## to land type emerges. It is possible that a "highly respiring" community
## exists. But seems unlikely, across all land types. 

## take the highest respiring sites from each land type, and look for 
## species that occur only in them? or run deseq on each land 
## type alone and take the species positively associated with respiration

## see if there are any common species. We'll start with deseq, but 
## I think for publication I'd like to build some bayesian models for
## differential abundance


##### deseq ##### 

## need deseq. Let's install it in the overall environment, not just spatialDirt env


if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("DESeq2")

## we need raw abundances

R

library('phyloseq')
library('DESeq2')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

psLandCont <- ps ## make a duplicate phyloseq obj to play with

sample_data(psLandCont)[c('C1.1','C1.2','C2.1','C2.2'),'Land.type']  <- "control" ## add info

tail(sample_data(psLandCont)) ## looks okay

## okay, now following the tutorial above:

diagdds = phyloseq_to_deseq2(psLandCont, ~ Land.type)
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")

resultsNames(diagdds) ## Forest vs. Grassland not mentioned, but still possible

## let's see how the results look:
res <- results(diagdds, contrast=c("Land.type","Forest","Arableland"))

## edit down to highly significant results:

alpha = 0.1 ## significance cutoff
res <- res[complete.cases(res),] ## necessary because not all ASVs in all landtypes? 
## but then are those ASVs that are present only in one or two land types excluded here?
## these would be among the most important, I would think...think about this later
sigtab = res[(res$padj < alpha), ] ## cut off below the alpha using adjusted pvalues
## add taxonomy:
sigtab = cbind(as(sigtab, "data.frame"), as(tax_table(psLandCont)[rownames(sigtab), ], "matrix"))

## to check ASVs associated with respiration?

R

library('phyloseq')
library('DESeq2')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

## just like before make a duplicate phyloseq obj to play with
## and get rid of controls while we're at it
psNoControl = prune_samples(!(rownames(sample_data(ps)) %in% c("C1.1","C1.2","C2.1","C2.2")), ps)

## get rid of NAs in basalResp
basalRespNotNA <- !is.na(sample_data(psNoControl)$soil.respiration)
psNoControl = prune_samples(basalRespNotNA, psNoControl)
## okay, same old code as before:
diagdds = phyloseq_to_deseq2(psNoControl, ~ soil.respiration) ## set "treatment" of interest
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")
resultsNames(diagdds)
res <- results(diagdds)

alpha = 0.01 ## significance cutoff
res <- res[complete.cases(res),] ## necessary because not all ASVs in all landtypes
sigtab = res[(res$padj < alpha), ] ## cut off below the alpha using adjusted pvalues

sigtab$log2FoldChange

View(sigtab)



write.csv(sigtab, file="respDiffSeq.csv")

## these are all the strongly positively associated ASVs with resp:

highResp <- tax_table(ps)[c('ASV5', 'ASV13', 'ASV85', 'ASV110', 'ASV371', 'ASV462', 'ASV621',
       'ASV1089', 'ASV1419', 'ASV1795', 'ASV1831', 'ASV1905', 'ASV2058',
       'ASV2184', 'ASV2773')]

write.csv(highResp, file="hiRespTax.csv")


## there is a geobacter (ASV371) in there, maybe two (ASV2058). Weird.

ASV1831

## let's look at the rep sequence of these weird ones when we have time.


## since our "gene length" is exactly the same for all reads, I think we 
## can interpret baseMean as abundance 

## is this true? for instance, ASV2 has baseMean of 590. Seems like this 
## would be the abundance of ASV among the samples of lowest respiration values.

 
sample_data(psNoControl)$Basal.respiration

sample_variables(psNoControl)

## find some of these ASVs associated with increasing respiration - are they co-occurring? Are they where we expect them to be?

##### diffseq with land type as covariate #####


## if we want to check using covariate of land type, something like this?:

#diagdds = phyloseq_to_deseq2(psNoControl, ~ soil.respiration + Land.type) ## set "treatment" of interest
## ^ I think order matters here, covariate of interest should be last? 
## as per https://support.bioconductor.org/p/100828/

diagdds = phyloseq_to_deseq2(psNoControl, ~ Land.type + soil.respiration) ## set "treatment" of interest
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")
resultsNames(diagdds)
res <- results(diagdds)

alpha = 0.01 ## significance cutoff
res <- res[complete.cases(res),] ## necessary because not all ASVs in all landtypes
sigtab = res[(res$padj < alpha), ] ## cut off below the alpha using adjusted pvalues
sigtab$log2FoldChange

## these should ASVs that are changing with respiration 
## regardless of land type...

aa <- sigtab[ sigtab$log2FoldChange > 0, ]
bb <- aa[ aa$baseMean > 1, ]

tax_table(ps)[row.names(bb)]

View(sigtab)

## with this approach, the asvs that come across as most important here are:

ASV1164 Chloroflexi
ASV1734 acidobacteriota, Bryobacter
ASV2626 alphaprot, Esterales 
ASV4902 Actinbacteriota, Solirubrobacteraceae
ASV6938 Alphaproteo, Sphingomonas

## what habitat do these microbes prefer?
## plotted below, seems like only ASV1164 is useful new info

## to be sure, maybe subset (split) by land type and run? We lose
## statistical power, but could be interesting.

## we need a way to rapidly map ASVs.

## we also need to figure out if these are cooccurring

## we have a bunch of old, crude tools...start with these, and think about 
## a proper bayesian model for finding which species are responding

## the bracod paper seems very promising, even though it has zero citations...
## hasn't been updated in a year but could be worse. Interesting, it 
## focuses on the species as predictors of a host trait. That fits our 
## setup just fine, with species as predictors of respiration. Can't
## tell what the backend is. 


## the other option seems to be BORAL. This has been cited a bunch, in nature 
## etc. Looks like a good package to know about. More generalized, etc.
## but also looks like we do 
## uses old gibbs samplers, I think. Weird. 

## let's give bracod a run tomorrow, see if it is easy to set up and use

## in the meantime, how do we map the observations of an ASV? 

## should be do-able in python with the data we have:


sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 

controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
envData.drop(controls, axis='rows', inplace=True)
comData.drop(controls, axis='rows', inplace=True)
plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )
sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration',
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)
sulariPlotsDF.to_crs('EPSG:32633', inplace=True)
landColorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

fig,ax = plt.subplots()

## how can we generalize this so it can handle multiple ASVs? 

def mapOneASV(asv, ax=None, color="b", jitter=0, showLand=False):
    if ax is None: ax = plt.gca()
    jitX = sulariPlotsDF['geometry'].x.apply(lambda x: x + np.random.normal(loc=0, scale=jitter))
    jitY = sulariPlotsDF['geometry'].y.apply(lambda x: x + np.random.normal(loc=0, scale=jitter))
    asvPlotPoints = gpd.points_from_xy( jitX, jitY, crs="EPSG:32633" )
    asvGEO = gpd.GeoDataFrame(pd.concat([comData[asv], sulariPlotsDF['Land_type']], axis=1), geometry=asvPlotPoints, crs="EPSG:32633")
    if showLand: 
        asvGEO['landCols'] = [ landColorDict[i] for i in asvGEO['Land_type'] ]
    else: 
        asvGEO['landCols'] = "k"
    rasterio.plot.show(fichtelMap, ax=ax)
    asvGEO.plot(
        marker="o",
        ax=ax,
        linewidth=1,
        edgecolor=asvGEO['landCols'],
        facecolor=color,
        markersize=asvGEO[asv]*10000)
    return(asvGEO)

plt.close('all')

## then we can make maps as we see fit, with 0-inf otus.

mapOneASV("ASV2", color="b", jitter=0, showLand=True)

asv1 = mapOneASV("ASV1", color="r", jitter=0)

fig,ax = plt.subplots()

mapOneASV("ASV2", color="b", jitter=0)

mapOneASV("ASV1", color="r", jitter=0)

mapOneASV("ASV3", color="k", jitter=0)

## side note, ASV1 seems to be in a lot of plots...
## I thought this was E. coli from our mock community. Is this index bleed?

## how to check...

## this would mean diving back into the phyloseq pipeline and tracking 
## the prevalence of these MC otus...ugh, what a pain. 

## anyway, fairly low levels. If we are sticking to community level
## questions, probably not important.

## let's map some of the ASVs that look important for 

ASV1831 "Gemmataceae"         "Zavarzinella"
ASV371  "Geobacteraceae"      "Geobacter"

aa = mapOneASV("ASV1831", color="b", jitter=0)

aa = mapOneASV("ASV371", color="y", jitter=100)

aa[aa["ASV371"] > 0].index.values

envData.loc(bb)

envData.loc["S36"]
envData.loc["S70"]

## let's map the five most plentiful 
respASV = pd.read_csv("respDiffSeq.csv", index_col=0)

respASVtax = pd.read_csv("hiRespTax.csv", index_col=0)


respASV.head()

respASV.index


respASV.query("log2FoldChange > 1" ).index


## these are not so rare:

respASV.query("log2FoldChange > 1 & baseMean > 7")

## look at udeabacter


udea = mapOneASV("ASV621", color="b", jitter=0, showLand=True)

## interesting, but are any of these non-grassland species?
## subset to just forest sites, then run the deseq
## check tomorrow, have to work on some other things...

## here are some species that still appeared sensitive to 
## respiration, after accounting for land type, using deseq2 above

## ASV1164 Chloroflexi  forest
## ASV1734 acidobacteriota, Bryobacter
## ASV2626 alphaprot, Esterales 
## ASV4902 Actinbacteriota, Solirubrobacteraceae
## ASV6938 Alphaproteo, Sphingomonas

fig,ax = plt.subplots()

plt.close('all')

mapOneASV("ASV1164", color="b", jitter=0, showLand=True) ## definitely forest associated

aa = mapOneASV("ASV1734", color="b", jitter=0, showLand=True) ## definitely forest associated
aa = mapOneASV("ASV2626", color="b", jitter=0, showLand=True) ## only found in one plot above thresholds? So this probably disappears after our transformations
aa = mapOneASV("ASV4902", color="b", jitter=0, showLand=True) ## same: only found in one plot above thresholds? So this probably disappears after our transformations
aa = mapOneASV("ASV6938", color="b", jitter=0, showLand=True) ## also only one point. Oh jeez. 

(aa.iloc[:,0] > 0).sum()

## so the only the Chloroflexi from forests seems informative here.
## let's try subsetting by landtype:

## I think we need some multivariate approaches here. Networks and bayesian lms, like maybe bradco

## not sure if it will work with our current conda env?:

pip install BRACoD ## nope

## try a new env:

conda deactivate

conda create -n BRACoD python=3.6

conda activate BRACoD

pip install BRACoD

## theano not working...needs this?
conda install mkl-service

## that seems to have installed okay...test it out with sample data

## following https://github.com/ajverster/BRACoD

python

import BRACoD
import numpy as np

sim_counts, sim_y, contributions = BRACoD.simulate_microbiome_counts(BRACoD.df_counts_obesity)

sim_y ## our response variable 

contributions.shape

sim_relab = BRACoD.scale_counts(sim_counts)

help(BRACoD.run_bracod)

trace = BRACoD.run_bracod(sim_relab, sim_y, n_sample = 1000, n_burn=1000, njobs=4)

## too many cores? try defaults?
trace = BRACoD.run_bracod(sim_relab, sim_y, n_sample = 1000, n_burn=1000)


## whoah, this is slow...why?
## not working, freezes up. No real use of memory/cores


BRACoD.convergence_tests(trace, sim_relab)

conda env export > my_bracod_env.yaml


## that appears to be dead. 

## okay, that rules out bracod for the moment. Let's pick a cooccurence network 
## analysis method, try an install:

install.packages("devtools")

install.packages("BiocManager")

BiocManager::install("limma")

# Install NetCoMi
devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))


## try some sample features:

library(NetCoMi)

data("amgut1.filt")

amgut1.filt[0:10,0:10]


data("amgut2.filt.phy")

amgut2.filt.phy ## phyloseq obj


net_spring <- netConstruct(amgut1.filt,
                           filtTax = "highestFreq",
                           filtTaxPar = list(highestFreq = 50),
                           filtSamp = "totalReads",
                           filtSampPar = list(totalReads = 1000),
                           measure = "spring",
                           measurePar = list(nlambda=10, 
                                             rep.num=10,
                                             Rmethod = "approx"),
                           normMethod = "none", 
                           zeroMethod = "none",
                           sparsMethod = "none", 
                           dissFunc = "signed",
                           verbose = 2,
                           seed = 123456)



props_spring <- netAnalyze(net_spring, 
                           centrLCC = TRUE,
                           clustMethod = "cluster_fast_greedy",
                           hubPar = "eigenvector",
                           weightDeg = FALSE, normDeg = FALSE)


#?summary.microNetProps


summary(props_spring, numbNodes = 5L)

p <- plot(props_spring, 
          nodeColor = "cluster", 
          nodeSize = "eigenvector",
          title1 = "Network on OTU level with SPRING associations", 
          showTitle = TRUE,
          cexTitle = 2.3)

legend(0.7, 1.1, cex = 2.2, title = "estimated association:",
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"), 
       bty = "n", horiz = TRUE)

## or with pearson:

net_pears <- netConstruct(amgut2.filt.phy,  
                          measure = "pearson",
                          normMethod = "clr",
                          zeroMethod = "multRepl",
                          sparsMethod = "threshold",
                          thresh = 0.3,
                          verbose = 3)

## try everything out for a bit, then update yaml backup for the repo


conda activate spatialDirt 

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
from matplotlib_scalebar.scalebar import ScaleBar
import pymc as pm


spatDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
## ^different on laptop
os.chdir(spatDir)

## we just want a map of points right now:

## on officeComp
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"
## on laptop
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"

envData = pd.read_csv(sulariEnvCSV)
## get rid of spaces
envData.rename({"Sample ID":"SampleID"}, axis="columns", inplace=True)
## we need to clean up the plot.ID. Sulari recorded season in the plot IDs,
## using letter codes. Also she has one double sampling, with an underscore
envData['Plot.ID'] = envData['Plot.ID'].str.slice(0,5)

## let's get rid of decimals in the names, weird for python work:
envData.rename({"Plot.ID":"PlotID", 
       "soil.respiration":"soil_respiration",
              "Land.type":"Land_type"}, 
                axis="columns", inplace=True)

envData.head()

## so we don't have to repeat:
envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index=False)

envData.head()


envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
 

## if we need to drop controls
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

## her otu table is really large:
##### R ###
library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")
logMin50ps
comdat <- as.data.frame(otu_table(logMin50ps))
write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")
############

## back in python

comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", index_col=0)

comData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

comData.head()

## the map below looks funny...looks like every forest plot has 
## a grassland under it...

dupsFilter = envData.Latitude.duplicated()
dupped = envData[dupsFilter].sort_values(by="Latitude")

dupped.groupby('Plot.ID').nunique()

envData.iloc[0:5,0:8]

## nope...looks okay

## we want a geodf...

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

## to convert to UTM? looks like we are in zone 33
## looks like:
## EPSG:32633

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

sulariPlot_utm.tail()

## can we import our georeferenced tif of the study area?
## lat/lon
#fichtelMap = rasterio.open("studyAreaClipped_modified.tif")
## UTM

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color code our points according to land type?
cdik={
"Arableland":"b",
"Grassland":"y",
"Forest":"g",
}
sulariPlotsDF['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]
sulariPlot_utm['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]

fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    facecolor=sulariPlotsDF['landColors'],
## we can use a centered log transformation, pearson correlation as per the examples 
## on the NetCoMi github

load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

net_pears <- netConstruct(ps,  
                          measure = "pearson",
                          normMethod = "clr",
                          zeroMethod = "multRepl", ## don't understand totally...
                          sparsMethod = "threshold", ## don't understand totally...
                          thresh = 0.3,
                          verbose = 3)

## killed. To much memory required?


props_pears <- netAnalyze(net_pears, 
                          clustMethod = "cluster_fast_greedy")

plot(props_pears, 
     nodeColor = "cluster", 
     nodeSize = "eigenvector",
     title1 = "Network on OTU level with Pearson correlations", 
     showTitle = TRUE,
     cexTitle = 2.3)

legend(0.7, 1.1, cex = 2.2, title = "estimated correlation:", 
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"), 
       bty = "n", horiz = TRUE)


## can we port this over to the lab computer?

nanoComp

## let's try using the house R install:

## make a working directory:

cd /media/vol1/daniel/sulariArne/soilAnalysis


#library('DESeq2')

if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install('phyloseq')

library('phyloseq')

setwd("/media/vol1/daniel/sulariArne/soilAnalysis")

download.file("https://github.com/danchurch/fichtelgebirgeSoils/raw/main/sulariData/sulariPhyloseqObject.rda", destfile="sulariPhyloseqObject.rda")

load("sulariPhyloseqObject.rda")

ps ## looks ok


BiocManager::install("limma")

BiocManager::install("zCompositions")

install.packages("devtools") 

devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))


## dev.tools failed. 
## the following packages failed. Work on it tomorrow. 

install.packages("systemfonts")
## which needs...
sudo apt install libfontconfig1-dev

install.packages(       "xml2")
## which needs...
sudo apt install libxml2-dev

install.packages("textshaping")
## which needs...
sudo apt install libharfbuzz-dev libfribidi-dev

install.packages(  "rversions") ## easy
install.packages( "urlchecker") ## easy

install.packages(    "openssl")
## which needs
sudo apt install libssl-dev

install.packages(       "ragg")
## which needs
sudo apt install libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev

install.packages("credentials") ## easy
install.packages(      "httr2") ## easy
install.packages(       "httr") ## easy
install.packages(       "gert") ## easy
install.packages(         "gh") ## easy
install.packages(    "usethis") ## easy
install.packages(    "pkgdown") ## easy
install.packages(   "roxygen2") ## easy

## and...
install.packages(   "devtools")

devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))

## failed again. Lots of failed dependencies:
‘SpiecEasi’, ‘mixedCCA’, ‘qgraph’, ‘SPRING’, ‘WGCNA’ are not available for package ‘NetCoMi’


library(devtools)
## and the following are not there:
ld -llapack --verbose
ld -lblas --verbose
sudo apt install liblapack-dev libopenblas-dev

install.packages("mixedCCA")
install.packages("qgraph") 
install.packages("SPRING")
install.packages("WGCNA")

## try again 
devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))

library('NetCoMi') ## no errors...finally...

R

library('phyloseq')
library('NetCoMi')
setwd("/media/vol1/daniel/sulariArne/soilAnalysis")

## does the plotter work on x11 forwarding?

plot(1) ## looks okay, for base plotter

#download.file("https://github.com/danchurch/fichtelgebirgeSoils/raw/main/sulariData/sulariPhyloseqObject.rda", destfile="sulariPhyloseqObject.rda")

load("sulariPhyloseqObject.rda")

## to run our data, we want a pretty rigorous minimum abundance threshold. 
## for all of our vegan stuff, we use a minimum abundance of 50 reads 
## per observation. 

## to be clear, we want all sample counts below 50 to be come zero

dropLow <- function(x) {
                           if (x < 0) {x = 0}
                          }

dropLow <- function(x) {
                           x <- x - 50
                           if (x < 0) {x = 0}
                           x
                          }

dropLow(100)
ps.atLeast50 <- transform_sample_counts(ps.filter.fam, dropLow)

## nope, can't handle if statements. Let' pull out the matrix and work 
## on it directly:


setwd("/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis")

load("../sulariData/sulariPhyloseqObject.rda")
library('vegan')
library('phyloseq')
library('NetCoMi')

ps.atLeast50 <- ps
aa <- otu_table(ps)
bb <- aa - 50 
bb[bb < 0] <- 0
otu_table(ps.atLeast50) <- bb
## can we trim out empty otus now? 
ps.atLeast50 <- prune_taxa( taxa_sums(ps.atLeast50) > 0, ps.atLeast50 )

otu_table(ps)[0:10,0:10]

otu_table(ps.atLeast50)[0:10,0:10]

dim(otu_table(ps)) 
dim(otu_table(ps.atLeast50)) ## down to just 4280 taxa, out of 36140

## ok, looks right
## now pass that to network software:

## just curious, can it handle the full ps? want to see how this changes the network.

#net_pears_fullPS <- netConstruct(ps,  
#                          measure = "pearson",
#                          normMethod = "clr",
#                          zeroMethod = "multRepl", ## don't understand totally...
#                          sparsMethod = "threshold", ## don't understand totally...
#                          thresh = 0.3,
#                          verbose = 3)

## nope, dies

## try smaller object:

net_pears_PSatleast50 <- netConstruct(ps.atLeast50,  
                          measure = "pearson",
                          normMethod = "clr",
                          zeroMethod = "multRepl", ## don't understand totally...
                          sparsMethod = "threshold", ## don't understand totally...
                          thresh = 0.3,
                          verbose = 3)

## that was computationally expensive:
save(net_pears_PSatleast50, file = "net_pears_PSatleast50.rda") ## 139 mb, pretty darn big..

props_pears <- netAnalyze(net_pears_PSatleast50, 
                          clustMethod = "cluster_fast_greedy")


?netConstruct

## huh, we can use a custom count matrix

## which means we can add columns. 
## so let's make a matrix that includes land-type, and 

?netAnalyze

## not run
plot(props_pears, 
     nodeColor = "cluster", 
     nodeSize = "eigenvector",
     title1 = "Network on OTU level with Pearson correlations", 
     showTitle = TRUE,
     cexTitle = 2.3)

legend(0.7, 1.1, cex = 2.2, title = "estimated correlation:", 
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"), 
       bty = "n", horiz = TRUE)

## text file local. For now, both vim and jupyter are on the nanocomp computer:


cd /media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis

conda activate spatialDirt 

jupyter notebook --no-browser --port=8080

## run this to activate the tcp forwarding
ssh -L 8080:localhost:8080 test@132.180.112.115

## then open browser to: 
http://localhost:8080/notebooks/spatialAnalysisSulariData.ipynb

## it's going to be confusing working on both computers. 
## just slow it down, save/pull/push.

## calculating the network statistics takes for ever with netcommi

## can we multithread? its only using one core...


## I think we need to revisit the network software 

## we need some how a vector of the ASVs most responsive to 
## respiration, added to a cooccurrence matrix.

## we need to mkae our own adjacency matrix, I guess. 

## anyway, to get the candidates for responsiveness to 
## respiration...seems like we can't avoid a full multivariate
## treatment of the community matrix?

## let's take a look at the BORAL tool...

##### boral install and test ####

## boral github repo is here:
https://github.com/emitanaka/boral

## try the install on local machine:

install.packages('boral') 

install.packages('R2jags') 

install.packages('rjags') 

sudo apt install jags ## and work back up

library(boral)

## and pretty much zero documentation...

#### HMSC install and test #####

## let's try HMSC package:

install.packages("devtools") # if not yet installed
install.packages("usethis") # if not yet installed

library(devtools)

install_github("hmsc-r/HMSC")

## cran could also work, for stable version. 

## there is a book. Maybe we order it? 

## but for the moment, try to find some online examples?

## they have a series of vignettes they want you to work through

https://cran.r-project.org/web/packages/Hmsc/index.html

## start with the first:

https://cran.r-project.org/web/packages/Hmsc/vignettes/vignette_1_univariate.pdf


library(Hmsc)

set.seed(1)

## make a simple linear model:
## using a maximum likelihood fit:

n = 50
x = rnorm(n)
alpha = 0
beta = 1
sigma = 1
L = alpha + beta*x
y = L + rnorm(n, sd = sigma)

plot(x, y, las=1)


df = data.frame(x,y)
m.lm = lm(y ~ x, data=df)
summary(m.lm)

## compare this to HMSC

Y = as.matrix(y)

XData = data.frame(x = x)

m = Hmsc(Y = Y, XData = XData, XFormula = ~x)

nChains = 2
thin = 5
samples = 1000
transient = 500*thin
verbose = 500*thin

?sampleMcmc

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
nChains = nChains, verbose = verbose)

mpost = convertToCodaObject(m)

summary(mpost$Beta)

preds = computePredictedValues(m)

?computePredictedValues

evaluateModelFit(hM=m, predY=preds)

plot(mpost$Beta)

effectiveSize(mpost$Beta)

## if these indicators are close to one, the 
## chains behaved similarly during sampling
gelman.diag(mpost$Beta,multivariate=FALSE)$psrf


## evaluating at a maxlikelihood linear model:

nres.lm = rstandard(m.lm)
preds.lm = fitted.values(m.lm)
par(mfrow=c(1,2))
hist(nres.lm, las = 1)
plot(preds.lm,nres.lm, las = 1)
abline(a=0,b=0)

plot(m.lm)

## checking out a bayesian lm:

preds.mean = apply(preds, FUN=mean, MARGIN=1)

nres = scale(y-preds.mean)

par(mfrow=c(1,2))

hist(nres, las = 1)
plot(preds.mean,nres, las = 1)

abline(a=0,b=0)

## generalized linear models (link function, different dists) possible, just a couple
## try everything out for a bit, then update yaml backup for the repo


conda activate spatialDirt 

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
from matplotlib_scalebar.scalebar import ScaleBar
import pymc as pm


spatDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
## ^different on laptop
os.chdir(spatDir)

## we just want a map of points right now:

## on officeComp
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"
## on laptop
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"

envData = pd.read_csv(sulariEnvCSV)
## get rid of spaces
envData.rename({"Sample ID":"SampleID"}, axis="columns", inplace=True)
## we need to clean up the plot.ID. Sulari recorded season in the plot IDs,
## using letter codes. Also she has one double sampling, with an underscore
envData['Plot.ID'] = envData['Plot.ID'].str.slice(0,5)

## let's get rid of decimals in the names, weird for python work:
envData.rename({"Plot.ID":"PlotID", 
       "soil.respiration":"soil_respiration",
              "Land.type":"Land_type"}, 
                axis="columns", inplace=True)

envData.head()

## so we don't have to repeat:
envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index=False)

envData.head()


envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
 

## if we need to drop controls
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

## her otu table is really large:
##### R ###
library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")
logMin50ps
comdat <- as.data.frame(otu_table(logMin50ps))
write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")
############

## back in python

comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", index_col=0)

comData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

comData.head()

## the map below looks funny...looks like every forest plot has 
## a grassland under it...

dupsFilter = envData.Latitude.duplicated()
dupped = envData[dupsFilter].sort_values(by="Latitude")

dupped.groupby('Plot.ID').nunique()

envData.iloc[0:5,0:8]

## nope...looks okay

## we want a geodf...

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

## to convert to UTM? looks like we are in zone 33
## looks like:
## EPSG:32633

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

sulariPlot_utm.tail()

## can we import our georeferenced tif of the study area?
## lat/lon
#fichtelMap = rasterio.open("studyAreaClipped_modified.tif")
## UTM

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color code our points according to land type?
cdik={
"Arableland":"b",
"Grassland":"y",
"Forest":"g",
}
sulariPlotsDF['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]
sulariPlot_utm['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]

fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    edgecolor='k',

plot(x,y, las = 1)

### example hierachical model

## 100 samples, 10 plots, one response variable, checking for plot effects
## additive effects, in other words looking for different intercepts
## due to the different plots:

## make fake data:
n = 100
x = rnorm(n)
alpha = 0
beta = 1
sigma = 1
L = alpha + beta*x
np = 10
sigma.plot = 1
sample.id = 1:n
plot.id = sample(1:np, n, replace = TRUE)
ap = rnorm(np, sd = sigma.plot)
a = ap[plot.id]
y = L + a + rnorm(n, sd = sigma)
plot.id = as.factor(plot.id)
plot(x,y,col = plot.id, las = 1)
XData = data.frame(x = x)
Y = as.matrix(y)
studyDesign = data.frame(sample = as.factor(sample.id), plot = as.factor(plot.id))
rL = HmscRandomLevel(units = studyDesign$plot)
m = Hmsc(Y=Y, XData=XData, XFormula=~x,
       studyDesign=studyDesign, ranLevels=list("plot"=rL))
m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
                nChains = nChains, nParallel = nChains, verbose = verbose)
preds = computePredictedValues(m)
MF = evaluateModelFit(hM=m, predY=preds)
MF$R2 ## ~.7, keeps changing
## that is classical R2, I guess, all data used to make the model
## and all data used to evaluate

## they offer rapid cross-validation predictive R2 
partition = createPartition(m, nfolds = 2, column = "sample")

partition

## not sure how this works, but model is refit using the likelihood from half 
## the data I guess:
preds = computePredictedValues(m, partition = partition, nParallel = nChains)

## seems like this would create two diff posteriors? no way, too weird. I just don't
## understand

MF = evaluateModelFit(hM = m, predY = preds)
MF$R2 ## ~0.7 still pretty good



## spatial autorrelation can be incorporated as an additional variable

## make a random dataset with y-variable autocorrelation that does
## not correspond with autocorrelation in x:

sigma.spatial = 2
alpha.spatial = 0.5
sample.id = rep(NA,n)
for (i in 1:n){
sample.id[i] = paste0("location_",as.character(i))
}
sample.id = as.factor(sample.id)
xycoords = matrix(runif(2*n), ncol=2)
rownames(xycoords) = sample.id
colnames(xycoords) = c("x-coordinate","y-coordinate")
a = MASS::mvrnorm(mu=rep(0,n),
Sigma = sigma.spatial^2*exp(-as.matrix(dist(xycoords))/alpha.spatial))
y = L + a + rnorm(n, sd = sigma)
Y=as.matrix(y)
colfunc = colorRampPalette(c("cyan", "red"))
ncols = 100
cols = colfunc(100)
par(mfrow=c(1,2))
for (i in 1:2){
if (i==1) value = x
if (i==2) value = y
value = value-min(value)
value = 1+(ncols-1)*value/max(value)
plot(xycoords[,1],xycoords[,2],col=cols[value],pch=16,main=c("x","y")[i], asp=1)
}

studyDesign = data.frame(sample = sample.id)

## tell the model we are expecting a spatial level to our model with sData setting?
rL = HmscRandomLevel(sData = xycoords)

m = Hmsc(Y=Y, XData=XData, XFormula=~x,
         studyDesign=studyDesign, ranLevels=list("sample"=rL))

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
         nChains = nChains, nParallel = nChains, verbose = verbose)


## not sure how they hande the estimation of the spatial effects,
## need to check under the hood a bit. But also need to keep moving with this
## analysis. 

## the second tutorial: simple multivariate data:

https://cran.r-project.org/web/packages/Hmsc/vignettes/vignette_2_multivariate_low.pdf

library(Hmsc)
library(corrplot)

set.seed(1) 

## five species example:

n = 100
x1 = rnorm(n)
x2 = rnorm(n)
XData = data.frame(x1=x1,x2=x2)
alpha = c(0,0,0,0,0)
beta1 = c(1,1,-1,-1,0)
beta2 = c(1,-1,1,-1,0)
sigma = c(1,1,1,1,1)
L = matrix(NA,nrow=n,ncol=5)

Y = matrix(NA,nrow=n,ncol=5)
for (j in 1:5){
L[,j] = alpha[j] + beta1[j]*x1 + beta2[j]*x2
Y[,j] = L[,j] + rnorm(n, sd = sigma[j])
}

m = Hmsc(Y = Y, XData = XData, XFormula = ~x1+x2)

nChains = 2
test.run = TRUE
if (test.run){
  #with this option, the vignette runs fast but results are not reliable
  thin = 1
  samples = 10
  transient = 5
  verbose = 0
} else {
  #with this option, the vignette evaluates slow but it reproduces the results of the
  #.pdf version
  thin = 10
  samples = 1000
  transient = 500*thin
  verbose = 0
}

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
    nChains = nChains, nParallel = nChains, verbose = verbose)

mpost = convertToCodaObject(m)

effectiveSize(mpost$Beta)

## the resulting checkerboard shows what happens when an important 
## predictor is left out. The residual covariance after that is left
## after the remaining environmental predictor is incorporated into 
## the latent variable that is intended to represent biotic interactions.
## So some species are covarying, but only because they both "like" 
## environmental conditions that weren't in the model, not because 
## they are in symbioses etc. This shows the limitations of this kind of analysis. 

## explanatory R2:
preds = computePredictedValues(m)

evaluateModelFit(hM = m, predY = preds)

m = Hmsc(Y=Y, XData=XData, XFormula=~x1,
          studyDesign=studyDesign, ranLevels=list(sample=rL))
m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
          nChains = nChains, nParallel = nChains, verbose = verbose)

postBeta = getPostEstimate(m, parName="Beta")

plotBeta(m, post=postBeta, param="Support", supportLevel = 0.95)

## the latent variable is actually doing a good job of 

## predictive/cross-validated R2
## making up the missing predictor. 

## simpleOrdination

studyDesign = data.frame(sample = as.factor(1:n))
rL = HmscRandomLevel(units = studyDesign$sample)

## force number of latent factors:

rL$nfMin=2
rL$nfMax=2
m = Hmsc(Y=Y, XData=XData, XFormula=~1,
         studyDesign=studyDesign, ranLevels=list(sample=rL))

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
nChains = nChains, nParallel = nChains, verbose = verbose)

etaPost=getPostEstimate(m, "Eta")

dim(etaPost$mean)
## interesting, number of rows is equal to number of sites,
## and number of columns is equal to number of latent factors
## but with lambda:

lambdaPost=getPostEstimate(m, "Lambda")

dim(lambdaPost$mean)
## number of rows is equal to number of latent variables and
## number of columns is equal to number of species
## inverted, don't know why, maybe for matrix multiplication?

lambdaPost$mean

biPlot(m, etaPost = etaPost, lambdaPost = lambdaPost, factors = c(1,2), "x2")


## but this breaks down when you split the data
## for training/prediction. 
## They don't describe this in the tutorial, but
## assume this because the latent variables 
## are given uninformative priors and the 
## likelihood isn't weighted as much with the 
## smaller amound of data/evidenc.

preds = computePredictedValues(m, partition = partition, nParallel = nChains)

evaluateModelFit(hM = m, predY = preds)

## possible to do predictions of a species, especially
## if you have data from the other species. So cross validation
## leaving one species out at a time also possible:

preds = computePredictedValues(m, partition=partition,
      partition.sp=c(1,2,3,4,5), mcmcStep=10, nParallel = nChains)


## this is mostly for when you have a small species matrix

##### Ordinations ####

rL$nfMin=2
rL$nfMax=2

m = Hmsc(Y=Y, XData=XData, XFormula=~1,
    studyDesign=studyDesign, ranLevels=list(sample=rL))
m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
    nChains = nChains, nParallel = nChains, verbose = verbose)

etaPost=getPostEstimate(m, "Eta")
lambdaPost=getPostEstimate(m, "Lambda")

biPlot(m, etaPost = etaPost, lambdaPost = lambdaPost, factors = c(1,2), "x2")

## interesting, but my results don't match the tutorial, so not sure how to 
## interpret

## essentially, the idea is similar to eigen-decomposition/PCA type 
## analysis, but the method is different. 

## they just introduced two vaguely defined variables the explain the 
## general mass of the data, without any previous hypotheses
## stated, but what are the priors and likelihoods on these? don't 
## understand. Some sort of gaussian or dirichilet process I guess. Beyond me. 

## then these two latent variables act pretty much like when we create 
## a composite variable using PCAs

## we subtract the variance we can explain from environmental conditions
## and biotic interactions by giving these to the model, and the remaining
## residual variation only is explained by these 

## you can mix up the link functions and residual models:

nChains = 2
thin = 10
samples = 1000
transient = 500*thin
verbose = 0
set.seed(2)
n = 100
x1 = rnorm(n)
x2 = rnorm(n)
alpha = c(0,0,0,0)
beta1 = c(1,1,-1,-1)
beta2 = c(1,-1,1,-1)
sigma = c(1,NA,NA,1)
XData = data.frame(x1=x1,x2=x2)
L = matrix(NA,nrow=n,ncol=4)
Y = matrix(NA,nrow=n,ncol=4)
for (j in 1:4){
  L[,j] = alpha[j] + beta1[j]*x1 + beta2[j]*x2
  }
Y[,1] = L[,1] + rnorm(n, sd = sigma[1])
Y[,2] = 1*((L[,2] + rnorm(n, sd = 1))>0)
Y[,3] = rpois(n, lambda = exp(L[,3]))
Y[,4] = rpois(n, lambda = exp(L[,4] + rnorm(n, sd = sigma[4])))

m = Hmsc(Y = Y, XData = XData, XFormula = ~x1+x2,
       distr = c("normal","probit","poisson","lognormal poisson"))

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
    nChains = nChains, nParallel = nChains, verbose = verbose)



mpost = convertToCodaObject(m)
effectiveSize(mpost$Beta)
gelman.diag(mpost$Beta, multivariate=FALSE)$psrf

preds = computePredictedValues(m, expected = FALSE)

evaluateModelFit(hM = m, predY = preds)

postBeta = getPostEstimate(m, parName="Beta")

plotBeta(m, post=postBeta, param="Support", supportLevel = 0.95)

## cool, works. But not sure how we would assign priors/likelihood models for
## all of the thousand species we're going to examine...
## maybe in the next tutorial:

## HSCMC tutorial #3:
## https://cran.r-project.org/web/packages/Hmsc/vignettes/vignette_3_multivariate_high.pdf

library(Hmsc)
library(corrplot)
library(ape)
library(MASS)
library(fields) ## for image.plot?
library(knitr)
set.seed(1)

ns = 50
## this will incorporate phylogeny, which we will randomly make up:
phy = ape::rcoal(n=ns, tip.label = sprintf('species_%.3d',1:ns), br = "coalescent")
plot(phy, show.tip.label = FALSE, no.margin = TRUE)

## model two traits - forest preference and thermal optimum
## assume there is a phylogenetic effect - more closely related 
## species are more likely to have the same preference for forest
## or more similar thermal optimum:

## make up the data so, this hurts my poor little brain:

C = vcv(phy, model = "Brownian", corr = TRUE) ## ape function for traits that are evolving
spnames = colnames(C)
traits = matrix(NA,ncol =2,nrow = ns)
## fill in the traits,  mvrnorm from MASS package
for (i in 1:2){
    traits[,i] = matrix(mvrnorm(n = 1, mu = rep(0,ns), Sigma=C))
    }
rownames(traits) = spnames
colnames(traits) = c("habitat.use","thermal.optimum")
traits = as.data.frame(traits)
par(fig = c(0,0.6,0,0.8), mar=c(6,0,2,0))
plot(phy, show.tip.label = FALSE)
par(fig = c(0.6,0.9,0.025,0.775), mar=c(6,0,2,0), new=T)
plot.new()
image.plot(t(traits),axes=FALSE,legend.width = 3,legend.shrink=1,
#imagePlot(t(traits),axes=FALSE,legend.width = 3,legend.shrink=1,
col = colorRampPalette(c("blue","white","red"))(200))
text(x=1.1, y=0.72, srt = 90, "H", cex=0.9, pos = 4)
text(x=1.4, y=0.72, srt = 90, "T", cex=0.9, pos = 4)

## neat. maybe do something similar with respiration and forest/grassland

## make some environmental and community data:

n = 200
habitat = factor(sample(x = c("forest","open"), size = n, replace=TRUE))
climate = rnorm(n)
nc = 4
mu = matrix(0,nrow=nc,ncol=ns)
#expected niche of each species related to the "covariate" intercept
mu[1, ] = -traits$thermal.optimum^2/4-traits$habitat.use
#expected niche of each species related to the covariate forest
#(open area as reference level, so included in intercept)
mu[2, ] = 2*traits$habitat.use
#expected niche of each species related to the covariate climate
mu[3, ] = traits$thermal.optimum/2
#expected niche of each species related to the covariate climate*climate
mu[4, ] = -1/4
beta = mu + 0.25*matrix(rnorm(n = ns*nc), ncol=ns)
X = cbind(rep(1,ns), as.numeric(habitat=="forest"), climate, climate*climate)
L = X%*%beta
Y = L + mvrnorm(n=n, mu=rep(0,ns), Sigma=diag(ns))
colnames(Y) = spnames


## didn't really understand all of the code, but the goal 
## is to create a community matrix of species that 
## affected by the two traits. 

Y[1:5,1:5] ## species are columns

## build a model that includes:
## the climate and habitat data, 
## a climate^2 term (allows for unimodal, non-linear climate niche curve)
## also a term for the phylogenetic signal

XData = data.frame(climate = climate, habitat = habitat)
XFormula = ~habitat + poly(climate,degree = 2,raw = TRUE)
TrFormula = ~habitat.use + thermal.optimum
studyDesign = data.frame(sample = sprintf('sample_%.3d',1:n), stringsAsFactors=TRUE)
rL = HmscRandomLevel(units = studyDesign$sample)
rL$nfMax = 15
m = Hmsc(
                Y = Y, 
            XData = XData, 
         XFormula = XFormula,
           TrData = traits, 
        TrFormula = TrFormula,
        phyloTree = phy,
      studyDesign = studyDesign, 
        ranLevels = list(sample = rL))


## not run - apparently this will take 2 hours.
## lets set up the labcomputer for this.

m = Hmsc(Y = Y, XData = XData, XFormula = XFormula,
         TrData = traits, TrFormula = TrFormula,
         phyloTree = phy,
         studyDesign = studyDesign, ranLevels = list(sample = rL))

nChains = 2
thin = 10
samples = 1000
transient = 500
verbose = 0

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
               nChains = nChains, nParallel = nChains, verbose = verbose)

## that actually ran for like ten minutes

## so now we have posterior probability for the community

## they look for four parameters:

## species niches (environmental predictors of species abundance)
## influence of traits on species niches (organismal traits predicting their response to environment)
## species-species interactions 
## effect of phylogeny

## the omega (s x s) matrix is 50 x 50 x 2500 elements. The 2500 is from the traces of the posterior, I think
## but I thought that part of the point of latent variables was to avoid building this massive matrix?

## anyway...figure out later. They subset to one 100 random species pairs

## check convergences

mpost = convertToCodaObject(m)

par(mfrow=c(3,2))
ess.beta = effectiveSize(mpost$Beta)
psrf.beta = gelman.diag(mpost$Beta, multivariate=FALSE)$psrf
hist(ess.beta)
hist(psrf.beta)
ess.gamma = effectiveSize(mpost$Gamma)
psrf.gamma = gelman.diag(mpost$Gamma, multivariate=FALSE)$psrf
hist(ess.gamma)
hist(psrf.gamma)
sppairs = matrix(sample(x = 1:ns^2, size = 100))
tmp = mpost$Omega[[1]]

for (chain in 1:length(tmp)){
tmp[[chain]] = tmp[[chain]][,sppairs]
}

ess.omega = effectiveSize(tmp)
psrf.omega = gelman.diag(tmp, multivariate=FALSE)$psrf
hist(ess.omega)
hist(psrf.omega)

print("ess.rho:")
effectiveSize(mpost$Rho)

print("psrf.rho:")
gelman.diag(mpost$Rho)$psrf

## convergence diagnostics look fine

## overall performance of the model, explanatory.
## for this, check the R2 on average for all 
## species predictions:

preds = computePredictedValues(m)
MF = evaluateModelFit(hM=m, predY=preds)

hist(MF$R2, xlim = c(0,1), main=paste0("Mean = ", round(mean(MF$R2),2)))

## hovering around .70  not bad

m$X

head(m$X)


## we can use our model to do Variance Partitioning:

VP = computeVariancePartitioning(m, group = c(1,1,2,2), groupnames = c("habitat","climate"))

?computeVariancePartitioning

plotVariancePartitioning(m, VP = VP)

kable(VP$R2T$Beta)

## we can get back to the environmental predictors, on a per-species basis: 
postBeta = getPostEstimate(m, parName = "Beta")

plotBeta(m, post = postBeta, param = "Support",
plotTree = TRUE, supportLevel = 0.95, split=.4, spNamesNumbers = c(F,F))

postGamma = getPostEstimate(m, parName = "Gamma")

plotGamma(m, post=postGamma, param="Support", supportLevel = 0.95)

OmegaCor = computeAssociations(m)

## what are these these objects?

str(OmegaCor)

str(OmegaCor[[1]][1])

## significance is judged by a "support" metric, not sure what this is
str(OmegaCor[[1]][2])

supportLevel = 0.95 ## what is this? some sort of credible interval?

toPlot = ((OmegaCor[[1]]$support>supportLevel)
        + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean

corrplot(toPlot, method = "color",
         col=colorRampPalette(c("blue","white","red"))(200),
         tl.cex=.6, tl.col="black",
         title=paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))

## no associations, because synthetic data, not included

summary(mpost$Rho)

## we can predict what a community might do over an environmental gradient:

Gradient = constructGradient(m,focalVariable = "climate",
                   non.focalVariables = list("habitat"=list(3,"open")))

Gradient$XDataNew

predY = predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew,
                ranLevels=Gradient$rLNew, expected=TRUE)

plotGradient(m, Gradient, pred=predY, measure="S", showData = TRUE)


## individual species response can be modeled:

par(mfrow=c(1,2))
plotGradient(m, Gradient, pred=predY, measure="S", index = 1, showData = TRUE)
plotGradient(m, Gradient, pred=predY, measure="Y", index = 2, showData = TRUE)

## trait values of the group changing with a gradient:
plotGradient(m, Gradient, pred=predY, measure="T", index = 3, showData = TRUE)


## "gradients" can be constructed for categorical variables?:

Gradient = constructGradient(m,focalVariable = "habitat",
                             non.focalVariables = list("climate"=list(1)))

Gradient$XDataNew

## ah, I get it. This is like a parameter sweep, where we hold everything else
## constant and just change the variable of interest.

## so in this case we have a trait for "preference for forest". We can select the species most 
## responsive to habitat and watch its response to being in either a forest or meadow:

predY = predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew,
                ranLevels=Gradient$rLNew, expected=TRUE)

plotGradient(m, Gradient, pred=predY, measure="Y", index=which.max(m$TrData$habitat.use),
             showData = TRUE, jigger = 0.2)

## predicted to drop, make sense.

## and we can see how the community mean for this trait will drop with a habitat-change:

plotGradient(m, Gradient, pred=predY, measure="T", index=2, showData = TRUE, jigger = 0.2)

## same story. The number of microbes with the "forest-preference trait" will increase
## when the community is subject to forests.

## to get closer to reality, they give the example of trying to model the simulated 
## community without all the predictors that were used to make the data
## so it's like real life, when we are hypothesizing what is driving the community
## dynamics, but don't really know.

## for instance, trying to explain the community composition with climate only:

XFormula.1 = ~poly(climate, degree = 2, raw = TRUE)
ma50 = Hmsc(Y=Y, XData=XData, XFormula = XFormula.1,
            TrData = traits, TrFormula = TrFormula,
            phyloTree = phy,
            studyDesign=studyDesign, ranLevels=list(sample=rL))

ma50 = sampleMcmc(ma50, thin = thin, samples = samples, transient = transient,
            nChains = nChains, nParallel = nChains, verbose = verbose)

print ("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! done !!!!!!!!!!!!!!!!")
print ("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! done !!!!!!!!!!!!!!!!")
print ("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! done !!!!!!!!!!!!!!!!")

## and so we have a lot more variance from randome effects

VP = computeVariancePartitioning(ma50, group = c(1,1,1), groupnames=c("climate"))
plotVariancePartitioning(ma50, VP = VP)

## which is now ascribed partially due to species associations, because 

OmegaCor = computeAssociations(ma50)

supportLevel = 0.95
toPlot = ((OmegaCor[[1]]$support>supportLevel)
  + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean

corrplot(toPlot, method = "color",
col=colorRampPalette(c("blue","white","red"))(200),
tl.cex=.6, tl.col="black",
title=paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))


## and other stuff about traits, controlling shrinkage on the hierarchical model 
## etc, etc. But generally, got the idea. 

## there is another tutorial, about spatial datasets. Seems pertinent:

library(Hmsc)
library(MASS)

set.seed(6)

## 100 sites, 5 species:

n = 100
ns = 5
beta1 = c(-2,-1,0,1,2)
alpha = rep(0,ns)
beta = cbind(alpha,beta1)
x = cbind(rep(1,n),rnorm(n))
Lf = x%*%t(beta)
xycoords = matrix(runif(2*n),ncol=2)
colnames(xycoords) = c("x-coordinate","y-coordinate")
rownames(xycoords) = 1:n

## exponentially decreasing autocorrelation model:
sigma.spatial = c(2)
alpha.spatial = c(0.35)
Sigma = sigma.spatial^2*exp(-as.matrix(dist(xycoords))/alpha.spatial)
eta1 = mvrnorm(mu=rep(0,n), Sigma=Sigma)
lambda1 = c(1,2,-2,-1,0) ## species spatial residuals
Lr = eta1%*%t(lambda1) ##  
L = Lf + Lr ## linear function of effects of environmental and spatial function
y = as.matrix(L + matrix(rnorm(n*ns),ncol=ns))
yprob = 1*((L +matrix(rnorm(n*ns),ncol=ns))>0)
XData = data.frame(x1=x[,2])

rbPal = colorRampPalette(c('cyan','red'))
par(mfrow=c(2,3))
Col = rbPal(10)[as.numeric(cut(x[,2],breaks = 10))]
plot(xycoords[,2],xycoords[,1],pch = 20,col = Col,main=paste('x'), asp=1)
for(s in 1:ns){
    Col = rbPal(10)[as.numeric(cut(y[,s],breaks = 10))]
    plot(xycoords[,2],xycoords[,1],pch = 20,col = Col,main=paste('Species',s), asp=1)
    }

## this diagram is useful for showing that autocorrelation is hard to pick up by eye 
## sometimes

####

## make a model spatial by adding a random effect with a spatial argument
 
studyDesign = data.frame(sample = as.factor(1:n))
rL.spatial = HmscRandomLevel(sData = xycoords) ## here
rL.spatial = setPriors(rL.spatial,nfMin=1,nfMax=1) #We limit the model to one latent variables for visualization
m.spatial = Hmsc(Y=yprob, XData=XData, XFormula=~x1,
                 studyDesign=studyDesign, ranLevels=list("sample"=rL.spatial),distr="probit")

## sample:
nChains = 2
thin = 10
samples = 1000
transient = 1000
verbose = 1

m.spatial = sampleMcmc(m.spatial, thin = thin, samples = samples, transient = transient,
                       nChains = nChains, nParallel = nChains, verbose = verbose,
                       updater=list(GammaEta=FALSE))

## they skip checking the convergences

#Explanatory power

preds.spatial = computePredictedValues(m.spatial)

MF.spatial = evaluateModelFit(hM=m.spatial, predY=preds.spatial)

?evaluateModelFit

MF.spatial

partition = createPartition(m.spatial, nfolds = 2, column = "sample")

cvpreds.spatial = computePredictedValues(m.spatial, partition=partition,
                          nParallel = nChains, updater=list(GammaEta=FALSE))

mpost.spatial = convertToCodaObject(m.spatial)

plot(mpost.spatial$Alpha[[1]])

mpost.spatial = convertToCodaObject(m.spatial)

plot(mpost.spatial$Alpha[[1]])

summary(mpost.spatial$Alpha[[1]])

## try the model without the spatial component (so no random effects)
m = Hmsc(Y=yprob, XData=XData, XFormula=~x1, studyDesign = studyDesign, distr="probit")

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
                nChains = nChains, nParallel = nChains, verbose = verbose)


preds = computePredictedValues(m)

MF = evaluateModelFit(hM=m, predY=preds)

MF

partition = createPartition(m, nfolds = 2, column = "sample")
preds = computePredictedValues(m, partition=partition, nParallel = nChains)

MF = evaluateModelFit(hM=m, predY=preds)

MF



### ok great...anything else we need to know before we dive into the real data?

## don't think so. 

## first step, what format do we want our data in?

## should use the >50, log-transformed data. 

## we have a lot of predictors. Model might get too
## complex. 

## maybe start without spatial effects, find best predictors,
## then include the spatial effects

####### try out HMSC on sulari data ##############

## still on lab comp

R

library(phyloseq)
library(vegan)
library(Hmsc)
library(corrplot)
library(ape)
library(MASS)
library(fields) 
library(knitr)

## sulari  community data:
comData <- read.csv("../sulariData/comdat.csv", row.names=1)
envData <- read.csv("sulariEnv.csv", row.names=1)

tail(comData)[,1:5]
tail(envData)[,1:5]

## let's just start working through the high-dim tutorial, using our 
## data. I think we need a 16s tree of our sequences...

## we need to get the reduced ~5000 ASV sequences out into fasta form:


load("../sulariData/sularilogMin50ps.rda")

## get rid of zero columns

logMin50ps = prune_taxa( taxa_sums(logMin50ps) > 0, logMin50ps )

## export the sequences as a fasta file for the aligner:

?Biostrings::writeXStringSet

Biostrings::writeXStringSet(refseq(logMin50ps), "sulariAbundantASV16s.fna", append=FALSE,
                                  compress=FALSE, compression_level=NA, format="fasta")

## align them with ssu-aligner:

## try it on the local desktop

conda activate spatialDirt

#conda install bioconda::ssu-align

## align these with SSU align
## as per https://www.biostars.org/p/11377/, 
## to produce a single tree from both archea and bact,
## we need to designate a single model:

ssu-align -n bacteria sulariAbundantASV16s.fna sulariAbundantASV16s_ali
## then a strict mask, don't trust any ambiguous calls
ssu-mask --pf 0.9999 --pt 0.9999 sulariAbundantASV16s_ali

## to get a fasta output of the alignment (what we probably need)
ssu-mask --stk2afa sulariAbundantASV16s_ali

ssu-mask --stk2afa sulariAbundantASV16s_ali

grep ">" sulariAbundantASV16s_ali/sulariAbundantASV16s_ali.bacteria.afa | wc -l ## 4300 sequences
grep ">" sulariAbundantASV16s.fna | wc -l ## 4370 sequences

## we lost 70 sequences. Probably all of our archea. 
## not sure 
## not sure if the HMSC can handle missing asvs
## we can try anyway. 

## use fastree? phyml?

## start with phyml:

## but our alignment 

## tree building... 

## try fasttree:

mv /home/daniel/Downloads/FastTree /home/daniel/.local/bin

ssuAlignOut="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariAbundantASV16s_ali/sulariAbundantASV16s_ali.bacteria.afa"
ls $ssuAlignOut

FastTree -gtr -nt < $ssuAlignOut > sulariFastTree.nwk 

## installing arb to look at this tree:

wget http://download.arb-home.de/release/latest/arb-7.0.ubuntu2004-amd64.tgz
wget http://download.arb-home.de/release/latest/arb_install.sh

less /usr/arb/arb_UBUNTU.txt

## looks the binaries need:
sudo apt install gnuplot
sudo apt install gv
sudo apt install libmotif-common
sudo apt install xfig
sudo apt install transfig
sudo apt install xterm

sudo bash arb_install.sh

sudo bash arb_installubuntu4arb.sh

## okay, seems to run. can look at the tree that way, 
## looks like a nice tree
## no use for it right now.  

## for arb:
## 1. import alignment, keeping gaps (necessary? not sure)
## 2. go to "tree admin", import, change suffix seach to "nwk"

## try everything out for a bit, then update yaml backup for the repo


conda activate spatialDirt 

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
from matplotlib_scalebar.scalebar import ScaleBar
import pymc as pm


spatDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
## ^different on laptop
os.chdir(spatDir)

## we just want a map of points right now:

## on officeComp
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"
## on laptop
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"

envData = pd.read_csv(sulariEnvCSV)
## get rid of spaces
envData.rename({"Sample ID":"SampleID"}, axis="columns", inplace=True)
## we need to clean up the plot.ID. Sulari recorded season in the plot IDs,
## using letter codes. Also she has one double sampling, with an underscore
envData['Plot.ID'] = envData['Plot.ID'].str.slice(0,5)

## let's get rid of decimals in the names, weird for python work:
envData.rename({"Plot.ID":"PlotID", 
       "soil.respiration":"soil_respiration",
              "Land.type":"Land_type"}, 
                axis="columns", inplace=True)

envData.head()

## so we don't have to repeat:
envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index=False)

envData.head()


envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
 

## if we need to drop controls
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

## her otu table is really large:
##### R ###
library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")
logMin50ps
comdat <- as.data.frame(otu_table(logMin50ps))
write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")
############

## back in python

comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", index_col=0)

comData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

comData.head()

## the map below looks funny...looks like every forest plot has 
## a grassland under it...

dupsFilter = envData.Latitude.duplicated()
dupped = envData[dupsFilter].sort_values(by="Latitude")

dupped.groupby('Plot.ID').nunique()

envData.iloc[0:5,0:8]

## nope...looks okay

## we want a geodf...

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

## to convert to UTM? looks like we are in zone 33
## looks like:
## EPSG:32633

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

sulariPlot_utm.tail()

## can we import our georeferenced tif of the study area?
## lat/lon
#fichtelMap = rasterio.open("studyAreaClipped_modified.tif")
## UTM

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color code our points according to land type?
cdik={
"Arableland":"b",
"Grassland":"y",
"Forest":"g",
## they use a prevalence threshold of 10% of PA sums (0.1) or greater, throwing out other PA columns
cond1=!(colSums(Fungi.pa)<=threshold.prev*ny)
## and only species contain at least one instance total relative abundance of 0.005 at a site
cond2=apply(Fungi.rel,2,max)>=threshold.abu

## after this the matrix is much smaller.
Fungidata <- Fungi[,cond1 & cond2]

Fungidata[1:5,1:5]

dim(Fungidata) ## only 452 species remain
## I'm not sure if we would do something similar. 

## but this is their community matrix:

Y <- as.matrix(Fungidata)

ny = dim(Y)[1]
ns = dim(Y)[2]


Env <- read.csv(file.path(dataDir,"env.csv"),sep=";")

FungiDepth<-log(rowSums(Fungi)) ## add in sequencing depth
Env$DecayTime<-as.factor(Env$Year)
levels(Env$DecayTime)<-list("<5"="2013","5-15"="2008","16-38"="1997",">38"="1975")

## however, think we have to fix the tree categorical variable:
Env$Tree<-as.factor(Env$Tree)

head(Env)

## 

XData<-data.frame(Env[,c(1,11,8,3,4,5,6,7)],FungiDepth)
## they take the log of the nitrogen values, why? Is this usually done?

XData$N = log(XData$N)
## standardize the diameter of the fallen wood.
## why only this variable? the rest are normalized later. 
XData$diam = scale(XData$diam)

head(XData)

## here the rest is scaled:
XData[,-c(1,2)]=scale(XData[,-c(1,2)])

head(XData)

## I think they use the trait slot for denoting 
## fungal or bacterial otu

XDataList = list()
for (j in 1:2){
   for (i in 1:ns){
      tmp = XData
      tmp$Depth = XData$FungiDepth
      XDataList[[i+(j-1)*ns]] = tmp
   }
}


XDataList[[1]] ## I think this isn't necessary for us, they are just giving the 
## different seq depth data for bact vs. fungi


## then they make the dataframe of relative abundances:

Yabu = Y
## zeros become missing values
Yabu[Y==0] = NA
Yabu = log(Yabu)

## another scaling/centering?
for (i in 1:ns){
   Yabu[,i] = Yabu[,i] - mean(Yabu[,i],na.rm=TRUE)
   Yabu[,i] = Yabu[,i] / sqrt(var(Yabu[,i],na.rm=TRUE))
}


Ypa = 1*(Y>0)

## and they put the P/A and the relative abundance  matrices side-by-side.

Y = cbind(Ypa,Yabu)

#### build model ###

sample.id = rep(NA,ny)

for(i in 1:ny){
   sample.id[i] = paste0("log_",as.character(i))
}

studyDesign = data.frame(Sample=as.factor(sample.id))

## set a random effect by site, explained 5-10 latent variables. 
## not sure what they mean by setPriors, I don't see any 
## specification of priors here. 
rL.Sample = setPriors(HmscRandomLevel(units = studyDesign$Sample), nfMin = 5, nfMax = 10)

## set formulas:

# REGRESSION MODEL FOR ENVIRONMENTAL COVARIATES.
XFormula = ~ water + pH + N + C + lig + diam + Tree + DecayTime + FungiDepth

## null model, to check for seq depth effects??:
XFormula1 = ~ Depth

thin = 100
samples = 1000
nChains = 4
set.seed(1)


## model. Probit, I guess this is makes sense for the PA in the X data
## but I thought this hurdle model was a mixture model...

m = Hmsc(Y=Y,
         XData = XData,  XFormula = XFormula,
         distr={"probit"} ,
         studyDesign=studyDesign, ranLevels={list(Sample=rL.Sample)})

## ah, here is the hurdle implementation
## here they change the distributions 
## the distr=probit above gives a value of 2,
## not sure how the dispersion parameter 
## set to zero came to be
## but now for the latter half of the 
## species (which are a repeat of the 
## the first, but with relative abundances
## instead of P/A). 
## the 1,1 setting indicates poisson 
## with log-link function

for (i in (ns+1):(2*ns)){
   m$distr[i,1:2] = c(1,1)
}


## try everything out for a bit, then update yaml backup for the repo


conda activate spatialDirt 

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
from matplotlib_scalebar.scalebar import ScaleBar
import pymc as pm


spatDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
## ^different on laptop
os.chdir(spatDir)

## we just want a map of points right now:

## on officeComp
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"
## on laptop
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"

envData = pd.read_csv(sulariEnvCSV)
## get rid of spaces
envData.rename({"Sample ID":"SampleID"}, axis="columns", inplace=True)
## we need to clean up the plot.ID. Sulari recorded season in the plot IDs,
## using letter codes. Also she has one double sampling, with an underscore
envData['Plot.ID'] = envData['Plot.ID'].str.slice(0,5)

## let's get rid of decimals in the names, weird for python work:
envData.rename({"Plot.ID":"PlotID", 
       "soil.respiration":"soil_respiration",
              "Land.type":"Land_type"}, 
                axis="columns", inplace=True)

envData.head()

## so we don't have to repeat:
envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index=False)

envData.head()


envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
 

## if we need to drop controls
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

## her otu table is really large:
##### R ###
library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")
logMin50ps
comdat <- as.data.frame(otu_table(logMin50ps))
write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")
############

## back in python

comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", index_col=0)

comData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

comData.head()

## the map below looks funny...looks like every forest plot has 
## a grassland under it...

dupsFilter = envData.Latitude.duplicated()
dupped = envData[dupsFilter].sort_values(by="Latitude")

dupped.groupby('Plot.ID').nunique()

envData.iloc[0:5,0:8]

## nope...looks okay

## we want a geodf...

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

## to convert to UTM? looks like we are in zone 33
## looks like:
## EPSG:32633

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

sulariPlot_utm.tail()

## can we import our georeferenced tif of the study area?
## lat/lon
#fichtelMap = rasterio.open("studyAreaClipped_modified.tif")
## UTM

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color code our points according to land type?
cdik={
"Arableland":"b",
"Grassland":"y",
"Forest":"g",
}
sulariPlotsDF['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]
## by the square of the variance...this is pretty transformed data by the end of it all

## I think this is just a z-score standardization
for (i in 1:ns){
   Yabu[,i] = Yabu[,i] - mean(Yabu[,i],na.rm=TRUE)
   Yabu[,i] = Yabu[,i] / c(sqrt(var(Yabu[,i],na.rm=TRUE)))
}

## so is this the same as doing?

Yabu2 <- Y
Yabu2[Y==0] <- NA
Yabu2 = log(Yabu2) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
bb <- scale(Yabu2)

Yabu[1:5,1:5]
bb[1:5,1:5]

all(Yabu[1:5,1:5] == bb[1:5,1:5], na.rm=TRUE)
all(Yabu == bb, na.rm=TRUE) ## nope, why? Looks the same
cc = which(Yabu != bb, arr.ind=TRUE)
Yabu[119,302] == bb[119,302] ## no, but...
Yabu[119,302] - bb[119,302] ## 2.775558e-17

## floating decimal difference. these are the same. ugh, waste of time.

## in the future, use:
Yabu <- Y
Yabu[Y==0] <- NA
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu)

## make our second, presence absence matrix:

Ypa = 1*(Y>0)

## bring them together for a final Y matrix
Y = cbind(Ypa,Yabu)

## below, found out that we have a lot of missing
## X values, especially from moisture. So for the moment,
## just to try out the model, let's drop the moisture 
## column, and keep only complete rows after that:

#XData = XData[,c("soil_respiration","Land_type","pH","N","C","Temperature","Moisture", "BacteriaDepth")]

## for the moment, gotta leave out moisture, missing too much data.
XData = XData[,c("soil_respiration","Land_type","pH","N","C","Temperature","BacteriaDepth")]

ccases <- complete.cases(XData) ## 112 samples, we only lose 8 

XData = XData[ccases,] 
Y = Y[ccases,] 

ny = dim(Y)[1]
ns = dim(Y)[2]/2

rownames(XData) == rownames(Y)

## in the future, I guess we can check out the watercontent variable in Betty's data 
## see how correlated it is, 

## for now, keep moving, want to try a model today

# STUDY DESIGN

sample.id = row.names(Y)
studyDesign = data.frame(Sample=as.factor(sample.id))
rL.Sample = setPriors(HmscRandomLevel(units = studyDesign$Sample), nfMin = 5, nfMax = 10) ## not sure how to optimize the number of laten variables?

# REGRESSION MODEL FOR ENVIRONMENTAL COVARIATES.
XFormula = ~ soil_respiration + Land_type + pH + N + C + Temperature
XFormula1 = ~ BacteriaDepth ## check this next. Sort of a null model, I guess.

m = Hmsc(Y=Y,
         XData = XData,  XFormula = XFormula,
         distr={"probit"} ,
         studyDesign=studyDesign, ranLevels={list(Sample=rL.Sample)})

## huh, NA's not allowed in the xdata

sum(complete.cases(XData)) ## 91 complete cases, out of 120. Shoot. That is a lot of data lost. 

colSums(is.na(XData)) ## Moisture is the problem.  Corrected above.

## change the priors for the abundance matrix:


head(m$distr)

tail(m$distr)

View(m$distr)

for (i in (ns+1):(2*ns)){
   m$distr[i,1:2] = c(1,1)
}

## try this:
thin = 1
samples = 1000
nChains = 4

#m = sampleMcmc(m, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = 4,verbose=1)


## looks good, no errors. Cut it off and run remotely. What do we need to do make a script for this?

## save out the model, unsampled

save(m, file="firstHMSCmodelCarbon4d.rds")


## script: ## mcmcSample.R
library(Hmsc)
thin = 1
samples = 1000
nChains = 4
setwd("/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis")
print(paste("start time is", Sys.time()))
load("firstHMSCmodelCarbon4d.rds")
m = sampleMcmc(m, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = 4,verbose=1)
save(m, file="firstHMSCmodelCarbon4d_sampled.rds")
print(paste("finish time is", Sys.time()))

## that only took ten minutes
## we can expand the above to include many more rare species, I think.

## now rerun it with more ASVs...

nohup Rscript mcmcSample.R &> mcmcSample.log & ## that ran for an hour and ten minutes. 

## very doable. Add in space and seasonality/time? 

## at least add it moisture. 

## that is on this "sciebo" site:

## for the moment, run through some of the tutorials to see if it actually worked. 

## or maybe the analysis that the paper used..

https://uni-muenster.sciebo.de/s/Ke1maWrj1cBHwVl?path=%2F

## soildata/master.csv, or something like that.

## read

dataDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
Env <- read.csv(file.path(dataDir,"sulariEnv.csv"), row.names="SampleID")
Env <- Env[!(rownames(Env) %in% c("C1.1","C1.2","C2.1","C2.2")),]

head(Env)

bettyData <- read.csv(file.path(dataDir,"master.csv"))

## we need only zero depths for sulari data

filt0 <- bettyData$depth == 0 

bettyData0 <- bettyData[filt0,]

dim(bettyData0)

head(bettyData0)

bettyData0$watercont 

bettyData0$plotID

## ugh, let's do this in python. 

conda activate spatialDirt

python3 

import pandas as pd
import os
import matplotlib.pyplot as plt; plt.ion()

#envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
envData = pd.read_csv('sulariEnv.csv', index_col='SampleID')
envData['Date'] = pd.to_datetime(envData['Date'], yearfirst=True, errors='coerce')

envData.dtypes

bettyData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/master.csv', parse_dates=['date'])
filt0 = bettyData['depth'] == '0'
bettyData0 = bettyData[filt0]

## all there?
bettyData0['watercont'].all()

## missing data are back. Thanks Betty.
bettyData[bettyData['plotID'] == "P0216"]
bettyData[bettyData['plotID'] == "P0116"]
bettyData[bettyData['plotID'] == "P0165"]


## how do we link these up, joining by plotid isn't enough, because of the multiple sampling in time
## do the dates agree?


bettyData0.plotID[bettyData0.plotID.duplicated()] ## yep lots of dups/resamples. This is a pain now, useful later.


## go through each of sulari's samples, find matching plotID and date
## in betty's data, pull out the watercont

## one line:

#s="S1"
s="S2"
#envData.loc[s]
a = envData.loc[s]['PlotID']
b = envData.loc[s]['Date'].strftime("%Y-%m-%d")
c = bettyData0.query(f'plotID == "{a}" & date == "{b}"')['watercont']

## in a function:

def get_waterCont(sample):
    a = envData.loc[sample]['PlotID']
    b = envData.loc[sample]['Date'].strftime("%Y-%m-%d")
    c = (bettyData0.query(f'plotID == "{a}" & date == "{b}"')['watercont'])
    try:
        c = c.to_list()[0]
    except:
        c = float("NaN")
    return(c)


sample="S1"

sample="S10"

sample="S116"

aa = get_waterCont(sample)

## looks okay.

aa = envData.reset_index()['SampleID'].apply(get_waterCont)

aa.index=envData.index


envData['waterCont'] = aa

envData.head()

envData.tail()

## sanity check:

## missing data:
aa.isna().any() ## y

envData[aa.isna()] ## three samples:

## there are three sites where data are missing
P0216 P0116 P0165

envData[envData$PlotID %in% c('P0216','P0116','P0165'),]
envData[envData$PlotID %in% c('P0216','P0116','P0165'),]

## are Moisture and waterCont correlated?

help(envData.plot)

envData.plot("Moisture", "waterCont", kind="scatter")

## looks good, very correlated

## One weird outlier. What is this point?

X,Y = plt.ginput(1)[0] ## 26.424, 0.549

bettyData0.query(f'plotID == "{a}" & date == "{b}"')['watercont'])

envData.query("waterCont > 0.5") ## 

## that was confusing. save this as our new environmental 
## matrix:

envData.to_csv('sulariEnv.csv')

## okay, so we need a new model, that includes the spatial
## and maybe temporal components?

## for the analysis we need to, not necessarily in order:

## 1 - remove the MC species, can't trust them
## 2 - build spatiatemporal model without temporal component
## 3 - build with temporal component
## 4 - compare

## to remove the mock community species, back to R:

R

library('phyloseq')

sulariDataDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/"
load(paste0(sulariDataDir,"sulariPhyloseqObject.rda"))
psNoControl = prune_samples(!(rownames(sample_data(ps)) %in% c("C1.1","C1.2","C2.1","C2.2")), ps)
## remove species that are now zero
psNoControl = prune_taxa( colSums(otu_table(psNoControl)) > 0, psNoControl)

## and how do we find our ASVs from the MC control?

## These will be the most abundant asvs in our positive controls
## the taxonomy should line up, also, with our notes

sample_names(ps) 

## the single species controls are obvious:
otu_table(ps)["C1.1",1:10]

c1.1 <- sort(otu_table(ps)@.Data["C1.1",],decreasing=TRUE)
c1.1 <- c1.1[c1.1 > 0]

c1.2 <- sort(otu_table(ps)@.Data["C1.2",],decreasing=TRUE)
c1.2 <- c1.2[c1.2 > 0]

## the most abundant ASVs in these:

c1.1[1:15]

c1.1ASVs <- c("ASV1","ASV1315","ASV997","ASV29","ASV39","ASV43","ASV62","ASV4162")

c1.2[1:15]

c1.2ASVs <- c("ASV1","ASV1315","ASV997","ASV4162","ASV29","ASV43","ASV39","ASV62")

## what is the taxonomy of these?

tax_table(ps)[c1.1ASVs,]

tax_table(ps)[c1.2ASVs,]

## there are several possible MC contaminants in here
## ASV1315 is probably a split ASV off of ASV1
## ASV29 bacillus
## ASV39 Chryseobacterium

c2.1 <- sort(otu_table(ps)@.Data["C2.1",],decreasing=TRUE)
c2.1 <- c2.1[c2.1 > 0]

c2.2 <- sort(otu_table(ps)@.Data["C2.2",],decreasing=TRUE)
c2.2 <- c2.2[c2.2 > 0]

## and these are:

c2.1[1:20]

c2.1ASVs <- c("ASV29","ASV39","ASV43","ASV62","ASV70","ASV86","ASV78","ASV200","ASV244","ASV275","ASV365","ASV1","ASV943","ASV997","ASV4")

c2.2[1:20]
c2.2ASVs <- c("ASV29"," ASV39"," ASV43"," ASV70"," ASV62"," ASV86"," ASV78","ASV200","ASV244","ASV275 ASV365","ASV1","ASV943","ASV997")


tax_table(ps)[c2.1ASVs,]

tax_table(ps)[c2.2ASVs,]

## how common are these elsehwere? 

otu_table(ps)[,"ASV29"]

sum(otu_table(ps)[,"ASV29"] > 0) ## 30 sites, out 120. Again, don't think we can trust this. 
## Too bad, because bacillus subtilis is actually known from soil

## franconibacter is an unusual one, I think:
sum(otu_table(ps)[,"ASV943"] > 0) ## 2 samples, as expected 
otu_table(ps)[,"ASV943"] ## not found outside of mock community. Great. 

## so index bleed is mostly a problem with the really abundant microorganisms

## clean up the index bleed problem and rerun the notebook with the cleaner dat
## then start again on the models  

## which are the members of the mock community, as far as we can tell?:

## C1.x controls are all e. coli, ASV1 and probably also ASV1315, and ASV997

otu="ASV1"
sum(otu_table(ps)[,otu] > 0) ## 58 samples, over half
otu_table(ps)[,otu] ## ~1000 reads in the mock community standards. elsewhere <500 reads 

otu="ASV1315"
sum(otu_table(ps)[,otu] > 0) ## only in c1.1/2, didn't bleed
otu_table(ps)[,otu] ## ~500 reads

otu="ASV997"
sum(otu_table(ps)[,otu] > 0) ## 
otu_table(ps)[,otu] ## funny, highly concentrated in controls, but present in a cluster of 
## sites S33-S40 at lower levels (40-120 reads each), probably not enough to matter but can
## be removed here to be sure

## so probably get rid of ASV1 and ASV99

## also of interest in the controls are four ASVs that 
## are probably mock community species:

ASV29   "Bacillaceae"        "Bacillus"
ASV39   "Weeksellaceae"      "Chryseobacterium"
ASV43   "Clostridiaceae"     "Clostridium sensu stricto 11"
ASV62   "Enterobacteriaceae" "Klebsiella"

otu="ASV29"
sum(otu_table(ps)[,otu] > 0) ## 
otu_table(ps)[,otu] ## 

otu="ASV39"
sum(otu_table(ps)[,otu] > 0) ## 
otu_table(ps)[,otu] ## 

otu="ASV43"
sum(otu_table(ps)[,otu] > 0) ## 
otu_table(ps)[,otu] ## 

otu="ASV62"
sum(otu_table(ps)[,otu] > 0) ## 
otu_table(ps)[,otu] ## 

otus=c("ASV29", "ASV39", "ASV43", "ASV62","ASV86","ASV70","ASV200")
otu_table(ps)[,otus] ## 

## site 120 seems to be contaminated more than any other by the controls  
sum(otu_table(ps)["S120",])

## the paper I'm following used a minimum abundance of 0.005 of a site to be considered:

head(tax_table(ps))


sum(otu_table(ps)["S120",])*.005 ## anything above 337.76 would be kept

## so some index bleed by those MC members, but probably not enough to worry about

## check the others:

c2.1 <- sort(otu_table(ps)@.Data["C2.1",],decreasing=TRUE)
c2.1 <- c2.1[c2.1 > 0]
c2.2 <- sort(otu_table(ps)@.Data["C2.2",],decreasing=TRUE)
c2.2 <- c2.2[c2.2 > 0]

c2.1[1:20]

c2.1ASVs <- c("ASV29","ASV39","ASV43","ASV62","ASV70","ASV86","ASV78","ASV200","ASV244","ASV275","ASV365","ASV1","ASV943","ASV997","ASV4")

otu_table(ps)[,c("ASV1",c2.1ASVs)]

sum(otu_table(ps)[,"ASV1"] > 0)


tax_table(ps)["ASV70",]

tax_table(ps)["ASV943",] ## franconibacter

tax_table(ps)["ASV997",] ## plesiomonas 


c2.2[1:20]
c2.2ASVs <- c("ASV29"," ASV39"," ASV43"," ASV70"," ASV62"," ASV86"," ASV78","ASV200","ASV244","ASV275 ASV365","ASV1","ASV943","ASV997")

## for example, the Klebsiella/Enterobactor
otu="ASV62"
sum(otu_table(ps)[,otu] > 0) ## shows up in 38 samples
otu_table(ps)[,otu] ## but mostly in trivial amounts, < 60 reads

## Bacillus subtilis:
otu="ASV29"
otu_table(ps)[,otu] ## but mostly in trivial amounts, =< 100 reads
sum(otu_table(ps)[,otu] > 0) ## shows up in 30 samples

rowSums(otu_table(ps)) ## 

rowSums(otu_table(ps))*0.005 ## in general, a 0.5% threshold requires ~ 300 reads 
rowSums(otu_table(ps))*0.003 ## in general, a 0.3% threshold requires ~ 100-200 reads 

## so we could go to 0.003 (.3%) and get rid of these index-bleeds

## okay, so the only real index bleed is with E. coli. Minimum thresholds
## should take care of the rest.

c2.1ASVs

tax_table(ps)[c2.1ASVs,][,'Genus']
c2.1[1:20]

tax_table(ps)[c2.2ASVs,]

## need to update the notebook with the above.
## standardize the data we are using a bit.

## best approach here - include a section at the beginning of the 
## notebook with these poking/prodding data, 

## also the plotting function that we used with sulari's 
## thesis:

rankAb <- function(phyObj, sampleName, ylimit=500, ntax=NULL, textatX=100, textatY=(ylimit-40)){
    sampleNo0filter <- get_taxa(phyObj, sampleName) > 0
    if(is.null(ntax)) ntax=sum(sampleNo0filter)
    print(sum(sampleNo0filter)) ## let user know how many unique taxa are in sample
    sampleNo0 <- get_taxa(phyObj, sampleName)[sampleNo0filter]
    sampleNo0 <- sort(sampleNo0, decreasing=TRUE)
    sampleNo0 <- sampleNo0[1:ntax]
    taxaNames=tax_table(phyObj)[ names(sampleNo0), "Genus"]
    nuASV <- paste("number of unique ASVs = ",sum(sampleNo0filter), sep="")
    par(cex.axis = .75, mar=c(10,4,4,2))
    barplot(sampleNo0,
        ylim = c(0,ylimit),
        main=sampleName,
        cex.main=2,
        las=2,
        names.arg=taxaNames,
        mar=c(20,10,2,2))
    text(textatX, textatY,  nuASV, cex = 2, )
}
 
rankAb(ps,"C1.1",ylimit=3000, ntax = 30)

par(cex.lab=20)

par(mar = c(30, 4.4, 4.1, 1.9))

par(mar = c(0, 0, 0, 0))

rankAb(ps,"C2.1",ylimit=10000, ntax = 30)

rankAb(ps,"C2.2",ylimit=10000, ntax = 30)


| DSMZ name | (Sub)Phylum | Classified as | ASV# |
|  :--- | :---: | :---: | ---: |
|  Bacillus subtilis          | Bacillota            | Bacillus                                           | ASV29  |
|  Chryseobacterium luteum    | Bacteroidota         | Chryseobacterium                                   | ASV39  |
|  Clostridium roseum         | Bacillota            | Clostridium sensu stricto 11                       | ASV43  |
|  Enterobacter mori          | Gammaproteobacteria  | Klebsiella                                         | ASV62  |
|  Franconibacter helveticus  | Gammaproteobacteria  | Franconibactor                                     | ASV86, ASV943  |
|  Glycomyces tenuis          | Actinomycetetota     | Glycomyces                                         | ASV200  |
|  Pseudomonas fluorescens    | Gammaproteobacteria  | Pseudomonas                                        | ASV70, ASV275  |
|  Pseudomonas putida         | Gammaproteobacteria  | Pseudomonas                                        | ASV70, ASV275  |
|  Rhizobium endophyticum     | Alphaproteobacteria  | Allorhizobium-Neorhizobium-Pararhizobium-Rhizobium | ASV78  |
|  Streptomyces acidiscabies  | Actinomycetatota     | Streptomyces                                       | ASV244  |
|  Variovorax guangxiensis    | Betaproteobacteria   | Variovorax                                         | ASV365  |
|  E. coli                    | Gammaproteobacteria  | Escherichia-Shigella                               | ASV365, ASV1315 |


## great. So how do we remove all this contamination? I think our original plan of 
## a minimum cutoff is better than a minimum % prevalence per sample as 
## used by Odriozola et al. 

## so enforce a minimum cutoff, and normalize for each site and take the log 
## essentially the same pipeline 
## also, just get rid of ASV1. 

## all this should be done in phylseq before exporting the community to downstream 
## software:


R

library('phyloseq')

sulariDataDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/"
load(paste0(sulariDataDir,"sulariPhyloseqObject.rda"))

psNoControl = prune_samples(!(rownames(sample_data(ps)) %in% c("C1.1","C1.2","C2.1","C2.2")), ps)
## remove species that are now zero
psNoControl = prune_taxa( colSums(otu_table(psNoControl)) > 0, psNoControl)

## remove ASV1 wherever it is found:

notASV1 <- taxa_names(psNoControl) != "ASV1"
psNoControl <- prune_taxa(notASV1, psNoControl)

## what is our minimum cutoff? Here we have a dilemma. 
## our last sample, 120, is extremely contaminated
## by our positive controls. In the worst case,
## ASV29 is >200 reads 

rowSums(otu_table(psNoControl))

otu_table(psNoControl)[0:10,0:10]

## in which case do we lose more OTUs?
## a simple minimum cutoff, or a relative minimum presence

## they enforce a relative prevalence threshold in this way:
## first, make a relative abundance matrix, line ~3400 above
## then
threshold.prev = 0.02 ## each OTU is retain only if present in at least 2% of samples
threshold.abu = 0.001 ## only OTU that reaches at least 0.1% abundance of one sample
bact.pa<-ifelse(bactRaw>0,1,0) ## make PA mat
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples?
cond2=apply(bact.rel,2,max)>=threshold.abu ## in at least one sample, abundance reaches m%
bactData <- bactRaw[,cond1 & cond2]

## with these thresholds, do the contaminants pass through?

mcBiggest <- c("ASV29","ASV39","ASV43","ASV62","ASV70","ASV86","ASV78","ASV200")
for (i in mcBiggest){
  print(i)
  print(bactData[,i])
}

otu_table(ps)[,mcBiggest]

bactData[,mcBiggest]

## yes, nearly all make it through.
## I think we have to enforce a minimum cutoff.

## I think the best strategy is to go by each line 
## and find the highest abundance of contamination 
## by the mock community.

## essentially a site-by-site cutoff, using the most abundant members of the 
## mock communitiy members to decide the cutoff 

## ASVE 78 is everywhere, btw. What is this?
tax_table(ps)["ASV78",] ## Rhizobium spp. This could be real. As far as I know, a non-nodule forming rhizobium
## https://doi.org/10.1016/j.syapm.2010.07.005
## 

## interesting, B. subtilis is reported in thousands of 
## soils samples across the world, but we don't really
## see it here except in samples that are obviously 
## contaminated. 

## anyway, what happens with a minimum cutoff of 225, 
## which is the highest level of contamination that 
## we see in S120, with ASV29, Bacillus subtilis

minCutoff <- 50

## make a copy of our otu_table from our original phyloseq object:

ot <- otu_table(ps)
## if it doesn't meet this cutoff, go to zero
ot <- ot - 225
ot[ot < 0] <- 0
## insert this into a new phyloseq object
psMinCutoff <- ps
otu_table(psMinCutoff) <- ot
## how many reads do we lose with this?

sample_names(psMinCutoff)

otu_table(psMinCutoff)[,mcBiggest]

otu_table(psMinCutoff)["c2.1",]

## remove controls and zeros

## remove species that are now zero
psMinCutoff = prune_taxa( colSums(otu_table(psMinCutoff)) > 0, psMinCutoff)

psMinCutoff ## down to 643 taxa

## this clears out all the major contaminants

otu_table(psMinCutoff)[,mcBiggest]

otu_table(psMinCutoff)[c("C1.1","C1.2"),]

par(mfrow=c(1,2))
rankAb(psMinCutoff,"C1.1",ylimit=500)
rankAb(ps,"C1.1",ylimit=500, ntax = 30)
par(mfrow=c(1,1))

## okay, so this gets rid of all contaminants. 
## but all rare species are lost, I assume

## for instance, do we find 

Candidatus Udaeobacter

onlySulari <- grep("S", filtFs)

grep("Udaeobacter", tax_table(ps), value=TRUE)

sum(grepl("Udaeobacter", tax_table(ps))) 

grepl("Udaeobacter", tax_table(ps)) 

## oh wow. Lots. 
filt <- grep("Udaeobacter", tax_table(ps))
filt <- grepl("Udaeobacter", tax_table(ps)) 

tax_table(ps)@.Data[filt]

head(tax_table(ps))

any(tax_table(ps)[,'Phylum'] == "Candidatus Udaeobacter")

grep("Candidatus Udaeobacter", tax_table(ps)[,'Phylum']) ## nope

## here's where they are:
filt <- grep("Candidatus Udaeobacter", tax_table(ps)[,'Genus'])
tax_table(ps)@.Data[filt,]

## lots of udeabacter, 

filt <- grep("Candidatus Udaeobacter", tax_table(ps)[,'Genus'])
udeas <- rownames(tax_table(ps)@.Data[filt,])
otu_table(ps)[,udeas]

## yes, looks like we may lose a lot of information if we use these
## strict cutoffs.

## it will be more reasonable to do some ASV/OTU clustering, 
## then enforce the minimums. Less loss of data. 

data("esophagus")
# for speed
esophagus = prune_taxa(taxa_names(esophagus)[1:25], esophagus)

plot_tree(esophagus, label.tips="taxa_names", size="abundance", title="Before tip_glom()")
plot_tree(tip_glom(esophagus, h=0.2), label.tips="taxa_names", size="abundance", title="After tip_glom()")

## great, so how do we bring in our tree for this phyloseq object?
## get the tree into R...

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
aa <- read_tree(paste0(spatDir,"/sulariFastTree.nwk"))
phy_tree(ps) <- aa

print(paste("start time is", Sys.time()))
#bb <- tip_glom(ps, h=0.03) ## looks like family level in tree below
# bb <- tip_glom(ps, h=0.02) ## lost Franconibacter, presumably to the Enterobacter branch
#bb <- tip_glom(ps, h=0.02) ## still too much lumping
#bb <- tip_glom(ps, h=0.02) ## very close - two pseudomonas, one franconibacter, but streptomyces lumped into another ASV
#bb <- tip_glom(ps, h=0.01) ## same as .02
bb <- tip_glom(ps, h=0.005) ## and...looks good!
print(paste("finish time is", Sys.time()))

## the thing to do here would be to look at a positive controls, 
## and find the h-value that gets rid of splitting in
## E. coli, Franconibacter, any others 
## but one that keeps the pseudomonases separate
## to get a phylo tree of just the controls

cutoffAbu <- 100
cc = prune_samples((rownames(sample_data(bb)) %in% c("C1.1","C1.2","C2.1","C2.2")), bb)
cc = prune_taxa( colSums(otu_table(cc)) > 0, cc)
## retain only those OTUs with high abundances in both MC samples:
mcOTUs <- taxa_names(cc)[colSums(otu_table(cc)[c("C2.1","C2.2"),] > cutoffAbu) == 2]
dd = prune_taxa( c("ASV1","ASV1315",mcOTUs), cc)
sample_data(dd)$Plot.ID <- c("ss","ss","mc","mc")

plot(phy_tree(dd))

par(mfrow = c(1,2))
rankAb(sp,"C1.1", ylimit=5000, ntax=10)
rankAb(dd,"C1.1", ylimit=5000)
par(mfrow = c(1,1))

## make a nice tree of the original PS mock community tree for comparison
psControls = prune_samples((rownames(sample_data(ps)) %in% c("C1.1","C1.2","C2.1","C2.2")), ps)
psControls = prune_taxa( colSums(otu_table(psControls)) > 0, psControls)
mcOTUs <- taxa_names(psControls)[colSums(otu_table(psControls)[c("C2.1","C2.2"),] > cutoffAbu) == 2]
psControls = prune_taxa( c("ASV1", "ASV1315", mcOTUs), psControls)
sample_data(psControls)$Plot.ID <- c("ss","ss","mc","mc")
plot_tree(psControls, label.tips="Genus", color="Plot.ID", size="abundance", title="original Mock Community and ASV1")

plot(phy_tree(psControls))



rankAb(dd,"C1.1", ylimit=5000)


plot_tree(dd, label.tips="taxa_names", color="Plot.ID", size="abundance")

plot_tree(dd, label.tips="Genus", color="Plot.ID", size="abundance")

plot_tree(dd, label.tips="Family", size="abundance")

plot_tree(dd, label.tips="Phylum", size="abundance")

plot_tree(dd, label.tips="Genus")

rankAb(dd,"C2.1",ylimit=500)

tax_table(dd)

## why are the taxa so different now, mostly unlabeled?

tax_table(ps)[,'Genus'])

## What are the E. coli otu again?

filt <- grep("Escherichia", tax_table(ps)[,'Genus'])
ecoliOTUs <- rownames(tax_table(ps)@.Data[filt,])
otu_table(ps)[,ecoliOTUs]
tail(otu_table(ps)[,ecoliOTUs])

## Franconibacter keeps disappearing...
filt <- grep("Franconibacter", tax_table(ps)[,'Genus'])
frankishBacteria <- rownames(tax_table(ps)@.Data[filt,])
otu_table(ps)[,frankishBacteria]

frankishBacteria 

## what else are we missing?

bactOfInt <- "Streptomyces" 
filt <- grep(bactOfInt, tax_table(ps)[,'Genus'])
bactOfInt <- rownames(tax_table(ps)@.Data[filt,])

bactOfInt

otu_table(ps)[,bactOfInt]

## so, looks like h=0.005 is a good phylogenetic distance (co-phenetic distance?)
## for agglomerating back the tips

## how do abundances etc look now? 
## try everything out for a bit, then update yaml backup for the repo


conda activate spatialDirt 

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
from matplotlib_scalebar.scalebar import ScaleBar
import pymc as pm


spatDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
## ^different on laptop
os.chdir(spatDir)

## we just want a map of points right now:

## on officeComp
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"
## on laptop
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"

envData = pd.read_csv(sulariEnvCSV)
## get rid of spaces
envData.rename({"Sample ID":"SampleID"}, axis="columns", inplace=True)
## we need to clean up the plot.ID. Sulari recorded season in the plot IDs,
## using letter codes. Also she has one double sampling, with an underscore
envData['Plot.ID'] = envData['Plot.ID'].str.slice(0,5)

## let's get rid of decimals in the names, weird for python work:
envData.rename({"Plot.ID":"PlotID", 
       "soil.respiration":"soil_respiration",
              "Land.type":"Land_type"}, 
                axis="columns", inplace=True)

envData.head()

## so we don't have to repeat:
envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index=False)

envData.head()


envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
 

## if we need to drop controls
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

## her otu table is really large:
##### R ###
library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")
logMin50ps
comdat <- as.data.frame(otu_table(logMin50ps))
write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")
############

## back in python

comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", index_col=0)

comData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

comData.head()

## the map below looks funny...looks like every forest plot has 
## a grassland under it...

dupsFilter = envData.Latitude.duplicated()
dupped = envData[dupsFilter].sort_values(by="Latitude")

dupped.groupby('Plot.ID').nunique()

envData.iloc[0:5,0:8]

## nope...looks okay

## we want a geodf...

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

## to convert to UTM? looks like we are in zone 33
## looks like:
## EPSG:32633

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

sulariPlot_utm.tail()

## can we import our georeferenced tif of the study area?
## lat/lon
#fichtelMap = rasterio.open("studyAreaClipped_modified.tif")
## UTM

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color code our points according to land type?
cdik={
"Arableland":"b",
"Grassland":"y",
"Forest":"g",
}
sulariPlotsDF['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]
sulariPlot_utm['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]

fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    facecolor=sulariPlotsDF['landColors'],
    markersize=400) 


## first step, fill in the moisture data holes. Betty found this information today...
## done

## with this new env matrix, start over with HMSC. Do this on the lab computer.

### start over with HMSC ###

cd "/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis"

## line 3392 above for previous attempt
R

library(Hmsc)
library(phyloseq)

envData <- read.csv("sulariEnv.csv", row.names="SampleID")

## this time, use our ps object with phylogeny and clustered OTUs:

load("psCleanedUP.rda")
psCleanedUp = prune_samples(!(rownames(sample_data(psCleanedUp)) %in% c("C1.1","C1.2","C2.1","C2.2")), psCleanedUp)
## remove species that are now zero
psCleanedUp = prune_taxa( colSums(otu_table(psCleanedUp)) > 0, psCleanedUp)
bactRaw <- otu_table(psCleanedUp)@.Data
## they include a term for sequencing depth
BacteriaDepth<-log(rowSums(bactRaw))
## make pa matrix
bact.pa<-ifelse(bactRaw>0,1,0)
## go to relative abundances
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat

## sanity check
bactRaw[1:5,1:5]
bact.rel[1:5,1:5]
## check some ASVs
20/sum(bactRaw[1,])
244/sum(bactRaw[3,]) 

## looks sane

## thresholds:
threshold.prev = 0.02 ## present in at least 2% of samples?, so 3 samples?
## skip this, because we already enforced some minimum cutoffs:
#threshold.abu = 0.001 ## only OTU that reaches at least 0.1% abundance of one sample
#####
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples? We lose ~300 OTUs with this
###cond2=apply(bact.rel,2,max)>=threshold.abu ## in at least one sample, abundance reaches m%
bactData <- bactRaw[,cond1]
Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
BacteriaDepth<-log(rowSums(bactRaw))
varsOfInterest = c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont")
XData<-data.frame(envData[,varsOfInterest],BacteriaDepth)
## let's try skipping the scaling, because the documentation says 
## that the X matrix is scaled by default.
#XData[,-2]=scale(XData[,-2])
## probably still need to convert categoricals to factor
XData$Land_type <- as.factor(XData$Land_type)

##########################################################
## forgot something...
## we need spatial data. I don't trust lat/long. get the UTMs out of our python spd:

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp

envData = pd.read_csv('sulariEnv.csv', index_col='SampleID')

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

pd.DataFrame(sulariPlot_utm)

dir(sulariPlot_utm['geometry'])

sulariPlot_utm['xx'] = sulariPlot_utm['geometry'].x
sulariPlot_utm['yy'] = sulariPlot_utm['geometry'].y

sulariPlot_utm[['xx','yy']].to_csv("sulariSpatial.csv")

## back to R
##########################################################
##########################################################


Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about. 
## the documentation for ?Hmsc does recommend scaling the Y matrix when using "default priors".
## I'm not sure that we are. I thought the defaults were gaussian dists, so this would make since. 
## ugh, head hurts. Just follow, be a good scientist, just follow...
## make our second, presence absence matrix:
Ypa = 1*(Y>0)
## bring them together for a final Y matrix
Y = cbind(Ypa,Yabu)
## I think HMSC won't accept missing data. I guess we could interpolate. 
## for now, just use what we have:
ccases <- complete.cases(XData) ## 112 samples, we only lose 8 
XData = XData[ccases,] 
Y = Y[ccases,] 
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))
sample.id = row.names(Y)
studyDesign = data.frame(Sample=as.factor(sample.id))
#rL.Sample = setPriors(HmscRandomLevel(units = studyDesign$Sample), nfMin = 5, nfMax = 10) ## not sure how to optimize the number of latent variables?
## define our sample random factor with spatial componenet below, not here
# REGRESSION MODEL FOR ENVIRONMENTAL COVARIATES.
XFormula = ~ soil_respiration + Land_type + pH + N + C + Temperature
XFormula1 = ~ BacteriaDepth ## check this next. Sort of a null model, I guess.
## how do we incorporate spatial patterns?
## it is another "random effect" (I hate this amalgamation of frequentist and bayesian terminology)
## with a specific sData argument

xycoords = read.csv("sulariSpatial.csv", row.names=1)
studyDesign = data.frame(sample = as.factor(1:n)) 

rL.spatial = HmscRandomLevel(sData = xycoords) 
rL.spatial = setPriors(rL.spatial,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.

## if trying the lower memory option:
#rL.nngp = HmscRandomLevel(sData = xycoords, sMethod = 'NNGP', nNeighbours = 10) 
#rL.nngp = setPriors(rL.nngp,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.
## describe model
## set prior to probit for the P/A half of the matrix, we'll manually 
## change the abundance matrix to matrix of gaussian  dists
## just following their example here, but why gaussian? 
m = Hmsc(Y=Y,
         XData = XData,  XFormula = XFormula,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         phyloTree=phy_tree(psCleanedUp),
         ranLevels={list(Sample=rL.nngp)})

## not sure, do we actually need to explicitly model the remaining, sample-level variation 
## to recover the species-species level variation?

#         ranLevels={list(Sample=rL.Sample,
#                         Sample=rL.Spatial)})

## change priors for abundance matrix to gaussian. 
## really not sure why we do this, maybe computationally cheap.
## or maybe I'm just confused. Anyway, we did scale everything,
## so I guess gaussian makes sense.

for (i in (ns+1):(2*ns)){
   m$distr[i,1:2] = c(1,1)
}

## try sampling that model. Save it out, run it on the lab computer.

save(m, file="hmscSpatialmodel.rds")

## interesting, even without sampling, that is 128 mb, too big for
## easy handling with github repo. 

#echo "hmscSpatialmodel_sampled.rds" >> .gitignore

## direct transfer
system("scp hmscSpatialmodel.rds test@132.180.112.115:/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis/")

## on nanocomp:
#echo "hmscSpatialmodel_sampled.rds" >> .gitignore

## still being tracked. Must have been in that last commit? not sure.
git rm -r --cached hmscSpatialmodel.rds

## update remote, try running model

conda activate spatialDirt

nohup Rscript mcmcSample.R &> mcmcSample.log & ## and dies after a few seconds

## no specific error reported, just "Killed" ...maxes out memory probably

## maybe try "thinning" the posterior?
## try line by line
## on nanocomp computer

R
library(Hmsc)
thin = 10
samples = 1000
nChains = 1
nP = 1
setwd("/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis")
print(paste("start time is", Sys.time()))
load("hmscSpatialmodel.rds")
m = sampleMcmc(m, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)

save(m, file="hmscSpatialmodel_sampled.rds")
print(paste("finish time is", Sys.time()))

## that ain't working...too much memory. 
## We can try using the less-memory hungry 
## approaches to spatial sampling?:

## new plan, get some PCNMs, use these instead of spatial latent variables,
## should drop the memory use
## use this for the presentation in two weeks

## and make a request to de.NBI for an instance tha

## for now, let's get out some dbMEM/PCNMs

##### dbMEM analysis ######

## not sure how the new packages will affect all this:

install.packages("ade4")
install.packages("adespatial")
install.packages("adegraphics")
install.packages("spdep")
install.packages("sp")

## following:
https://cran.r-project.org/web/packages/adespatial/vignettes/tutorial.html

library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(sp)

data("mafragh")

str(mafragh)

names(mafragh)

str(mafragh$env)

head(mafragh$env)

mxy <- as.matrix(mafragh$xy)
rownames(mxy) <- NULL
s.label(mxy, ppoint.pch = 15, ppoint.col = "darkseagreen4", Sp = mafragh$Spatial.contour)

fpalette <- colorRampPalette(c("white", "darkseagreen2", "darkseagreen3", "palegreen4"))
sp.flo <- SpatialPolygonsDataFrame(Sr = mafragh$Spatial, data = mafragh$flo, match.ID = FALSE)
s.Spatial(sp.flo[,c(1, 11)], col = fpalette(3), nclass = 3)

## 

mafragh$Spatial

class(mafragh$Spatial)

nb.maf <- poly2nb(mafragh$Spatial)

s.Spatial(mafragh$Spatial, nb = nb.maf, plabel.cex = 0, pnb.edge.col = 'red')

set.seed(3)

xyir <- mxy[sample(1:nrow(mafragh$xy), 20),]

s.label(xyir)

nbnear1 <- dnearneigh(xyir, 0, 50)

nbnear2 <- dnearneigh(xyir, 0, 305)

g1 <- s.label(xyir, nb = nbnear1, pnb.edge.col = "red", main = "neighbors if 0<d<50", plot = FALSE)
g2 <- s.label(xyir, nb = nbnear2, pnb.edge.col = "red", main = "neighbors if 0<d<305", plot = FALSE)
cbindADEg(g1, g2, plot = TRUE)

### adapt to our data ##

mxy <- as.matrix(mafragh$xy) ## matrix

## ours are here:

## on denbi
conda activate r_env
soil
R
library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(sp)

library(phyloseq)

envData = read.csv("sulariEnv.csv", row.names=1)
xycoords = read.csv("sulariSpatial.csv", row.names=1)
envSpatData <- merge(x=envData, y=xycoords, by=0)
rownames(envSpatData) <- envSpatData$Row.names
envSpatData <- envSpatData[,-1]
envSpatData <- envSpatData[rownames(envData),]
aa <- as.matrix(read.csv("sulariSpatial.csv", row.names=1), rownames=1)
## there are repeats, and we need to jitter these, as a temporary solution
dupRows <- rownames(aa)[duplicated(aa)]
envSpatData['xxJit'] <- envSpatData['xx']
envSpatData['yyJit'] <- envSpatData['yy']
for (i in dupRows){
    envSpatData[i,'xxJit'] <- jitter(envSpatData[i,"xx"],1,2)
    envSpatData[i,'yyJit'] <- jitter(envSpatData[i,"yy"],1,2)
    print(envSpatData[i,c('xx','yy')])
    print(envSpatData[i,c('xxJit','yyJit')])
    print("#################################")
}

## sanity checks
envSpatData[dupRows,c('xx','xxJit','yy','yyJit')]

envSpatData[,c('xx','xxJit','yy','yyJit')]

any(duplicated(envSpatData[,c('xxJit','yyJit')]))
## looks okay, use this as our new spatial data 
## to find PCNMs

xyJit = envSpatData[,c('xxJit','yyJit')]
rownames(xyJit) <- NULL
## try everything out for a bit, then update yaml backup for the repo


conda activate spatialDirt 

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
from matplotlib_scalebar.scalebar import ScaleBar
import pymc as pm


spatDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
## ^different on laptop
os.chdir(spatDir)

## we just want a map of points right now:

## on officeComp
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"
## on laptop
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"

envData = pd.read_csv(sulariEnvCSV)
## get rid of spaces
envData.rename({"Sample ID":"SampleID"}, axis="columns", inplace=True)
## we need to clean up the plot.ID. Sulari recorded season in the plot IDs,
## using letter codes. Also she has one double sampling, with an underscore
envData['Plot.ID'] = envData['Plot.ID'].str.slice(0,5)

## let's get rid of decimals in the names, weird for python work:
envData.rename({"Plot.ID":"PlotID", 
       "soil.respiration":"soil_respiration",
              "Land.type":"Land_type"}, 
                axis="columns", inplace=True)

envData.head()

## so we don't have to repeat:
envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index=False)

envData.head()


envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
 

## if we need to drop controls
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

## her otu table is really large:
##### R ###
library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")
logMin50ps
comdat <- as.data.frame(otu_table(logMin50ps))
write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")
############

## back in python

comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", index_col=0)

comData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

comData.head()

## the map below looks funny...looks like every forest plot has 
## a grassland under it...

dupsFilter = envData.Latitude.duplicated()
dupped = envData[dupsFilter].sort_values(by="Latitude")

dupped.groupby('Plot.ID').nunique()

envData.iloc[0:5,0:8]

## nope...looks okay

## we want a geodf...

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

## to convert to UTM? looks like we are in zone 33
## looks like:
## EPSG:32633

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

sulariPlot_utm.tail()

## can we import our georeferenced tif of the study area?
## lat/lon
#fichtelMap = rasterio.open("studyAreaClipped_modified.tif")
## UTM

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color code our points according to land type?
cdik={
"Arableland":"b",
"Grassland":"y",
"Forest":"g",
}
sulariPlotsDF['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]
sulariPlot_utm['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]

fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    facecolor=sulariPlotsDF['landColors'],
    markersize=400) 


grassPatch = Patch(color='y', label='grassland',)
help(envData[repFilt].sort_values)

dir(envData[repFilt])


comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0).drop([ "C1.1", "C1.2", "C2.1", "C2.2"])


comData.head()

## we want to average the abundances of each otu for each group of repeated 
## columns. For instance, 


'P0103': ['S11', 'S53', 'S88', 'S105']



(envData.index == comData.index).all()

aa = comData.copy()

aa['PlotID'] = envData['PlotID']
bb = aa.groupby('PlotID').mean()

## sanity check:
aa[aa['PlotID'] == 'P0104']['ASV287']
bb.loc["P0104"]['ASV287']
aa[aa['PlotID'] == 'P0104']['ASV287'].mean()

## okay, so for the moment, we shift our analysis from sample based
## to plot based?

## our new community matrix would then be? 

############# setup denbi for HMSC ###################

## but we also have a denbi machine now. Should we even mess with the
## PCNM analysis? We can try the latent variable approach and 
## see if it is now possible with the large amount of RAM.

## which means setting up a denbi machine and all that...
## 

## to set up the denbi instance:

## from lab computer:

ssh -Yi /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6 -p 30267

## saved as

alias denbiDirt="ssh -Yi /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6 -p 30267"

## where do we enable x11 port forwarding again?

sudo vim /etc/ssh/ssh_config

sudo systemctl restart sshd

## fix github 
## need a key:
cd ~/.ssh
ssh-keygen -t rsa -f denbiHMSC_2git
git config --global user.email "danchurchthomas@gmail.com"
git config --global user.name "danchurch"
git remote add origin https://github.com/danchurch/fichtelgebirgeSoils.git
git branch -M main
git remote set-url origin git@github.com:danchurch/fichtelgebirgeSoils.git
git push -u origin main

ssh -vT git@github.com

## for some reason, the ssh-agent isn't starting:

## if I direct ssh to the key, works fine.
ssh  -i ~/.ssh/denbiHMSC_2git -vT git@github.com

## I have to manually start the ssh-agent now. Why?
ssh-agent bash ## starts a new shell with active ssh-agent. weird
ssh-add ~/.ssh/denbiHMSC_2git

## adding this to the bashrc file. but use eval, don't want a new shell
eval "$(ssh-agent)"
ssh-add ~/.ssh/denbiHMSC_2git

## update the fstab

## where is our storage?

lsblk ## vdd, but what is the uuid?

lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE | egrep -v "^loop"


lsblk /dev/vdd

lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE,UUID | egrep -v "^loop"
## no uuid, need to mount first?

sudo mkdir /media/storage

sudo mount /dev/vdd /vol/data ## wrong fs type?

## not yet formatted?
sudo mkfs.ext4 /dev/vdd

sudo mount /dev/vdd /vol/data ## yup, works now

## now we have a uuid:
lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE,UUID | egrep -v "^loop"

##e3b057cc-7097-4d41-aed6-cb0c52fd85ee

## add line to /etc/fstab
UUID=e3b057cc-7097-4d41-aed6-cb0c52fd85ee       /vol/data      auto    defaults        0       2
#

## get conda working denbi

mkdir -p ~/miniconda3

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh

bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3


~/miniconda3/bin/conda init bash

conda config --set auto_activate_base false

## didn't update my path. adding this to .profile on the denbi machine:
export Path=/home/ubuntu/miniconda3/bin:$PATH


## to get updated yaml and R, git the repo:
## on local machine, update yaml
conda env export > spatialDirt.yaml

git clone https://github.com/danchurch/fichtelgebirgeSoils.git


## update the one in the repo:
yaml=/home/ubuntu/fichtelgebirgeSoils/spatialDirt.yaml

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

## is the samba solver in there already?
conda update -n base conda
conda install -n base conda-libmamba-solver
conda config --set solver libmamba

conda config --describe solver 

## try it out:
conda env create --name spatialDirt --file=$yaml



## seems like it didn't work. But not clear what failed.
## graph-viz, maybe, cuz only error is:
ERROR: Could not find a version that satisfies the requirement python-graphviz==0.20.3 (from versions: none)
ERROR: No matching distribution found for python-graphviz==0.20.3
## not sure what this will break, maybe the cooccurrence network programs

## maybe jupyter? check

conda activate spatialDirt

jupyter notebook --no-browser --port=8080


## nope, didn't work. 

## outside the yaml listings, loosen up versions etc, might work
pip install notebook

conda activate spatialDirt 

jupyter notebook

## yup

## to get a bash kernel on there? https://github.com/takluyver/bash_kernel

pip install bash_kernel

python -m bash_kernel.install

## try port forwarding:

conda activate spatialDirt 

jupyter notebook --no-browser --port=8080


## run this locally to activate the tcp forwarding
ssh -L 8080:localhost:8080 ubuntu@129.70.51.6 -p 30267

## then copy the url from the denbi notebook process to local firefox

## and need to update r?
## to match the home comp, best to install 4.3.3

## meh, just install the latest stable, it's close and simpler.
## following: https://cran.r-project.org/

# update indices
sudo apt update -qq

# install two helper packages we need
sudo apt install --no-install-recommends software-properties-common dirmngr
# add the signing key (by Michael Rutter) for these repos
# To verify key, run gpg --show-keys /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc 
# Fingerprint: E298A3A825C0D65DFD57CBB651716619E084DAB9
wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc
# add the R 4.0 repo from CRAN -- adjust 'focal' to 'groovy' or 'bionic' as needed
sudo add-apt-repository "deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/"

sudo apt install --no-install-recommends r-base

## get the R kernel for the notebooks:
## doing the R installations outside conda, so:

## we're missing some basics:

conda deactivate

sudo apt install make
sudo apt install gcc
sudo apt install g++
sudo apt install gfortran

## and other supporting packages
sudo apt-get install liblapack-dev libblas-dev
sudo apt install libssl-dev
sudo apt install libfontconfig1-dev
sudo apt install libharfbuzz-dev libfribidi-dev
sudo apt install libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev
sudo apt install libcurl4-openssl-dev
sudo apt install libxml2-dev

sudo R

install.packages('IRkernel') 

IRkernel::installspec() ## nope

## here it gets weird. The kernel needs the jupyter notebook,
## which I installed the conda environment. Let's see if 
## this works while inside the environment:

conda activate spatialDirt

sudo R

IRkernel::installspec() ## nope

remove.packages(IRkernel)

IRkernel::installspec() ## nope

install.packages('IRkernel')  ## no errors
## check for kernel on notebook

library('IRkernel') 

IRkernel::installspec() ## nope

## try github install?


install.packages('devtools')

ERROR: dependencies ‘usethis’, ‘miniUI’, ‘pkgdown’, ‘rcmdcheck’, ‘roxygen2’, ‘rversions’, ‘urlchecker’

install.packages('openssl')

library(devtools)

devtools::install_github('IRkernel/IRkernel')

library('IRkernel') 

IRkernel::installspec() ## nope

## is the issue with the limited install of jupyter within an environment?

conda list

conda remove jupyter

pip uninstall notebook

## outside of environment again:
install.packages("BiocManager")

## get phyloseq and HMSC while we are at it:
BiocManager::install("phyloseq") ## failed

## but this is all getting really messy. 
## maybe let's try this approach:
https://docs.anaconda.com/free/working-with-conda/packages/using-r-language/

## most of what we are doing right now we are doing in R
## so let's let's build an environment that can handle HMSC and phyloseq together, 
## just for the denbi computer:

conda remove -n spatialDirt --all

## looks like this has jupyter notebook builtin, anyway, probably has the kernel
conda create -n r_env r-essentials r-base

## now will these fit in this environment?
conda activate r_env

conda install conda-forge::r-hmsc

conda install bioconda::bioconductor-phyloseq

## no errors reported. 

## adespatial, etc, for PCNM?

conda activate r_env
conda install conda-forge::r-ade4
conda install conda-forge::r-adespatial

library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(sp)


## will this overwhelm the storage we have on our front node?
## so far not bad, but should move activity to our big drives


conda activate r_env

jupyter notebook --no-browser --port=8080
## run this locally to activate the tcp forwarding
ssh -L 8080:localhost:8080 ubuntu@129.70.51.6 -p 30267
## saving this as 

alias denbiDirtTCP="ssh -L 8080:localhost:8080 ubuntu@129.70.51.6 -p 30267"

## works, R and python kernels in there. will pip get us the bash kernel?

## our instance, the conda base env and the conda R env all have their own python, so
## be careful 

conda activate

conda activate r_env

conda deactivate

pip install bash_kernel
python -m bash_kernel.install

## and seems to work. Notebooks on the denbi instance, inside the r_env, have all three kernels.

## can we move our git repo to the volume without hurting anything?

mv ~/fichtelgebirgeSoils /vol/data

## seems fine. can our new environment repeat the HMSC code above? starting from line 4443

conda activate r_env

R

library(Hmsc)
library(phyloseq)

rm(list=ls())
## raw data, import and cleanup/transform
envData <- read.csv("sulariEnv.csv", row.names="SampleID")
## spatial data
xycoords = read.csv("sulariSpatial.csv", row.names=1)
## add spatial data to environmental:
aa <- merge(x=envData, y=xycoords, by=0, )
row.names(aa) <- aa$Row.names
aa[1] <- NULL
aa <- aa[rownames(envData),]
envData <- aa; rm(aa)
load("psCleanedUP.rda")
psCleanedUp = prune_samples(!(rownames(sample_data(psCleanedUp)) %in% c("C1.1","C1.2","C2.1","C2.2")), psCleanedUp)
## remove species that are now zero
psCleanedUp = prune_taxa( colSums(otu_table(psCleanedUp)) > 0, psCleanedUp)
bactRaw <- otu_table(psCleanedUp)@.Data
## they include a term for sequencing depth
BacteriaDepth<-log(rowSums(bactRaw))
## make pa matrix
bact.pa<-ifelse(bactRaw>0,1,0)
## go to relative abundances
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat
## thresholds:
threshold.prev = 0.02 ## present in at least 2% of samples?, so 3 samples?
## skip this, because we already enforced some minimum cutoffs:
#threshold.abu = 0.001 ## only OTU that reaches at least 0.1% abundance of one sample
#####
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples? We lose ~300 OTUs with this
###cond2=apply(bact.rel,2,max)>=threshold.abu ## in at least one sample, abundance reaches m%
bactData <- bactRaw[,cond1]
Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
BacteriaDepth<-log(rowSums(bactRaw))
varsOfInterest = c("PlotID","soil_respiration","Land_type","pH","N","C","Temperature","waterCont","xx","yy")
XData<-data.frame(envData[,varsOfInterest],BacteriaDepth)
## let's try skipping the scaling, because the documentation says 
## that the X matrix is scaled by default.
#XData[,-2]=scale(XData[,-2])
## probably still need to convert categoricals to factor
XData$Land_type <- as.factor(XData$Land_type)

## community data
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about. 
## the documentation for ?Hmsc does recommend scaling the Y matrix when using "default priors".
## I'm not sure that we are. I thought the defaults were gaussian dists, so this would make since. 
## ugh, head hurts. Just follow, be a good scientist, just follow...
## make our second, presence absence matrix:
Ypa = 1*(Y>0)
## bring them together for a final Y matrix
Y = cbind(Ypa,Yabu)
## I think HMSC won't accept missing data. I guess we could interpolate. 
## for now, just use what we have:
ccases <- complete.cases(XData) ## 112 samples, we only lose 8 
XData = XData[ccases,] 
Y = Y[ccases,] 
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))


## spatial data
## but we want these on a plot level:

## study design 

## sample effects
sample.id = row.names(Y)
## plot effects, also for spatial
samplePlotid = XData$PlotID
studyDesign = data.frame("sample"=as.factor(sample.id), "site_spatial"=as.factor(samplePlotid))
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID 
plot_coords$PlotID <- NULL

head(studyDesign)

## cleanup XData a bit:
varsOfmodel <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont","BacteriaDepth")
XData <- XData[,c(varsOfmodel)]

## random level for plots
rL.sample = HmscRandomLevel(units = sample.id)
## random level for spatial dist
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.

## regression model for environmental covariates.
XFormula = ~ soil_respiration + Land_type + pH + N + C + Temperature
XFormula1 = ~ BacteriaDepth ## check this next. Sort of a null model, I guess.

## if trying the lower memory option:
#rL.nngp = HmscRandomLevel(sData = xycoords, sMethod = 'NNGP', nNeighbours = 10) 
#rL.nngp = setPriors(rL.nngp,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.
## describe model
## set prior to probit for the P/A half of the matrix, we'll manually 
## change the abundance matrix to matrix of gaussian  dists
## just following their example here, but why gaussian? 

m = Hmsc(Y=Y,
         XData = XData,  XFormula = XFormula,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         phyloTree=phy_tree(psCleanedUp),
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})
## try everything out for a bit, then update yaml backup for the repo


conda activate spatialDirt 

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
from matplotlib_scalebar.scalebar import ScaleBar
import pymc as pm


spatDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
## ^different on laptop
os.chdir(spatDir)

## we just want a map of points right now:

## on officeComp
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"
## on laptop
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"

envData = pd.read_csv(sulariEnvCSV)
## get rid of spaces
envData.rename({"Sample ID":"SampleID"}, axis="columns", inplace=True)
## we need to clean up the plot.ID. Sulari recorded season in the plot IDs,
## using letter codes. Also she has one double sampling, with an underscore
envData['Plot.ID'] = envData['Plot.ID'].str.slice(0,5)

## let's get rid of decimals in the names, weird for python work:
envData.rename({"Plot.ID":"PlotID", 
       "soil.respiration":"soil_respiration",
              "Land.type":"Land_type"}, 
                axis="columns", inplace=True)

envData.head()

## so we don't have to repeat:
envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index=False)

envData.head()


envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
 

## if we need to drop controls
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

## her otu table is really large:
##### R ###
library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")
logMin50ps
comdat <- as.data.frame(otu_table(logMin50ps))
write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")
############

## back in python

comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", index_col=0)

comData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

comData.head()

## the map below looks funny...looks like every forest plot has 
## a grassland under it...

dupsFilter = envData.Latitude.duplicated()
dupped = envData[dupsFilter].sort_values(by="Latitude")

dupped.groupby('Plot.ID').nunique()

envData.iloc[0:5,0:8]

## nope...looks okay

## we want a geodf...

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

## to convert to UTM? looks like we are in zone 33
## looks like:
## EPSG:32633

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

sulariPlot_utm.tail()

## can we import our georeferenced tif of the study area?
## lat/lon
#fichtelMap = rasterio.open("studyAreaClipped_modified.tif")
## UTM

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color code our points according to land type?
cdik={
"Arableland":"b",
"Grassland":"y",
"Forest":"g",
}
sulariPlotsDF['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]
sulariPlot_utm['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]

fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    facecolor=sulariPlotsDF['landColors'],
    markersize=400) 


grassPatch = Patch(color='y', label='grassland',)
forestPatch = Patch(color='g', label='forest')
farmPatch = Patch(color='b', label='arable land')
ax.legend(handles=[grassPatch, forestPatch, farmPatch], 
          loc="lower left",
          fontsize=15,
)

## if we want to compare just grassland and forest

plt.close('all')
onlyGrassForest = sulariPlot_utm[sulariPlot_utm['Land.type'].apply(lambda x: x in ["Forest", "Grassland"])]
fig, ax = plt.subplots()
dataDir="/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/sulariData/"

comData <- read.csv(paste0(dataDir,"comdat.csv"), row.names=1)
comData <- comData[!row.names(comData) %in% c("C1.1","C1.2","C2.1","C2.2"),]

## there are lots of mems...let's select from among them with 
## rda, model selection: 

## as per Dray's example, section6.2, do a PCA on the abundances
## they don't show it, but the code and my previous experience makes 
## me think we are supposed to do a hellinger transformation here 
## before doing a PCA...


load("xyJit00.rda")

nb <- chooseCN(coordinates(xyJit00), type = 1, plot.nb = FALSE)

lw <- nb2listw(nb, style = 'S', zero.policy = TRUE)

nbtri <- tri2nb(xyJit00)

s.label(as.data.frame(xyJit00), nb = nbtri, pnb.edge.col = "red", main = "Delauney")

data("mafragh")
mxy <- as.matrix(mafragh$xy)
rownames(mxy) <- NULL
s.label(mxy, ppoint.pch = 15, ppoint.col = "darkseagreen4", Sp = mafragh$Spatial.contour)

xyir <- mxy[sample(1:nrow(mafragh$xy), 20),]

s.label(xyir, ppoint.pch = 15, ppoint.col = "darkseagreen4", Sp = mafragh$Spatial.contour)

## species as columns:
com.hell <- decostand(comData, 'hellinger')
pca.hell <- dudi.pca(df = com.hell, scale = FALSE, scannf = FALSE, nf = 4)
mem.sel <- mem.select(pca.hell$tab, listw = lw)

## are these still the same as we saw before?
mem.sel$MEM.select

dev.new()

plot(mem.sel$MEM.select, SpORcoords = xyJit00)

plot(mem.gab.sel$MEM.select, SpORcoords = xyJit00)

?mem.select

## these look pretty good, looks like we picked up the signature
## of the forest and mountain

## let's update notebook, and think about how to use these in the full model

## neighborhoos graphs:
par(mfrow=c(1,2))

nbtri <- tri2nb(xyJit00)

class(xyJit00)

s.label(xyJit00, nb = nbtri, pnb.edge.col = "red", main = "Delauney")

s.label(as.data.frame(xyJit00), nb = nbtri, pnb.edge.col = "red", main = "Delauney")

adegraphics::s.label(as.data.frame(xyJit00), nb = nbtri, pnb.edge.col = "red", main = "Delauney")

ade4::s.label(xyJit00, nb = nbtri, pnb.edge.col = "red", main = "Delauney")

s.label(xyJit00, nb = nbtri, pnb.edge.col = "red", main = "Delauney")

library(ade4)

data(atlas)

s.label(atlas$xy, lab = atlas$names.district, area = atlas$area, inc = FALSE, addax = FALSE)

s.label(atlas$xy, lab = atlas$names.district)

dev.new()

nbgab <- graph2nb(gabrielneigh(xyJit00), sym = TRUE)

s.label(xyJit00, nb = nbgab, pnb.edge.col = "red", main = "Gabriel")

## run the notebook from the lab computer, because the MEM selection takes 
## while

ssh -L 8080:localhost:8080 ubuntu@129.70.51.6 -p 30267

## catch that environment up a little:

jupyter notebook --no-browser --port=8080

## for adespatial:
libudunits2-dev

########## rerun model with PCNMS? ###############

## can we incorporate our new MEMs as variables in our 
## model with HMSC?

## run this on the denbi instance

## mass of code from above (line ~5260)

soil

conda activate r_env

R

library(Hmsc)
library(phyloseq)

rm(list=ls())
## raw data, import and cleanup/transform
envData <- read.csv("sulariEnv.csv", row.names="SampleID")
load("psCleanedUP.rda")
psCleanedUp = prune_samples(!(rownames(sample_data(psCleanedUp)) %in% c("C1.1","C1.2","C2.1","C2.2")), psCleanedUp)
## remove species that are now zero
psCleanedUp = prune_taxa( colSums(otu_table(psCleanedUp)) > 0, psCleanedUp)
bactRaw <- otu_table(psCleanedUp)@.Data
## they include a term for sequencing depth
BacteriaDepth<-log(rowSums(bactRaw))
## make pa matrix
bact.pa<-ifelse(bactRaw>0,1,0)
## go to relative abundances
## try everything out for a bit, then update yaml backup for the repo


conda activate spatialDirt 

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
from matplotlib_scalebar.scalebar import ScaleBar
import pymc as pm


spatDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
## ^different on laptop
os.chdir(spatDir)

## we just want a map of points right now:

## on officeComp
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"
## on laptop
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"

envData = pd.read_csv(sulariEnvCSV)
## get rid of spaces
envData.rename({"Sample ID":"SampleID"}, axis="columns", inplace=True)
## we need to clean up the plot.ID. Sulari recorded season in the plot IDs,
## using letter codes. Also she has one double sampling, with an underscore
envData['Plot.ID'] = envData['Plot.ID'].str.slice(0,5)

## let's get rid of decimals in the names, weird for python work:
envData.rename({"Plot.ID":"PlotID", 
       "soil.respiration":"soil_respiration",
              "Land.type":"Land_type"}, 
                axis="columns", inplace=True)

envData.head()

## so we don't have to repeat:
envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index=False)

envData.head()


envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
 

## if we need to drop controls
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

## her otu table is really large:
##### R ###
library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")
logMin50ps
comdat <- as.data.frame(otu_table(logMin50ps))
write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")
############

## back in python

comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", index_col=0)

comData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

comData.head()

## the map below looks funny...looks like every forest plot has 
## a grassland under it...

dupsFilter = envData.Latitude.duplicated()
dupped = envData[dupsFilter].sort_values(by="Latitude")

dupped.groupby('Plot.ID').nunique()

envData.iloc[0:5,0:8]

## nope...looks okay

scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat
## thresholds:
threshold.prev = 0.02 ## present in at least 2% of samples?, so 3 samples?
## skip this, because we already enforced some minimum cutoffs:
#threshold.abu = 0.001 ## only OTU that reaches at least 0.1% abundance of one sample
#####
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples? We lose ~300 OTUs with this
###cond2=apply(bact.rel,2,max)>=threshold.abu ## in at least one sample, abundance reaches m%
bactData <- bactRaw[,cond1]
Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
BacteriaDepth<-log(rowSums(bactRaw))
varsOfInterest = c("PlotID","soil_respiration","Land_type","pH","N","C","Temperature","waterCont")

## let's add some of the MEMs here?
load("mems.rda")
rownames(mem.sel$MEM.select) <- rownames(envData)
mems <- mem.sel$MEM.select

XData <- data.frame(envData[,varsOfInterest],BacteriaDepth, mems)
## let's try skipping the scaling, because the documentation says 
## that the X matrix is scaled by default.
#XData[,-2]=scale(XData[,-2])
## probably still need to convert categoricals to factor
XData$Land_type <- as.factor(XData$Land_type)
XData$PlotID <- as.factor(XData$PlotID)

## community data
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about. 
## the documentation for ?Hmsc does recommend scaling the Y matrix when using "default priors".
## I'm not sure that we are. I thought the defaults were gaussian dists, so this would make since. 
## ugh, head hurts. Just follow, be a good scientist, just follow...
## make our second, presence absence matrix:
Ypa = 1*(Y>0)
## bring them together for a final Y matrix
Y = cbind(Ypa,Yabu)
## I think HMSC won't accept missing data. I guess we could interpolate. 
## for now, just use what we have:
ccases <- complete.cases(XData) ## 112 samples, we only lose 8 
XData = XData[ccases,] 
mems = mems[ccases,] 
Y = Y[ccases,] 
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))

## study design 
## sample effects
sample.id = row.names(Y)
## plot effects, also for spatial
samplePlotid = XData$PlotID
studyDesign = data.frame("sample"=as.factor(sample.id), "site"=as.factor(samplePlotid))
XData$PlotID

head(studyDesign)

## cleanup XData a bit:

## random level for plots
rL.sample = HmscRandomLevel(units = sample.id)
## random level for spatial dist
rL.site = HmscRandomLevel(units = XData$PlotID)
rL.site = setPriors(rL.site,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.

## regression models for environmental covariates.
XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature
XFormula1 = ~ soil_respiration + Land_type + pH + N + C + Temperature + MEM8 + MEM10 + MEM23 + MEM40
XFormula2 = ~ MEM8 + MEM10 + MEM23 + MEM40
XFormulaB = ~ BacteriaDepth ## check this next. Sort of a null model, I guess.

## if trying the lower memory option:
#rL.nngp = HmscRandomLevel(sData = xycoords, sMethod = 'NNGP', nNeighbours = 10) 
#rL.nngp = setPriors(rL.nngp,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.
## describe model
## set prior to probit for the P/A half of the matrix, we'll manually 
## change the abundance matrix to matrix of gaussian  dists
## just following their example here, but why gaussian? 

m0 = Hmsc(Y=Y,
         XData = XData,  XFormula = XFormula0,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         phyloTree=phy_tree(psCleanedUp),
         ranLevels={list("sample"=rL.sample, "site" = rL.site)})

save(m0, file="m0.rda")

## try running this:

#####################################################
library(Hmsc)
load("m0.rda")
thin = 1
samples = 1000
nChains = 1
nP = 1
print(paste("start time is", Sys.time()))
m0 = sampleMcmc(m0, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)
save(m0, file="m0_sampled.rds")
print(paste("finish time is", Sys.time()))
## ugh, head hurts. Just follow, be a good scientist, just follow...
## make our second, presence absence matrix:
Ypa = 1*(Y>0)
## bring them together for a final Y matrix
Y = cbind(Ypa,Yabu)
## I think HMSC won't accept missing data. I guess we could interpolate. 
## for now, just use what we have:
ccases <- complete.cases(XData) ## 112 samples, we only lose 8 
XData = XData[ccases,] 
Y = Y[ccases,] 
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))

# spatial data
## but we want these on a plot level:

## study design 

## sample effects
sample.id = row.names(Y)
## plot effects, also for spatial
samplePlotid = XData$PlotID
studyDesign = data.frame("sample"=as.factor(sample.id), "site_spatial"=as.factor(samplePlotid))
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID 
plot_coords$PlotID <- NULL

head(studyDesign)

## cleanup XData a bit:
varsOfmodel <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont","BacteriaDepth")
XData <- XData[,c(varsOfmodel)]
## random level for plots
rL.sample = HmscRandomLevel(units = sample.id)
## random level for spatial dist
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.

## regression model for environmental covariates.
XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature
XFormula1 = ~ BacteriaDepth ## check this next. Sort of a null model, I guess.

m0 = Hmsc(Y=Y,
         XData = XData,  XFormula = XFormula0,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         phyloTree=phy_tree(psCleanedUp),
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})

## change priors for abundance matrix to gaussian. 

for (i in (ns+1):(2*ns)){
   m0$distr[i,1:2] = c(1,1)
}

save(m0, file="m0.rda")

## now try the sampling again, with this lower number of species:

vim m0mcmcSample.R

#####################################################

library(Hmsc)

load("m0.rda")

thin = 1
samples = 1000
nChains = 1
nP = 1

print(paste("start time is", Sys.time()))

m0 = sampleMcmc(m0, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)

save(m0, file="m0_sampled.rds")
print(paste("finish time is", Sys.time()))

#####################################################

soil
conda activate r_env
nohup Rscript m0mcmcSample.R &> m0mcmcSample.log  

## something is wrong with the tree:
https://github.com/hmsc-r/HMSC/issues/140

soil
conda activate r_env

ape::is.ultrametric(P)

ape::is.ultrametric(phy_tree(psCleanedUp))

?ape::is.ultrametric

tre <- phy_tree(psCleanedUp)

is.ultrametric(tre)

phylcor <- vcv(tre)

det(phylcor) ## should be > 0, it's not

duplicated(phylcor) ## no dups...

## try some quick fixes?: https://github.com/richfitz/diversitree/issues/17

## but not now. Let's try the model without phylogeny. If this doesn't work, 
## this will have to wait for now, just present the preliminary results 

######### spatial HMSC model, no phylogeny ###############

denbiDirt

soil
conda activate r_env

R

library(Hmsc)
library(phyloseq)
library(ape)

rm(list=ls())
## merge the spatial and env data:
xycoords = read.csv("sulariSpatial.csv", row.names=1)
envData <- read.csv("sulariEnv.csv", row.names="SampleID")
aa <- merge(x=envData, y=xycoords, by=0, )
row.names(aa) <- aa$Row.names
aa[1] <- NULL
aa <- aa[rownames(envData),]
envData <- aa; rm(aa)
## this time, use our ps object with phylogeny and clustered OTUs:
load("psCleanedUP.rda")
psCleanedUp = prune_samples(!(rownames(sample_data(psCleanedUp)) %in% c("C1.1","C1.2","C2.1","C2.2")), psCleanedUp)
## remove species that are now zero
psCleanedUp = prune_taxa( colSums(otu_table(psCleanedUp)) > 0, psCleanedUp)
bactRaw <- otu_table(psCleanedUp)@.Data
## they include a term for sequencing depth
BacteriaDepth<-log(rowSums(bactRaw))
## make pa matrix
bact.pa<-ifelse(bactRaw>0,1,0)
## go to relative abundances
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat

## sanity check
bactRaw[1:5,1:5]
bact.rel[1:5,1:5]
## check some ASVs
20/sum(bactRaw[1,])
244/sum(bactRaw[3,]) 

## looks sane

## thresholds:
threshold.prev = 0.02 ## present in at least 2% of samples?, so 3 samples?
## try a secondary reduction in diversity, for memory purposes:
threshold.abu = 0.003 ## only OTU that reaches at least 0.3% abundance of one sample
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples? We lose ~300 OTUs with this
cond2=apply(bact.rel,2,max)>=threshold.abu 
bactData <- bactRaw[, cond1 & cond2 ] 
################################################
sum(bactData > 0)
sum(bact.pa)
dim(bactData)
dim(bact.pa)


Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
BacteriaDepth<-log(rowSums(bactRaw))
varsOfInterest = c("PlotID","soil_respiration","Land_type","pH","N","C","Temperature","waterCont","xx","yy")
XData<-data.frame(envData[,varsOfInterest],BacteriaDepth)
## let's try skipping the scaling, because the documentation says 
## that the X matrix is scaled by default.
#XData[,-2]=scale(XData[,-2])
## probably still need to convert categoricals to factor
XData$Land_type <- as.factor(XData$Land_type)
## community data
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about. 
## the documentation for ?Hmsc does recommend scaling the Y matrix when using "default priors".
## I'm not sure that we are. I thought the defaults were gaussian dists, so this would make since. 
## ugh, head hurts. Just follow, be a good scientist, just follow...
## make our second, presence absence matrix:
Ypa = 1*(Y>0)
## bring them together for a final Y matrix
Y = cbind(Ypa,Yabu)
## I think HMSC won't accept missing data. I guess we could interpolate. 
## for now, just use what we have:
ccases <- complete.cases(XData) ## 112 samples, we only lose 8 
XData = XData[ccases,] 
Y = Y[ccases,] 
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))
## sample effects
sample.id = row.names(Y)
## plot effects, also for spatial
samplePlotid = XData$PlotID
studyDesign = data.frame("sample"=as.factor(sample.id), "site_spatial"=as.factor(samplePlotid))
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID 
plot_coords$PlotID <- NULL
## cleanup XData a bit:
varsOfmodel <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont","BacteriaDepth")
XData <- XData[,c(varsOfmodel)]
## regression model for environmental covariates.
XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature
XFormula1 = ~ BacteriaDepth ## check this next. Sort of a null model, I guess.
## random level for plots
rL.sample = HmscRandomLevel(units = sample.id)
## random level for spatial dist
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.

m0 = Hmsc(Y=Y,
         XData = XData,  XFormula = XFormula0,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         phyloTree=phy_tree(psCleanedUp), ## try without 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})

## change priors for abundance matrix to gaussian. 
for (i in (ns+1):(2*ns)){
   m0$distr[i,1:2] = c(1,1)
}

save(m0, file="m0.rda")

## on denbi bash
vim m0mcmcSample.R

#####################################################

library(Hmsc)

load("m0.rda")

print(paste("start time is", Sys.time()))

thin = 1
samples = 1000
nChains = 4
nP = 3
m0 = sampleMcmc(m0, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)

save(m0, file="m0_sampled.rds")
print(paste("finish time is", Sys.time()))

#####################################################

soil
conda activate r_env

nohup Rscript m0mcmcSample.R &> m0mcmcSample.log &

## running for now.
## nope, killed. but went for much longer. progress.
## now try reduced diversity

## and fucked. bugs in the software.

##################################################

## create a report for github
## maybe add it to this issue:
https://github.com/hmsc-r/HMSC/issues/183

sink(file="rEnv.txt")
R.Version()
installed.packages()
sink()

## R code for report ##


XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature

m0 = Hmsc(Y=Y,
         XData = XData,  XFormula = XFormula0,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})


write.csv(Y, file="Y.csv")
write.csv(XData, file="XData.csv")

################ make some graphics for presentation ##################################

## for research update

## can we get a map of udeabacter?

## we had a function for this way back, around line 1360

## bring it back:

## on local tower 

conda activate spatialDirt

python3

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import matplotlib.colors 
import os
import scipy.spatial as sp
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
import rasterio
import rasterio.plot


sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 
controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
comData.drop(controls, axis='rows', inplace=True)
plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration',
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

sulariPlotsDF.to_crs('EPSG:32633', inplace=True)
landColorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

fig,ax = plt.subplots()

## how can we generalize this so it can handle multiple ASVs? 

def mapOneASV(asv, ax=None, color="b", jitter=0, showLand=False):
    if ax is None: ax = plt.gca()
    jitX = sulariPlotsDF['geometry'].x.apply(lambda x: x + np.random.normal(loc=0, scale=jitter))
    jitY = sulariPlotsDF['geometry'].y.apply(lambda x: x + np.random.normal(loc=0, scale=jitter))
    asvPlotPoints = gpd.points_from_xy( jitX, jitY, crs="EPSG:32633" )
    asvGEO = gpd.GeoDataFrame(pd.concat([comData[asv], sulariPlotsDF['Land_type']], axis=1), geometry=asvPlotPoints, crs="EPSG:32633")
    if showLand: 
        asvGEO['landCols'] = [ landColorDict[i] for i in asvGEO['Land_type'] ]
    else: 
        asvGEO['landCols'] = "k"
    rasterio.plot.show(fichtelMap, ax=ax)
    asvGEO.plot(
        marker="o",
        ax=ax,
        linewidth=2,
        edgecolor=asvGEO['landCols'],
        facecolor=color,
        markersize=asvGEO[asv]*10000)
    return(asvGEO)

mapOneASV("ASV2", color="b", jitter=0, showLand=True)

## works. Now, which were Udeaobacter?

## back in R for a second...where is udaeobacter?

library(phyloseq)

load("psCleanedUP.rda")

genera <- tax_table(psCleanedUp)@.Data[,'Genus']
filtUdaeo <- genera == "Candidatus Udaeobacter"
filtUdaeo[is.na(filtUdaeo)]  <- FALSE
sum(filtUdaeo)
udaeoASVs <- names(genera[filtUdaeo])
udaeoASVs ## ASV621 is our new udaeo

### back in python

plt.close('all')
mapOneASV("ASV621", color="red", jitter=0, showLand=True)

#### okay presentation done, back to getting HMSC to work #####

## there seemed to be several issues,
## start with the phylogenetic tree - not ultrametric, apparently

## did I mess up the fasttree2 settings? is the tree rooted?
## line 3074 above 

ssuAlignOut="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariAbundantASV16s_ali/sulariAbundantASV16s_ali.bacteria.afa"

ls $ssuAlignOut

FastTree -h

FastTree -gtr -nt < $ssuAlignOut > sulariFastTree.nwk 

## let's get the silva tree on ARB:

sudo chown daniel:daniel /media/vol
mkdir /media/vol/phylo_dbs

mkdir -p /media/vol/phylo_dbs/silva/silva138.1

cd /media/vol/phylo_dbs/silva/silva138.1

## nice "skeleton" of small subunit tree of life available from ARB/SILVA folks:
wget https://www.arb-silva.de/fileadmin/arb_web_db/release_138_1/ARB_files/SILVA_138.1_SSURef_NR99_12_06_20_opt.arb.gz
gunzip SILVA_138.1_SSURef_NR99_12_06_20_opt.arb.gz

## can we open this in ARB?

## where are our ref sequences? 

## old ones are here:
sulariAbundantRefSeqs="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariAbundantASV16s_ali/sulariAbundantASV16s_ali.bacteria.fa"

## but maybe we should make a new one?:

R

library('phyloseq')
load("psCleanedUP.rda")
Biostrings::writeXStringSet(refseq(psCleanedUp), "sulariCleanedUP16s.fna", append=FALSE,
                                  compress=FALSE, compression_level=NA, format="fasta")

## so these are at: 
ls /home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariCleanedUP16s.fna

## get rid of line breaks:
seqtk seq -l 0 sulariCleanedUP16s.fna > sulariCleanedUP16s_noLB.fna

mv sulariCleanedUP16s_noLB.fna sulariCleanedUP16s.fna



ls /home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariCleanedUP16s_noLB.fna

## Dimitri says use sina to do an alignment, outside of arb because arb has an old version of sina:

## sina folks recommend conda:

conda create -n sina sina

conda activate sina

## the command that dimitri gave me for this:
sina \
    --threads 12 \
    --db /home/dmeier/ARB/SILVA_138.1_SSURef_NR99_pin50_algn70.arb \
    -i MnOX_Chris_ASVs.fasta \
    -o MnOX_Chris_ASVs_aligned.arb \
    --search \
    --search-min-sim 0.7 \
    --search-no-fast \
    --fs-kmer-no-fast \
    --auto-filter-field tax_slv\
    --lca-fields tax_slv,tax_gtdb \
    --outtype csv \
    -o MnOX_Chris_ASVs_LCA_classified.tsv


cd /media/vol/phylo_dbs/silva/silva138.1


## modified for this job

conda activate sina

ls $sulariRepSeqs
ls $silvaDB

## on nanocomp
sulariRepSeqs=/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis/sulariCleanedUP16s.fna
silvaDB=/media/vol1/daniel/sulariArne/soilAnalysis/files2big4github/SILVA_138.1_SSURef_NR99_12_06_20_opt.arb

## on local tower
sulariRepSeqs=/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariCleanedUP16s.fna
silvaDB=/media/vol/phylo_dbs/silva/silva138.1/SILVA_138.1_SSURef_NR99_12_06_20_opt.arb

conda activate sina

sina \
    --threads 3 \
    --db $silvaDB \
    -i $sulariRepSeqs \
    -o sulariRepSeqs_aligned.fna \
    -o sulariRepSeqs_aligned.arb \
    -o sulariRepSeqs_aligned.csv 

## default outputs an arb database
## which won't merge, issues with name server or something, don't understand

cd /media/vol1/daniel/sulariArne/soilAnalysis/files2big4github

wget https://www.arb-silva.de/fileadmin/arb_web_db/release_138_1/ARB_files/SILVA_138.1_SSURef_NR99_12_06_20_opt.arb.gz

gunzip SILVA_138.1_SSURef_NR99_12_06_20_opt.arb.gz

## and arb not installed:

cd /media/vol1/daniel/arbInstall

wget --recursive \
  --no-parent -nH --cut-dirs=2 \
  http://download.arb-home.de/release/latest/
sudo bash /usr/arb/SH/arb_installubuntu4arb.sh arb
## etc etc

## try again on nanoComp 

## and that did not work. all asvs went into archea, for some reason. 
## let's try a different approach - make the denovo 16s tree, then 
## import it into ARB and root it. See if this fixes our not-ultrametic
## problem.

## if not, drop phylogeny, and keep on debugging HMSC models

## so, make a tree with the cleaned up 
## I think we can do this locally, not on the lab computer or denbi

## so remake the tree:


## line 3074 above , now using the cleanup OTUs
conda activate spatialDirt

ssu-align -n bacteria sulariCleanedUP16s.fna sulariAbundantASV16s_ali

## then a strict mask, don't trust any ambiguous calls
ssu-mask --pf 0.9999 --pt 0.9999 sulariAbundantASV16s_ali

## get a fasta alignment
ssu-mask --stk2afa sulariAbundantASV16s_ali

grep ">" sulariAbundantASV16s_ali/sulariAbundantASV16s_ali.bacteria.afa | wc -l ## 3314 sequences

grep ">" sulariCleanedUP16s.fna | wc -l ## 3314 sequences, same. Good sign, I guess. Last time some sequences were unalignable.


ssuAlignOut="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariAbundantASV16s_ali/sulariAbundantASV16s_ali.bacteria.afa"

ls $ssuAlignOut

FastTree -gtr -nt < $ssuAlignOut > sulariFastTree.nwk 

## look at it with APE and arb:
## arb crashed. that old dinosaur

R 
library(ape) 
library(phyloseq) 
library(Hmsc) 

load("psCleanedUp.rda")

aa = ape::read.tree("sulariFastTree.nwk")

## what should we use as an outgroup? Archea?

psCleanedUp

unique(tax_table(psCleanedUp)[,"Kingdom"])

archFilter <- tax_table(psCleanedUp)[,"Kingdom"] == "Archaea"
archFilter <- archFilter@.Data[,1]
archeaPS = prune_taxa( archFilter, psCleanedUp)

taxa_names(archeaPS)

## just checking, do the archea lump together?

png('unrooted.png')
plot_tree(psCleanedUp, color="Kingdom")
dev.off()

taxa_names(archeaPS) ## can we use this?

rooted_tree = root(aa, outgroup = taxa_names(archeaPS), resolve.root = TRUE)


phy_tree(psCleanedUp) <- rooted_tree
plot_tree(psCleanedUp, color="Kingdom")

## that looks better
is.rooted(phy_tree(psCleanedUp))

#png('unrooted.png')
plot_tree(psCleanedUp, color="Kingdom")
#dev.off()

class(phy_tree(psCleanedUp))

## can we give this to our phanghorn, in a different env?
tree = phy_tree(psCleanedUp)
save(tree, file="rooted_tree.rda")

## update notebook with this...

load("psCleanedUp.rda")


## okay, does HMSC consider this tree ultrametric?

##### try again on HMSC with new tree #######3

## as above on line 5961

denbiDirt

soil

conda activate r_env

R

library(Hmsc)
library(phyloseq)
library(ape)


rm(list=ls())
## merge the spatial and env data:
xycoords = read.csv("sulariSpatial.csv", row.names=1)
envData <- read.csv("sulariEnv.csv", row.names="SampleID")
aa <- merge(x=envData, y=xycoords, by=0, )
row.names(aa) <- aa$Row.names
aa[1] <- NULL
aa <- aa[rownames(envData),]
envData <- aa; rm(aa)

## this time, use our ps object with phylogeny and clustered OTUs:
load("psCleanedUp.rda")
psCleanedUp = prune_samples(!(rownames(sample_data(psCleanedUp)) %in% c("C1.1","C1.2","C2.1","C2.2")), psCleanedUp)
## remove species that are now zero
psCleanedUp = prune_taxa( colSums(otu_table(psCleanedUp)) > 0, psCleanedUp)
bactRaw <- otu_table(psCleanedUp)@.Data
## they include a term for sequencing depth
BacteriaDepth<-log(rowSums(bactRaw))
## make pa matrix
bact.pa<-ifelse(bactRaw>0,1,0)
## go to relative abundances
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat

## sanity check
bactRaw[1:5,1:5]
bact.rel[1:5,1:5]
## check some ASVs
33/sum(bactRaw[1,])
415/sum(bactRaw[3,]) 

## thresholds:
threshold.prev = 0.05 ## present in at least 5% of samples
## try a secondary reduction in diversity, for memory purposes:
threshold.abu = 0.003 ## only OTU that reaches at least 0.3% abundance of one sample
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples? We lose ~300 OTUs with this
cond2=apply(bact.rel,2,max)>=threshold.abu 
bactData <- bactRaw[, cond1 & cond2 ] 
################################################
sum(bactData > 0)
sum(bact.pa)
dim(bactData)
dim(bact.pa)


Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
BacteriaDepth<-log(rowSums(bactRaw))
varsOfInterest = c("PlotID","soil_respiration","Land_type","pH","N","C","Temperature","waterCont","xx","yy")
XData<-data.frame(envData[,varsOfInterest],BacteriaDepth)
## let's try skipping the scaling, because the documentation says 
## that the X matrix is scaled by default.
#XData[,-2]=scale(XData[,-2])
## probably still need to convert categoricals to factor
XData$Land_type <- as.factor(XData$Land_type)
## community data
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about. 
## the documentation for ?Hmsc does recommend scaling the Y matrix when using "default priors".
## I'm not sure that we are. I thought the defaults were gaussian dists, so this would make since. 
## ugh, head hurts. Just follow, be a good scientist, just follow...
## make our second, presence absence matrix:
Ypa = 1*(Y>0)
## bring them together for a final Y matrix
Y = cbind(Ypa,Yabu)
## I think HMSC won't accept missing data. I guess we could interpolate. 
## for now, just use what we have:
ccases <- complete.cases(XData) ## 112 samples, we only lose 8 
XData = XData[ccases,] 
Y = Y[ccases,] 
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))
## sample effects
sample.id = row.names(Y)
## plot effects, also for spatial
samplePlotid = XData$PlotID
studyDesign = data.frame("sample"=as.factor(sample.id), "site_spatial"=as.factor(samplePlotid))
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID 
plot_coords$PlotID <- NULL
## cleanup XData a bit:
varsOfmodel <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont","BacteriaDepth")
XData <- XData[,c(varsOfmodel)]
## regression model for environmental covariates.
XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature
XFormula1 = ~ BacteriaDepth ## check this next. Sort of a null model, I guess.
## random level for plots
rL.sample = HmscRandomLevel(units = sample.id)
## random level for spatial dist
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.

m0 = Hmsc(Y=Y,
         XData = XData,  XFormula = XFormula0,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         phyloTree=phy_tree(psCleanedUp), 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})

## change priors for abundance matrix to gaussian. 
for (i in (ns+1):(2*ns)){
   m0$distr[i,1:2] = c(1,1)
}

print(paste("start time is", Sys.time()))

thin = 1
samples = 1000
nChains = 4
nP = 10
m0 = sampleMcmc(m0, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)

## same error for tree - why?
## https://github.com/hmsc-r/HMSC/issues/140


load(psCleanedUP.rda)



ape::is.ultrametric(phy_tree(psCleanedUp)) ## nope

tre <- (phy_tree(psCleanedUp)) 
phylcor <- vcv(tre)
## phylcor need to be inverted, and for that its determinant should be >0
det(phylcor) ## 0, that is a problem...
## check the rank
attr(chol(phylcor, pivot=TRUE), "rank")
## dim is 3313, and rank 3313
dim(phylcor)
which(duplicated(phylcor)) ## 0, no dups...

## ugh, not sure 

#conda activate r_env
#conda install bioconda::r-phangorn

## phanghorn won't work in our environment, try the system R:

install.packages("BiocManager")
BiocManager::install("phyloseq")
install.packages("phangorn")

R

library(ape)
library("phangorn")
library(phyloseq)

aa = ape::read.tree("sulariFastTree.nwk")

load( "rooted_tree.rda")

ape::is.ultrametric(tree) ## nope

ape::is.ultrametric(aa) ## nope

ape::is.rooted(aa) ## nope

tree<-nnls.tree(cophenetic(aa),aa,
        rooted=TRUE,trace=0)

tree<-nnls.tree(cophenetic(tree),tree, rooted=TRUE,trace=0)

ape::is.ultrametric(tree) 

ape::is.rooted(tree) 

## that helps. can we plug this object into the other R environment?

save(tree, file='ultraTree.rda')

load('ultraTree.rda')

ape::is.ultrametric(tree)  ## works. Add this to our psobject instead?

ape::is.rooted(tree)  ## works. Add this to our psobject instead?


### start over  with phyloseq

load("psCleanedUp.rda")

psCleanedUp

plot_tree(psCleanedUp, color="Kingdom")

aa <- phy_tree(psCleanedUp)


ape::is.rooted(aa)  ## works. Add this to our psobject instead?



ultratree <- nnls.tree(cophenetic(aa),aa, method="ultrametric",trace=0)

ape::is.ultrametric(ultratree)  ## yup
ape::is.rooted(ultratree)  ## yup

phy_tree(psCleanedUp) <- ultratree

ape::is.ultrametric(phy_tree(psCleanedUp))  ## yup
ape::is.rooted(phy_tree(psCleanedUp)) ## yup

dev.new()

plot_tree(psCleanedUp, color="Kingdom")

save(psCleanedUp, file="psCleanedUp.rda")

## and back to hmsc, r_env

## still getting the ultrametric error:

tre <- phy_tree(psCleanedUp)

is.ultrametric(tre)
phylcor <- vcv(tre)

## phylcor need to be inverted, and for that its determinant should be >0

det(phylcor) ## 0, why???

attr(chol(phylcor, pivot=TRUE), "rank")

## dim is 1440, and rank 1400

dim(phylcor)

## not sure.
## write a report for the developers, move on without phylogeny
which(duplicated(phylcor))  ## none. not sure.

## I see two possibilities - the assymetry between the tree and the 
## thinned data are causing problems, or the hurdle model, with it's 
## double entries for all OTUs is causing the problem.

## but we need some success here. Just try running the model without 
## phylogeny. 



R

library(Hmsc)
library(phyloseq)
library(ape)


rm(list=ls()) ## merge the spatial and env data:
xycoords = read.csv("sulariSpatial.csv", row.names=1)
envData <- read.csv("sulariEnv.csv", row.names="SampleID")
aa <- merge(x=envData, y=xycoords, by=0, )
row.names(aa) <- aa$Row.names
aa[1] <- NULL
aa <- aa[rownames(envData),]
envData <- aa; rm(aa)

## this time, use our ps object with phylogeny and clustered OTUs:
load("psCleanedUp.rda")
psCleanedUp = prune_samples(!(rownames(sample_data(psCleanedUp)) %in% c("C1.1","C1.2","C2.1","C2.2")), psCleanedUp)
## remove species that are now zero
psCleanedUp = prune_taxa( colSums(otu_table(psCleanedUp)) > 0, psCleanedUp)
bactRaw <- otu_table(psCleanedUp)@.Data
## they include a term for sequencing depth
BacteriaDepth<-log(rowSums(bactRaw))
## make pa matrix
bact.pa<-ifelse(bactRaw>0,1,0)
## go to relative abundances
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat

## sanity check
bactRaw[1:5,1:5]
bact.rel[1:5,1:5]
## check some ASVs
33/sum(bactRaw[1,])
415/sum(bactRaw[3,]) 

## thresholds:
threshold.prev = 0.05 ## present in at least 5% of samples
## try a secondary reduction in diversity, for memory purposes:
threshold.abu = 0.003 ## only OTU that reaches at least 0.3% abundance of one sample
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples? We lose ~300 OTUs with this
cond2=apply(bact.rel,2,max)>=threshold.abu 
bactData <- bactRaw[, cond1 & cond2 ] 
################################################
sum(bactData > 0)
sum(bact.pa)
dim(bactData)
dim(bact.pa)


Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
BacteriaDepth<-log(rowSums(bactRaw))
varsOfInterest = c("PlotID","soil_respiration","Land_type","pH","N","C","Temperature","waterCont","xx","yy")
XData<-data.frame(envData[,varsOfInterest],BacteriaDepth)
## let's try skipping the scaling, because the documentation says 
## that the X matrix is scaled by default.
#XData[,-2]=scale(XData[,-2])
## probably still need to convert categoricals to factor
XData$Land_type <- as.factor(XData$Land_type)
## community data
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about. 
## the documentation for ?Hmsc does recommend scaling the Y matrix when using "default priors".
## I'm not sure that we are. I thought the defaults were gaussian dists, so this would make since. 
## ugh, head hurts. Just follow, be a good scientist, just follow...
## make our second, presence absence matrix:
Ypa = 1*(Y>0)
## bring them together for a final Y matrix
Y = cbind(Ypa,Yabu)
## I think HMSC won't accept missing data. I guess we could interpolate. 
## for now, just use what we have:
ccases <- complete.cases(XData) ## 112 samples, we only lose 8 
XData = XData[ccases,] 
Y = Y[ccases,] 
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))
## sample effects
sample.id = row.names(Y)
## plot effects, also for spatial
samplePlotid = XData$PlotID
studyDesign = data.frame("sample"=as.factor(sample.id), "site_spatial"=as.factor(samplePlotid))
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID 
plot_coords$PlotID <- NULL
## cleanup XData a bit:
varsOfmodel <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont","BacteriaDepth")
XData <- XData[,c(varsOfmodel)]
## regression model for environmental covariates.
XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature
XFormula1 = ~ BacteriaDepth ## check this next. Sort of a null model, I guess.
## random level for plots
rL.sample = HmscRandomLevel(units = sample.id)
## random level for spatial dist
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.

m0 = Hmsc(Y=Y,
         XData = XData,  XFormula = XFormula0,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})

## change priors for abundance matrix to gaussian. 
for (i in (ns+1):(2*ns)){
   m0$distr[i,1:2] = c(1,1)
}

print(paste("start time is", Sys.time()))

thin = 1
samples = 1000
nChains = 4
nP = 10

m0 = sampleMcmc(m0, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)

## some other problem now, don't understand the error

## what if we run a simpler model.
## try abundances without the hurdle model, just to see if it works:

conda activate r_env

soil

R
library(Hmsc)
library(phyloseq)
library(ape)

rm(list=ls()) ## merge the spatial and env data:
xycoords = read.csv("sulariSpatial.csv", row.names=1)
envData <- read.csv("sulariEnv.csv", row.names="SampleID")
aa <- merge(x=envData, y=xycoords, by=0, )
row.names(aa) <- aa$Row.names
aa[1] <- NULL
aa <- aa[rownames(envData),]
envData <- aa; rm(aa)
## this time, use our ps object with phylogeny and clustered OTUs:
load("psCleanedUp.rda")
psCleanedUp = prune_samples(!(rownames(sample_data(psCleanedUp)) %in% c("C1.1","C1.2","C2.1","C2.2")), psCleanedUp)
## remove species that are now zero
psCleanedUp = prune_taxa( colSums(otu_table(psCleanedUp)) > 0, psCleanedUp)
bactRaw <- otu_table(psCleanedUp)@.Data
## make pa matrix
bact.pa<-ifelse(bactRaw>0,1,0)
## go to relative abundances
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat
## thresholds:
threshold.prev = 0.05 ## present in at least 5% of samples
## try a secondary reduction in diversity, for memory purposes:
threshold.abu = 0.01 ## only OTU that reaches at least 0.3% abundance of one sample
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples?
cond2=apply(bact.rel,2,max)>=threshold.abu 
bactData <- bactRaw[, cond1 & cond2 ] 
################################################
sum(bactData > 0)
sum(bact.pa)
dim(bactData)
dim(bact.pa)
Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
BacteriaDepth<-log(rowSums(bactRaw))
varsOfInterest = c("PlotID","soil_respiration","Land_type","pH","N","C","Temperature","waterCont","xx","yy")
XData<-data.frame(envData[,varsOfInterest],BacteriaDepth)
#XData[,-2]=scale(XData[,-2]) ## think this is done by default?
## probably still need to convert categoricals to factor
XData$Land_type <- as.factor(XData$Land_type)
## community data
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about. 
## the documentation for ?Hmsc does recommend scaling the Y matrix when using "default priors".
## I'm not sure that we are. I thought the defaults were gaussian dists, so this would make since. 
## ugh, head hurts. Just follow, be a good scientist, just follow...
## make our second, presence absence matrix:
Ypa = 1*(Y>0)
## bring them together for a final Y matrix
##################################################################
## I think HMSC won't accept missing data. I guess we could interpolate. 
## for now, just use what we have:
ccases <- complete.cases(XData) ## 112 samples, we only lose 8 
XData = XData[ccases,] 
Y = Y[ccases,] 
Ypa = Ypa[ccases,]
Yabu = Yabu[ccases,]
Ycombo = cbind(Ypa,Yabu)
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))
## sample effects
sample.id = row.names(Y)
## plot effects, also for spatial
samplePlotid = XData$PlotID
studyDesign = data.frame("sample"=as.factor(sample.id), "site_spatial"=as.factor(samplePlotid))
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID 
plot_coords$PlotID <- NULL
## cleanup XData a bit:
varsOfmodel <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont","BacteriaDepth")
XData <- XData[,c(varsOfmodel)]
## regression model for environmental covariates.
XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature
XFormula1 = ~ BacteriaDepth ## check this next. Sort of a null model, I guess.
## random level for plots
rL.sample = HmscRandomLevel(units = sample.id)
## random level for spatial dist
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.
m_pa = Hmsc(Y=Ypa,
         XData = XData,  XFormula = XFormula0,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})
m_abu = Hmsc(Y=Yabu,
         XData = XData,  XFormula = XFormula0,
         distr={"normal"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})
m_combo = Hmsc(Y=Ycombo,
         XData = XData,  XFormula = XFormula0,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})

m_combo_nosp = Hmsc(Y=Ycombo,
                    XData = XData,  XFormula = XFormula0,
                    distr={"probit"} ,
                    studyDesign=studyDesign, 
                    ranLevels={list("sample"=rL.sample)})

## change priors for abundance matrix to gaussian. 
for (i in (ns+1):(2*ns)){
   m_combo$distr[i,1:2] = c(1,1)
}
print(paste("start time is", Sys.time()))
thin = 5
samples = 1000
nChains = 2
nP = 8

m_pa = sampleMcmc(m_pa, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)
m_abu = sampleMcmc(m_abu, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)
m_combo = sampleMcmc(m_combo, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)

m_combo_nosp = sampleMcmc(m_combo_nosp, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)

## dropping the spatial component seems to have fixed the "missing value" error...why?

## the last model reports that it has a posterior sampling, with some weird warnings:
1: In for (v in val) { ... : closing unused connection 5 (<-localhost:11362)
2: In for (v in val) { ... : closing unused connection 4 (<-localhost:11362)

## it is 100M. these things get big quickly

## save it, look at it tomorrow:

save(m_combo_nosp, file="m_combo_nosp.rda") ## then moved to "tooBig" folder

## this model runs (including full hurdle model), keep this text as is
## this means it is the spatial model that is breaking things.
## tomorrow, 
## explore non spatial model
## try non-spatial model with more species?
## try using a fake, simple spatial design and see if the complexity 
## is causing the problem?

##### HMSC  simplify spatial design #####

## let's try a spatial design in which there are just two regions,
## and all samples come from one or the other

denbiDirt

conda activate r_env
soil
R
library(Hmsc)
library(phyloseq)
library(ape)

rm(list=ls()) ## merge the spatial and env data:
xycoords = read.csv("sulariSpatial.csv", row.names=1)
envData <- read.csv("sulariEnv.csv", row.names="SampleID")
aa <- merge(x=envData, y=xycoords, by=0, )
row.names(aa) <- aa$Row.names
aa[1] <- NULL
aa <- aa[rownames(envData),]
envData <- aa; rm(aa)
## this time, use our ps object with phylogeny and clustered OTUs:
load("psCleanedUp.rda")
psCleanedUp = prune_samples(!(rownames(sample_data(psCleanedUp)) %in% c("C1.1","C1.2","C2.1","C2.2")), psCleanedUp)
## remove species that are now zero
psCleanedUp = prune_taxa( colSums(otu_table(psCleanedUp)) > 0, psCleanedUp)
bactRaw <- otu_table(psCleanedUp)@.Data
## make pa matrix
bact.pa<-ifelse(bactRaw>0,1,0)
## go to relative abundances
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat
## thresholds:
threshold.prev = 0.05 ## present in at least 5% of samples
## try a secondary reduction in diversity, for memory purposes:
threshold.abu = 0.003 ## only OTU that reaches at least 0.3% abundance of one sample
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples?
cond2=apply(bact.rel,2,max)>=threshold.abu 
bactData <- bactRaw[, cond1 & cond2 ] 
################################################
sum(bactData > 0)
sum(bact.pa)
dim(bactData)
dim(bact.pa)
Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
BacteriaDepth<-log(rowSums(bactRaw))
varsOfInterest = c("PlotID","soil_respiration","Land_type","pH","N","C","Temperature","waterCont","xx","yy")
XData<-data.frame(envData[,varsOfInterest],BacteriaDepth)
#XData[,-2]=scale(XData[,-2]) ## think this is done by default?
## probably still need to convert categoricals to factor
XData$Land_type <- as.factor(XData$Land_type)
## community data
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about. 
## the documentation for ?Hmsc does recommend scaling the Y matrix when using "default priors".
## I'm not sure that we are. I thought the defaults were gaussian dists, so this would make since. 
## ugh, head hurts. Just follow, be a good scientist, just follow...
## make our second, presence absence matrix:
Ypa = 1*(Y>0)
## bring them together for a final Y matrix
##################################################################
## I think HMSC won't accept missing data. I guess we could interpolate. 
## for now, just use what we have:
ccases <- complete.cases(XData) ## 112 samples, we only lose 8 
XData = XData[ccases,] 
Y = Y[ccases,] 
Ypa = Ypa[ccases,]
Yabu = Yabu[ccases,]
Ycombo = cbind(Ypa,Yabu)
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))
## sample effects
sample.id = row.names(Y)
## plot effects, also for spatial
#############################################################################################
## attempted, "real" spatial variable
samplePlotid = XData$PlotID
studyDesign = data.frame("sample"=as.factor(sample.id), "site_spatial"=as.factor(samplePlotid))
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID 
plot_coords$PlotID <- NULL
#############################################################################################
################################################################################################
## toy spatial design:
#samplePlotid = XData$PlotID
#studyDesign = data.frame("sample"=as.factor(sample.id), "site_spatial"=as.factor(samplePlotid))
#plot_coords <- XData[,c("PlotID","xx","yy")]
#plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
#rownames(plot_coords) <- plot_coords$PlotID 
### make up two sites: the coordinates from 109 and 119 are maximally far apart, they are at P204 and P0114:
#plot_coordsPlay <- plot_coords[c("P0204","P0114"),]
### replace our study design so that each site only belongs to one or the other of these two plots:
#plot_coords$PlotID <- NULL
#plot_coordsPlay$PlotID <- NULL
#studyDesignPlay <- studyDesign
#studyDesignPlay["site_spatial"] <- as.factor(sample( rownames(plot_coordsPlay), nrow(studyDesign), replace=TRUE))
### check:
##plot_coords
##plot_coordsPlay
##studyDesign
##studyDesignPlay
### looks good
################################################################################################
## cleanup XData a bit:
varsOfmodel <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont","BacteriaDepth")
XData <- XData[,c(varsOfmodel)]
## regression model for environmental covariates.
XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature
XFormula1 = ~ BacteriaDepth ## check this next. Sort of a null model, I guess.
## random level for plots
rL.sample = HmscRandomLevel(units = sample.id)
## random level for spatial dist
## original model:
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.
## playing with spatial model:
#rL.site_spatialPlay = HmscRandomLevel(sData = plot_coordsPlay)
#rL.site_spatialPlay = setPriors(rL.site_spatial,nfMin=1,nfMax=3) 
#m_pa = Hmsc(Y=Ypa,
#         XData = XData,  XFormula = XFormula0,
#         distr={"probit"} ,
#         studyDesign=studyDesign, 
#         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})
#m_abu = Hmsc(Y=Yabu,
#         XData = XData,  XFormula = XFormula0,
#         distr={"normal"} ,
#         studyDesign=studyDesign, 
#         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})
m_combo = Hmsc(Y=Ycombo,
         XData = XData,  XFormula = XFormula0,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})
#m_combo_nosp = Hmsc(Y=Ycombo,
#                    XData = XData,  XFormula = XFormula0,
#                    distr={"probit"} ,
#                    studyDesign=studyDesign, 
#                    ranLevels={list("sample"=rL.sample)})
#m_combo_spPlay = Hmsc(Y=Ycombo,
#                    XData = XData,  XFormula = XFormula0,
#                    distr={"probit"} ,
#                    studyDesign=studyDesignPlay, 
#                    ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatialPlay)})
## change priors for abundance matrix to gaussian. 
#for (i in (ns+1):(2*ns)){
#   m_combo$distr[i,1:2] = c(1,1)
#}
#for (i in (ns+1):(2*ns)){
#   m_combo_nosp$distr[i,1:2] = c(1,1)
#}
#for (i in (ns+1):(2*ns)){
#   m_combo_spPlay$distr[i,1:2] = c(1,1)
#}
#print(paste("start time is", Sys.time()))
########## runit #############
thin = 5
samples = 1000
nChains = 3
nP = 8
#m_pa = sampleMcmc(m_pa, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)
#m_abu = sampleMcmc(m_abu, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)
m_combo = sampleMcmc(m_combo, 
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP,
                     verbose=1)
#m_combo_nosp = sampleMcmc(m_combo_nosp, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)
#m_combo_spPlay = sampleMcmc(m_combo_spPlay, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = nP,verbose=1)
#m_combo_spPlay = sampleMcmc(m_combo_spPlay, 
#                            samples = samples,  
#                            thin=thin, 
#                            #adaptNf=ceiling(0.4*samples*thin), 
#                            transient = ceiling(0.5*samples*thin), 
#                            nChains = nChains, 
#                            nParallel = nP, 
#                            verbose=1)

## if we leave out the "adaptNf" setting, the model runs with the simplified spatial setup.
## try it with the actual spatial 
## seems to be working. no error reported after, RAM usage seems okay asdf

## what is this setting? It's related to optimizing the latent factors I think, not clear
## with thresholds at 
## threshold.prev = 0.05 ## present in at least 5% of samples
## threshold.abu = 0.003 ## only OTU that reaches at least 0.3% abundance of one sample
## otus = 848, still a lot, but ram usage stays low
## 3 chains doesn't add that much, but does seem to increase memory use. Consider keeping at 2 chains if RAM is a problem

## looks like maybe the spatial issue is resolved.

## revisit taxonomic issue one more time?

## theory - subsetting species renders our otherwise fine tree out of format
sulariPlot_utm['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]

fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    facecolor=sulariPlotsDF['landColors'],
    markersize=400) 


grassPatch = Patch(color='y', label='grassland',)
forestPatch = Patch(color='g', label='forest')
farmPatch = Patch(color='b', label='arable land')
ax.legend(handles=[grassPatch, forestPatch, farmPatch], 
          loc="lower left",
          fontsize=15,
)

## if we want to compare just grassland and forest

plt.close('all')
onlyGrassForest = sulariPlot_utm[sulariPlot_utm['Land.type'].apply(lambda x: x in ["Forest", "Grassland"])]
fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
onlyGrassForest.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    linewidths=2,
    facecolor=onlyGrassForest['landColors'],
    markersize=200) 
ax.ticklabel_format(style='plain', axis='y', useOffset=False)
grassPatch = Patch(color='y', label='grassland',)
forestPatch = Patch(color='g', label='forest')
ax.legend(handles=[grassPatch, forestPatch], loc='lower left')
ax.add_artist(ScaleBar(1, location='lower right')) 
ax.set_xlim([265500, 286930])
ax.set_ylim([5547227, 5570000])
plt.savefig('forestVsGrasslandMapUTM.png', dpi=600, format='png')

## Look at the turnover data:

## lat/long
aa = pd.DataFrame({'xx':envData.Longitude, 'yy':envData.Latitude})
physDist = sp.distance.pdist(aa, metric='euclidean')
bcDist = sp.distance.pdist(comData, metric='brayCurtis')
fig, ax = plt.subplots()
ax.scatter(physDist, bcDist)
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title(label="Turnover with Lat/Lon", loc='center')

## utms
aa = pd.DataFrame({'xx':sulariPlot_utm.geometry.x, 'yy':sulariPlot_utm.geometry.y})
physDist = sp.distance.pdist(aa, metric='euclidean')
bcDist = sp.distance.pdist(comData, metric='brayCurtis')
fig, ax = plt.subplots()
ax.scatter(physDist, bcDist)
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title(label="Turnover with UTM", loc='center')

plt.close('all')
## subset by landtype
for lt in [ "Arableland" ,"Grassland" ,"Forest"]: 
    print(lt)
    edf = envData[envData['Land.type'] == lt]
    cdf = comData.loc[edf.index]
    aa = pd.DataFrame({'xx':edf.Longitude, 'yy':edf.Latitude})
    aa = aa.iloc[0:120,:]
    physDist = sp.distance.pdist(aa, metric='euclidean')
    bcDist = sp.distance.pdist(cdf, metric='brayCurtis')
    fig, ax = plt.subplots()
    ax.scatter(physDist, bcDist)
    ax.set_title(lt)
    ax.set_title(label= (lt + " in degrees"), loc='center')
    X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
    ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')

## well that looks pretty much like I hypothesized
## good stuff.

sulariPlot_utm.head()

plt.close('all')
plt.rc('ytick', labelsize=15)
plt.rc('xtick', labelsize=15)
lts = [ "Arableland" ,"Grassland" ,"Forest"]
#lts = [ "Grassland" ,"Forest"]
fig, axes = plt.subplots(nrows=1, ncols=len(lts), sharey=True)
axes = axes.flatten()
for nu,lt in enumerate(lts):
varsOfmodel <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont","BacteriaDepth")
XData <- XData[,c(varsOfmodel)]
## regression model for environmental covariates.
XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature
XFormula1 = ~ BacteriaDepth ## check this next. Sort of a null model, I guess.
## random level for plots
rL.sample = HmscRandomLevel(units = sample.id)
## random level for spatial dist
## original model:
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.
## playing with spatial model:
#rL.site_spatialPlay = HmscRandomLevel(sData = plot_coordsPlay)
#rL.site_spatialPlay = setPriors(rL.site_spatial,nfMin=1,nfMax=3) 
m_combo = Hmsc(Y=Ycombo,
         XData = XData,  XFormula = XFormula0,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         phyloTree=tre, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})
## change priors for abundance matrix to gaussian. 
for (i in (ns+1):(2*ns)){
   m_combo$distr[i,1:2] = c(1,1)
}
print(paste("start time is", Sys.time()))

########## runit #############
thin = 5
samples = 1000
nChains = 3
nP = 8
m_hurdle_phylo  = sampleMcmc(m_combo, 
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP,
                     verbose=1)



## we figured out how to make an ultrametric tree with somewhere around line 6680

## do we need to repeat this process whenever we set thresholds and subset 
## our species?

## to subset our phylogenetic tree:

aa <-  prune_taxa( colnames(bactData), psCleanedUp)

aa; psCleanedUp

ape::is.ultrametric(phy_tree(psCleanedUp))  ## yup
ape::is.rooted(phy_tree(psCleanedUp)) ## yup
ape::is.ultrametric(phy_tree(aa))  ## yup
ape::is.rooted(phy_tree(aa)) ## yup

colnames(bactData)

## so should be:
tre <- phy_tree(prune_taxa( colnames(bactData), psCleanedUp))

ape::is.ultrametric(tre)  ## yup

ape::is.rooted(tre) ## yup

dim(bact.pa)

## the above model still returns the error, not ultrametric

## send a report and keep moving. Don't expect much out of the
## phylogeny, anyway.

## set up model to run over weekend, see if it made it on monday

## use the model above, clean up and repeat here:



denbiDirt

conda activate r_env

soil

R

library(Hmsc)
library(phyloseq)
library(ape)

rm(list=ls()) ## merge the spatial and env data:
xycoords = read.csv("sulariSpatial.csv", row.names=1)
envData <- read.csv("sulariEnv.csv", row.names="SampleID")
aa <- merge(x=envData, y=xycoords, by=0, )
row.names(aa) <- aa$Row.names
aa[1] <- NULL
aa <- aa[rownames(envData),]
envData <- aa; rm(aa)
load("psCleanedUp.rda")
psCleanedUp = prune_samples(!(rownames(sample_data(psCleanedUp)) %in% c("C1.1","C1.2","C2.1","C2.2")), psCleanedUp)
psCleanedUp = prune_taxa( colSums(otu_table(psCleanedUp)) > 0, psCleanedUp)
bactRaw <- otu_table(psCleanedUp)@.Data
bact.pa<-ifelse(bactRaw>0,1,0)
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat
threshold.prev = 0.05 ## present in at least n% of samples
threshold.abu = 0.002 ## only OTU that reaches at least n% abundance of one sample
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples?
cond2=apply(bact.rel,2,max)>=threshold.abu 
bactData <- bactRaw[, cond1 & cond2 ] 
################################################
sum(bactData > 0)
sum(bact.pa)
dim(bactData)
dim(bact.pa)

Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
BacteriaDepth<-log(rowSums(bactRaw))
varsOfInterest = c("PlotID","soil_respiration","Land_type","pH","N","C","Temperature","waterCont","xx","yy")
XData<-data.frame(envData[,varsOfInterest],BacteriaDepth)
XData$Land_type <- as.factor(XData$Land_type)
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about. 
Ypa = 1*(Y>0)
ccases <- complete.cases(XData) ## 112 samples, we only lose 8 
XData = XData[ccases,] 
Y = Y[ccases,] 
Ypa = Ypa[ccases,]
Yabu = Yabu[ccases,]
Ycombo = cbind(Ypa,Yabu)
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))
sample.id = row.names(Y)
samplePlotid = XData$PlotID
studyDesign = data.frame("sample"=as.factor(sample.id), "site_spatial"=as.factor(samplePlotid))
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID 
plot_coords$PlotID <- NULL
varsOfmodel <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont","BacteriaDepth")
XData <- XData[,c(varsOfmodel)]
XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature
rL.sample = HmscRandomLevel(units = sample.id)
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.
m_hurdleSpatial = Hmsc(Y=Ycombo,
         XData = XData,  XFormula = XFormula0,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)})
#save(m_hurdleSpatial, file="m_hurdleSpatial.rda")

########## runit #############

thin = 5
samples = 1000
nChains = 3
nP = 8
m_hurdleSpatial = sampleMcmc(m_hurdleSpatial, 
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP,
                     verbose=1)

## with thresholds at 
## threshold.prev = 0.05 
## threshold.abu = 0.002 
## otus = 1315, still a lot, but ram usage stays low
## looks like 3 chains is too much, go to 2 chains


################## runSample.r #######################


library(Hmsc)
#load("m_hurdleSpatial.rda")
load("m_hurdleSpatial.rda")

thin = 5
samples = 1000
nChains = 2
nP = 8
m_hurdleSpatial = sampleMcmc(m_hurdleSpatial, 
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP,
                     verbose=1)

save(m_hurdleSpatial, file="m_hurdleSpatial_sampled.rda")

######################################################

soil 
conda activate r_env
nohup Rscript runSample.r &> runSample.log &

## this is finished, ran for ~3 weeks, precious, so back it up on desktop:

## get it local
getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/m_hurdleSpatial_sampled.rda"
putItHere="/media/vol/fichtelberg/hmsc/fullModels/noEffluxNoSptempBig/"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere

## running for a while. to check on it:

ssh -p 30267 -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6 cat /vol/data/fichtelgebirgeSoils/spatialAnalysis/runSample.log

ssh -p 30267 -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6 ps 2140 ## parent process

ssh -p 30267 -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6 free -h

ssh -p 30267 -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6 ps -f 2140

## not run, but might be useful if we need more data:
c.Hmsc ## for combining chains from exact resamples

## seems likely, because I think we set the burn too high and the thinning will take out a most of 
## the sample anyway

## so what to do now? look up scripts from generating sampling statistic, 
## adapt to our data 

## we will need to address seasonality somehow. How?

## build a smaller model with just the sites that have repeated data


import pandas as pd

sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"

envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
envData['PlotID'].unique()

envData['PlotID'].duplicated(keep=False).to_list()

envData['PlotID'].duplicated(keep=False).sum()

repFilt = envData['PlotID'].duplicated(keep=False)

repeatedPlots = envData[repFilt].sort_values(by="PlotID")

repeatedPlots["PlotID"].unique() ## there are ten plots that were repeated through time

repeatedPlots.head()

len(repeatedPlots["PlotID"].unique()) ## there are ten plots that were repeated through time

## how many different sampling times do each have?j
repeatedPlots[["PlotID","Date"]].groupby("PlotID").nunique()

## interesting, S89 and S102 are the same plot and date...

ploti="P0228"

ploti="P0103"

repeatedPlots[repeatedPlots["PlotID"] == ploti] 

ploti="P0228"
repeatedPlots[repeatedPlots["PlotID"] == ploti].iloc[0,:]

repeatedPlots[repeatedPlots["PlotID"] == ploti].iloc[1,:]

## also, different land types?
## S89 is listed as arable land,
## S102 listed as forest.
## both are listed as P0228. Name implies forest.
## have the same sampling date

## will have to ask. Any other problems like that?

## all samples:
multLT = envData[["PlotID",'Land_type']].groupby("PlotID").nunique()

(multLT["Land_type"] > 1).to_list()
(multLT["Land_type"] > 1).sum() ## no, only looks like a problem at P0228, samples 89 and 102

## ask sulari. For the moment, discard. Not sure, maybe we should restart Hmsc models

## let's keep them going, just to know how long it will take if we run it again. 

## from the name and missing data, makes more sense to remove S89. This is the more
## likely error. Switch over to R to see if this improves our NMS:

library(vegan)

comData = read.csv("/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/comdat.csv", row.names=1)
envData = read.csv("sulariEnv.csv", row.names=1)
## get rid of controls
notControls=!(row.names(comData) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comData <- comData[notControls,]
## get rid of S89, think it is an error
comData <- comData[!(rownames(comData) == "S89"),]
comNMS <- metaMDS(comData, try=40)

write.csv(comNMS$points, file='comNMS.csv')

## and python for plotting as before:

python3
import matplotlib.pyplot as plt
plt.ion()
import pandas as pd
envData = pd.read_csv('sulariEnv.csv', index_col='SampleID')
nmsPts = pd.read_csv("comNMS.csv", index_col=0)
envData.drop('S89', inplace=True)
colorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
landCols = [ colorDict[i] for i in envData['Land_type'] ]

plt.close('all')
fig, ax = plt.subplots()
ax.scatter(x=nmsPts["MDS1"],
           y=nmsPts["MDS2"],
           c=landCols,
           s=190,
          )

Arableland_patch = Patch(color='#862d2d', label='Arableland')
Forest_patch = Patch(color='#006600', label='Forest')
Grassland_patch = Patch(color='#FF7F00', label='Grassland')
ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])

## yes, this loses on of the weird farmland points, but the 
## nms is otherwise almost identical. 

## but this won't affect our larger model, these points 
## were removed because of missing data

## won't worry about for the moment. 

## and on with temporal analysis


################################

## side project - repeat sulari's respiration graphs
## sulari needs some lines fit to her respiration data

## the graph of interest is soil respiration by microbial biomass

## a simple scatter graph to start:

conda activate spatialDirt

python3

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt; plt.ion()
import os
from sklearn.linear_model import LinearRegression
from matplotlib.patches import Patch

aa = pd.read_csv('sulariEnv.csv', index_col="SampleID")
## remove the weird samples we found yesterday = S89
aa.drop('S89',inplace=True)
aa.drop('S78',inplace=True) ## outlier, explanation below

## to plot one land type:

## 'Arableland', 'Grassland', 'Forest'

plt.close('all')
fig, ax = plt.subplots()
plotSpecs=[{"typ":"Arableland",
            "mkr":"o",
            "clr":"#eca2a5"},
           {"typ":"Grassland",
            "mkr":"s",
            "clr":"#98b7f2"},
           {"typ":"Forest",
            "mkr":"^",
            "clr":"#008000"}]
ax.set_xlabel('Microbial biomass (mg microbial C g soil$^{-1}$)')
ax.set_ylabel('Soil respiration (μg CO$_2$ hours$^{-1}$ g Soil$^{-1}$)')
for i in plotSpecs:
    bb = aa[aa['Land_type'] == i['typ']]
    ax.scatter(bb['MBC'], bb['soil_respiration'], marker=i['mkr'], c=i['clr'])
    cc = bb[['MBC','soil_respiration']].dropna()
    X = np.array(cc['MBC']).reshape(-1,1)
    Y = np.array(cc['soil_respiration']).reshape(-1,1)
    linMod =  LinearRegression().fit(X, Y)
    sortedX = cc['MBC'].sort_values().to_numpy().reshape(-1,1)
    ax.plot( sortedX, linMod.predict(sortedX), ls="--", lw=3, c=i['clr'])

grassPatch = Patch(color='#98b7f2', label='grassland',)
forestPatch = Patch(color='#008000', label='forest')
farmPatch = Patch(color='#eca2a5', label='arable land')
ax.legend(handles=[grassPatch, forestPatch, farmPatch],
          loc="lower right",
          bbox_to_anchor=(1.3, 0.0),
          fontsize=10
          )


print(lt, stats.linregress(physDist,bcDist)) 
## there is an outlier that Sulari left out
## should ask her why. 
## find it:

X,Y = plt.ginput(1)[0] ## 2.2346, 13.659
aa.query("MBC > 2 & soil_respiration > 13")
## S78. Still not sure why I should take this out, but ok
## dropped it above and reran

## also curious, there is a plot with very low respiration
## despite high microbial biomass

## it is:
aa.query("MBC > 2 & soil_respiration < 1")
## S12, P0118

aa.query("MBC > 2 & soil_respiration < 1").iloc[0,:]

sum(aa["PlotID"] == "P0118") ## not a repeated plot, sampled in summer. 
## not sure, no reason to throw it out.


##### sulari figure, grassland abundances #####

## she also needs a figure that shows just the ten most common 
## species for the ten most abundant figures in grasslands

## which ps should we use?

library(phyloseq)

load("psCleanedUp.rda")

#png(file="grasslandMostAbundantForSulari.png")
#svg(file="grasslandMostAbundantForSulari.svg")
## subset to grasslands:
onlyGrass <- prune_samples(sample_data(psCleanedUp)$Land.type == "Grassland", psCleanedUp)
## get the ten most common, and their taxonomic id on these:
nam <- names(sort(taxa_sums(onlyGrass), decreasing=TRUE)[1:10])
tax_table(onlyGrass)[nam,]
## looking at this, curate this a bit:
xlabNames <- c("Candidatus Udaeobacter","Rhizobiales","Bradyrhizobium","Nitrososphaeraceae","Chloroflexi, KS4-96","Solirubrobacterales", "Micrococcaceae", "Gaiella", "Acidothermus", "Rhizobiales")
options(scipen=999)
par(mar=c(12,6,2,2))
barplot(sort(taxa_sums(onlyGrass), decreasing=TRUE)[1:10],
        las=2,
        names.arg=xlabNames,
        ylab="",
        )
title(ylab="adjusted read abundances", line=4.5, cex.lab=1.2, family="Calibri Light")
#dev.off()

######## examine CO2 soil efflux data ########

## we have data for the CO2/licor chambers, but not entirely sure 
## what forms these take

## Betty seems to have created a file "COefflux.csv", 
## which is a processed, corrected data of some 
## kind from the licor data, I think, but I have
## lots of questions about how it was made and 
## what it is intended for. 

## so I need help from Betty or Nele, 
## and for now I will march on with exploring the 
## spatiotemporal model.

###### check ess from big model #####

## let's go ahead and start a second sampling of the
## big model, just in case. Probably we will
## have to restart it when we figure out the 
## CO2 flux data. But if not 

## can we check the the ESS of the variable before we
## do this?

denbiDirt

soil

conda activate r_env

R

library(Hmsc)

## on denbi:

load("/vol/data/fichtelgebirgeSoils/spatialAnalysis/m_hurdleSpatial_sampled.rda")

m_hurdleSpatial

mpost = convertToCodaObject(m_hurdleSpatial) ## too memory intensive. 

## does it help to pickout only the variable of interest?

?convertToCodaObject

mpost = convertToCodaObject(m_hurdleSpatial, 
                                Beta = TRUE,
                                Gamma = FALSE,
                                V = FALSE,
                                Sigma = FALSE,
                                Rho = FALSE,
                                Eta = FALSE,
                                Lambda = FALSE,
                                Alpha = FALSE,
                                Omega = FALSE,
                                Psi = FALSE,
                                Delta = FALSE)

## that worked for the moment

## plot these?

pdf(file="traces_m_hurdleSpatial.pdf")
plot(mpost$Beta)
dev.off()

## how is our beta ess?

es.beta <- effectiveSize(mpost$Beta)
ge.beta <- gelman.diag(mpost$Beta,multivariate=FALSE)$psrf ## this takes a very long time.

## make a script?:

## gelmanDiag_m_hurdleSpatial.r ##
####### script to calculate gelman psrf (start) ######
library(Hmsc)
load("/vol/data/fichtelgebirgeSoils/spatialAnalysis/m_hurdleSpatial_sampled.rda")
mpost = convertToCodaObject(m_hurdleSpatial, Beta = TRUE, Gamma = FALSE, V = FALSE, Sigma = FALSE, Rho = FALSE,
                                Eta = FALSE, Lambda = FALSE, Alpha = FALSE, Omega = FALSE, Psi = FALSE, Delta = FALSE)
ge.beta <- gelman.diag(mpost$Beta,multivariate=FALSE)$psrf ## this takes a very long time.
save(ge.beta, file='gelmanDiag_m_hurdleSpatial.rda')
####### script to calculate gelman psrf (end) ######

soil; conda activate r_env
nohup Rscript gelmanDiag_m_hurdleSpatial.r &> gelmanDiag_m_hurdleSpatial.log &

## back on interp

load("/vol/data/fichtelgebirgeSoils/spatialAnalysis/gelmanDiag_m_hurdleSpatial.rda")

hist(es.beta, main="effective sample size for beta")
## actually not too bad, a lot of species are over 1000,
## bimodal, a lot reached near 2000, 
## and then a lot of shitty ones, <400

hist(ge.beta, main="potential scale reduction factor for beta)") ## funny, these convergences look mostly okay.

## on desktop
getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/traces_m_hurdleSpatial.pdf"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere

## check for autocorrelation:

?autocorr.plot

class(mpost$Beta)

class(mpost$Beta[[1]])

autocorr.plot(mpost$Beta[[1]],lag.max=20) ## can't do it, too much memory needed

## a lot of the traces show shifting averages, and divergence
## in the chains. 
## it does indeed look like we need more sampling, but 
## how do we do this, do we exactly repeat the sampling, or increase 
## the thinning? 

## increasing the thinning will dramatically increase the sampling 
## time, and we don't have this time, really. 
## sampling for the posterior predictions will take as long or longer

## so what is the strategy here, going forward?

## it looks like we need to simplify the model, cut the species
## down to a couple hundred?

## rerun with fewer species and bigger thinning steps. Get it started today,
## then keep exploring the spatiotemporal model in the meantime.


## rerunning big spatial model:

denbiDirt

conda activate r_env

soil

R

library(Hmsc)
library(phyloseq)

rm(list=ls()) ## merge the spatial and env data:
xycoords = read.csv("sulariSpatial.csv", row.names=1)
envData <- read.csv("sulariEnv.csv", row.names="SampleID")
aa <- merge(x=envData, y=xycoords, by=0, )

row.names(aa) <- aa$Row.names
aa[1] <- NULL
aa <- aa[rownames(envData),]
envData <- aa; rm(aa)
varsOfInterest = c("PlotID","soil_respiration","Land_type","pH","N","C","Temperature","waterCont","xx","yy")
XData<-data.frame(envData[,varsOfInterest])
XData$Land_type <- as.factor(XData$Land_type)
## we can only use complete cases
ccases <- complete.cases(XData) ## 112 samples, we only lose 8 
XData = XData[ccases,] 
load("psCleanedUp.rda")
psCleanedUp = prune_samples(!(rownames(sample_data(psCleanedUp)) %in% c("C1.1","C1.2","C2.1","C2.2")), psCleanedUp)
psCleanedUp = prune_taxa( colSums(otu_table(psCleanedUp)) > 0, psCleanedUp)
bactRaw <- otu_table(psCleanedUp)@.Data
bactRaw = bactRaw[ccases,]  ## complete cases only
XData['BacteriaDepth'] <- log(rowSums(bactRaw))
bact.pa<-ifelse(bactRaw>0,1,0)
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat
##################### thresholds!! ###########
## larger, prev model had 1315 spp, x2 in hurdle model, threshold.prev=0.05, threshold.abu=0.002
## we'll do a much harsher subset for the moment. If sampling is cheap, soften it
## need to reduce to at least half, maybe more
threshold.prev = 5/112 ## present in at least 5 samples, out of 112 samples 
threshold.abu = 0.01 ## only OTU that reaches at least 1% abundance of one sample
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) 
cond2=apply(bact.rel,2,max)>=threshold.abu 
bactData <- bactRaw[, cond1 & cond2 ] 
sum(bactData > 0)
sum(bact.pa)
dim(bactData) ## 166 OTUs remain, out of 
dim(bact.pa) ## 3313 OTUs
################################################
Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Ypa = 1*(Y>0)
all(rownames(XData) == rownames(Y)) ## check check
sample.id = row.names(Y)
samplePlotid = XData$PlotID
studyDesign = data.frame("sample"=as.factor(sample.id), "site_spatial"=as.factor(samplePlotid))
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID 
plot_coords$PlotID <- NULL
varsOfmodel1 <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont")
XData1 <- XData[,c(varsOfmodel1)]
XFormula1 = ~ soil_respiration + Land_type + pH + N + C + Temperature
XData0 <- XData[,"BacteriaDepth",drop=FALSE]
XFormula0 = ~ BacteriaDepth
rL.sample = HmscRandomLevel(units = sample.id)
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.

## we want three models, 
## one that contains only the covariate of read depth
## one that is based on presence absence (probit) and all important env data
## and one that is based on read abundances and all important env data

## for the null abundance model with no environmental covariates, they use a 
## lognormal poisson...why?:
m_spat_noEnv = Hmsc(Y=Yabu,
         XData = XData0,  XFormula = XFormula0,
         distr={"lognormal poisson"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)},
         YScale=TRUE 
)
save(m_spat_noEnv, file="m_spat_noEnv_01percent.rda")
## for the PA model with all environmental covariates:
m_spat_PA = Hmsc(Y=Ypa,
         XData = XData1,  XFormula = XFormula1,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)},
         YScale=TRUE 
)
save(m_spat_PA, file="m_spat_PA_01percent.rda")
## for the abundance model with all environmental covariates:
m_spat_Abu = Hmsc(Y=Yabu,
         XData = XData1,  XFormula = XFormula1,
         distr={"normal"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)},
         YScale=TRUE 
)
save(m_spat_Abu, file="m_spat_Abu_01percent.rda")


################## sampleHurdleSpatial_01percent.r #######################
library(Hmsc)
print(paste("start time is", Sys.time()))
load("m_spat_noEnv_01percent.rda")
load("m_spat_PA_01percent.rda")
load("m_spat_Abu_01percent.rda")
thin = 20
samples = 1000
nChains = 4
nP = 4
print("starting null model")
m_spat_noEnv = sampleMcmc(m_spat_noEnv, 
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP
                     )
save(m_spat_noEnv, file="m_spat_noEnv_01percent_sampled.rda")
print("finished null model, start PA model")
m_spat_PA = sampleMcmc(m_spat_PA, 
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP
                     )
save(m_spat_PA, file="m_spat_PA_01percent.rda_sampled.rda")
print("finished PA model, start abundance model")
m_spat_Abu = sampleMcmc(m_spat_Abu, 
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP
                     )
save(m_spat_Abu, file="m_spat_Abu_01percent_sampled.rda")
print(paste("finish time is", Sys.time()))
######################################################

nohup Rscript sampleHurdleSpatial_01percent.r &> sampleHurdleSpatial_01percent.log &

## this is using a fraction of the RAM, each chain is ~1gig
## maybe the complexity scales geometrically with the number of species...

## let's see how long this needs to run...

## and back on the spatiotemp model while this runs

########### back after a month #####

## oh jeez, my brain hurts. What did I do?
## where were these results...

denbiDirt

conda activate r_env

soil

R

library(Hmsc)
library(corrplot)
library(RColorBrewer)
library(lobstr)
load("m_spat_noEnv_01percent_sampled.rda")
load("m_spat_PA_01percent.rda_sampled.rda")
load("m_spat_Abu_01percent_sampled.rda")

m_spat_noEnv

m_spat_PA  

m_spat_Abu 

## first step is to check how well mixed the samples 
## are of the posterior:

## for the "null" model:
npEnvCoda = convertToCodaObject(m_spat_noEnv)

str(m_spat_noEnv)

es.beta = effectiveSize(npEnvCoda$Beta)  ## mean around 500
hist(es.beta, main="effective sample size for beta", breaks=30)

es.beta

ge.beta = gelman.diag(npEnvCoda$Beta,multivariate=FALSE)$psrf

hist(ge.beta) ## mostly ~1, that's good 

pdf(file="traces_mixingSpatialNull.pdf")
plot(npEnvCoda$Beta)
dev.off()

## get it local ##
getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/traces_mixingSpatialNull.pdf"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git/"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere
################

## they look ~ok. chains didn't totally converge on a lot
## Definitely more sampling in order, as soon as we are more 
## confident in the models we want

## gamma, species niche. We didn't give priors for niches, not sure
## how this works in that case...
es.gamma = effectiveSize(npEnvCoda$Gamma) ## same, ~500
hist(es.beta, main="effective sample size for gamma", breaks=30)

es.gamma

ge.gamma = gelman.diag(npEnvCoda$Gamma,multivariate=FALSE)$psrf
ge.gamma ## only 2 values, the intercept and 
## ah, somehow bacterial depth is included here...not sure

## V is unexplained variation in species niches...confusing, but here it is: 
es.V = effectiveSize(npEnvCoda$V) # also around 500
hist(es.beta, main="effective sample size for V", breaks=30)
ge.V = gelman.diag(npEnvCoda$V,multivariate=FALSE)$psrf

## species interactions, with no real environmental data:

length(npEnvCoda)

str(npEnvCoda[[1]])

str(npEnvCoda[[2]])


npEnvCoda$Omega

#install.packages("lobstr")

## the omega part of the object is huge:
tree(npEnvCoda) 

## they subsample the omega
npEnvCoda$temp = npEnvCoda$Omega[[1]]
for(i in 1:length(npEnvCoda$temp)){
  npEnvCoda$temp[[i]] = npEnvCoda$temp[[i]][,1:1000]
}

tree(npEnvCoda$temp)

pdf(file="traces_mixingSpatialNull.pdf")
plot(npEnvCoda$temp)
dev.off() ## get local etc

es.omega1 = effectiveSize(npEnvCoda$temp)
ge.omega1 = gelman.diag(npEnvCoda$temp,multivariate=FALSE)$psrf
## actually, that looks a lot better than our Beta
hist(es.omega1) 
hist(ge.omega1)

## so mixing is okay for exploration, but should start a new model 
## with more sampling, hopefully this afternoon

## to check model fit:
preds = computePredictedValues(m_spat_noEnv)
MF = evaluateModelFit(hM=m_spat_noEnv, predY=preds)

?evaluateModelFit

postBeta = getPostEstimate(m_spat_noEnv, parName="Beta")

str(MF)

hist(MF$RMSE)
hist(MF$C.RMSE)
## etc, explanatory power of model pretty good. 

partition = createPartition(m_spat_noEnv, nfolds = 2, column = "sample")

preds = computePredictedValues(m_spat_noEnv, partition=partition, nParallel = 4)
## started 14:26

MF = evaluateModelFit(hM=m, predY=preds)

### actually, we need a script for that, I think:

########### checkModel_spatNoEnv.r #################
library(Hmsc)
load("m_spat_noEnv_01percent_sampled.rda")
partition = createPartition(m_spat_noEnv, nfolds = 2, column = "sample")
preds = computePredictedValues(m_spat_noEnv, partition=partition, nParallel = 4)
MF = evaluateModelFit(hM=########### update ################, predY=preds)
save(preds, file="predsSpat_noEnv.rda")
save(MF, file="modelFitSpat_noEnv.rda")
####################################################

nohup Rscript checkModel_spatNoEnv.r &> checkModel_spatNoEnv.log & ## started 15:07, jul1

## that is going to run for a while, it seems. 

## let's try looking at the species associations

?computeAssociations

OmegaCor = computeAssociations(m_spat_noEnv) ## took ~3 min

?computeAssociations

OmegaCor[[1]]$mean[1:3,1:3]

str(OmegaCor[[2]])

?corrMatOrder

plotOrder <- corrMatOrder(OmegaCor[[1]]$mean,order="AOE")

supportLevel = 0.75
toPlot = ((OmegaCor[[1]]$support>supportLevel) + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean

corrplot(toPlot[plotOrder,plotOrder], method = "color", type="lower",diag=FALSE,
         col=colorRampPalette(c("blue","white","red"))(200),tl.cex=0.4,title=paste("random effect level:",m_spat_noEnv$rLNames[1]), mar=c(0,0,1,0))
         col=colorRampPalette(c("blue","white","red"))(200),tl.cex=0.4, mar=c(0,0,1,0))

m_spat_noEnv[1]

m_spat_noEnv$rLNames

## there are strong species associations, 
## to be expected, considering this does not include environmental associations

## how can we see how many latent variables were fit?

## and if we can do an ordination with them:

str(m_spat_noEnv)

biPlot(m_spat_noEnv, 
       etaPost = getPostEstimate(m_spat_noEnv, "Eta"), 
       lambdaPost = getPostEstimate(m_spat_noEnv, "Lambda"),
       spNames=NULL)

?biPlot

?getPostEstimate

aa[[1]][1:5,]

## we should be able to do network diagrams and analyses with this.

## let's run the full environmental spatial models through the above code, first
## because we are only really interested in the omega matrix after accounting 
## for the beta. 
## this

## presence/absence ##
m_spat_PA  

m_spat_PA_Coda = convertToCodaObject(m_spat_PA)

PA_es.beta = effectiveSize(m_spat_PA_Coda$Beta)  ## much better than the null model, mean of 3300 or so
hist(PA_es.beta, main="effective sample size for beta", breaks=30)
mean(PA_es.beta)

ge.beta = gelman.diag(m_spat_PA_Coda$Beta,multivariate=FALSE)$psrf
hist(ge.beta) ## mostly ~1, that's good 

pdf(file="traces_mixingSpatialPA.pdf")
plot(m_spat_PA_Coda$Beta)
dev.off()
## on local:
getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/traces_mixingSpatialPA.pdf"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git/"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere
## looks fine. Maybe even better than the null model

## gamma, the effect of traits on niches, or something like that
es.gamma = effectiveSize(m_spat_PA_Coda$Gamma) ## 
hist(es.gamma, main="effective sample size for gamma", breaks=30)

ge.gamma = gelman.diag(m_spat_PA_Coda$Gamma,multivariate=FALSE)$psrf

ge.gamma ## there is one trait value here, even though I didn't set any

## V is unexplained variation in species niches...confusing, but here it is: 
es.V = effectiveSize(m_spat_PA_Coda$V) # also around 500
hist(es.V, main="effective sample size for V", breaks=30)
ge.V = gelman.diag(m_spat_PA_Coda$V,multivariate=FALSE)$psrf
## looks fine

m_spat_PA_Coda$temp = m_spat_PA_Coda$Omega[[1]]
for(i in 1:length(m_spat_PA_Coda$temp)){
  m_spat_PA_Coda$temp[[i]] = m_spat_PA_Coda$temp[[i]][,1:1000]
}

pdf(file="traces_mixingSpatialPA_omega.pdf")
plot(m_spat_PA_Coda$temp)
dev.off() ## get local etc

## on local:
getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/traces_mixingSpatialPA_omega.pdf"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git/"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere

## a lot of them look weird, but oh well. 
## 
es.omega1 = effectiveSize(m_spat_PA_Coda$temp)
ge.omega1 = gelman.diag(m_spat_PA_Coda$temp,multivariate=FALSE)$psrf

## actually, that looks a lot better than our Beta
hist(es.omega1) 

hist(ge.omega1[,1])


## "null" model, for comparison
preds_spat_noEnv = computePredictedValues(m_spat_noEnv)
MF_spat_noEnv = evaluateModelFit(hM=m_spat_noEnv, predY=preds_spat_noEnv)

str(MF_spat_noEnv)

## PA_full model
preds_spat_PA = computePredictedValues(m_spat_PA)
MF_spat_PA = evaluateModelFit(hM=m_spat_PA, preds_spat_PA)

par(mfrow = c(1,2))
hist(MF_spat_noEnv$RMSE)
hist(MF_spat_PA$RMSE)

MF_spat_PA$RMSE ## not sure if this is meaningful with PA

## funny, we get less explanatory power in this PA with environmental covariates
## maybe this is the P/A, we'll see how the abundance model worked.

supportLevel = 0.75
## null, spatial
OmegaCor_spat_noEnv = computeAssociations(m_spat_noEnv) 
plotOrder_spat_noEnv <- corrMatOrder(OmegaCor_spat_noEnv[[1]]$mean,order="AOE")
toPlot_spat_noEnv = ((OmegaCor_spat_noEnv[[1]]$support>supportLevel) + (OmegaCor_spat_noEnv[[1]]$support<(1-supportLevel))>0)*OmegaCor_spat_noEnv[[1]]$mean
## environmental, spatial
OmegaCor_spat_PA = computeAssociations(m_spat_PA) 
plotOrder_spat_PA <- corrMatOrder(OmegaCor_spat_PA[[1]]$mean,order="AOE")
toPlot_spat_PA = ((OmegaCor_spat_PA[[1]]$support>supportLevel) + (OmegaCor_spat_PA[[1]]$support<(1-supportLevel))>0)*OmegaCor_spat_PA[[1]]$mean


pdf('noEnv_vs_Env_omega.pdf', width=20, height=20)
par(mfrow = c(1,2))
corrplot(toPlot_spat_noEnv[plotOrder_spat_noEnv,plotOrder_spat_noEnv], method = "color", type="lower",diag=FALSE,
         col=colorRampPalette(c("blue","white","red"))(200),tl.cex=0.4,title=paste("random effect level:",m_spat_noEnv$rLNames[1]), mar=c(0,0,1,0))
## plot using same order:
corrplot(toPlot_spat_PA[plotOrder_spat_noEnv,plotOrder_spat_noEnv], method = "color", type="lower",diag=FALSE,
         col=colorRampPalette(c("blue","white","red"))(200),tl.cex=0.4,title=paste("random effect level:",m_spat_PA$rLNames[1]), mar=c(0,0,1,0))
## plot using different, optimized order:
#corrplot(toPlot_spat_PA[plotOrder_spat_PA,plotOrder_spat_PA], method = "color", type="lower",diag=FALSE,
#         col=colorRampPalette(c("blue","white","red"))(200),tl.cex=0.4,title=paste("random effect level:",m_spat_PA$rLNames[1]), mar=c(0,0,1,0))
dev.off()

getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/noEnv_vs_Env_omega.pdf"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git/"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere

par(mfrow = c(1,1))

## spatial environmental abundance ##

m_spat_Abu 

m_spat_Abu_Coda = convertToCodaObject(m_spat_Abu)

abu_es.beta = effectiveSize(m_spat_Abu_Coda$Beta)  

hist(abu_es.beta, main="effective sample size for beta", breaks=30) ## looks even better than the PA

mean(abu_es.beta) ##~3400

ge.beta = gelman.diag(m_spat_Abu_Coda$Beta,multivariate=FALSE)$psrf

hist(ge.beta[,1]) ## mostly ~1, that's good 

pdf(file="traces_mixingSpatialAbu_beta.pdf")
plot(m_spat_Abu_Coda$Beta)
dev.off()

## on local:
getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/traces_mixingSpatialAbu_beta.pdf"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git/"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere

## looks fine.

## omega:

m_spat_Abu_Coda$temp = m_spat_Abu_Coda$Omega[[1]]
for(i in 1:length(m_spat_Abu_Coda$temp)){
  m_spat_Abu_Coda$temp[[i]] = m_spat_Abu_Coda$temp[[i]][,1:1000]
}
es.omega1 = effectiveSize(m_spat_Abu_Coda$temp)
ge.omega1 = gelman.diag(m_spat_Abu_Coda$temp,multivariate=FALSE)$psrf


hist(es.omega1, breaks=20)

## looks okay. There are a few below 1000, but not many and not much fewer
hist(ge.omega1[,1]) ## fine. This index doesn't seem very sensitive.

pdf(file="traces_mixingSpatialAbu_omega.pdf")
plot(m_spat_Abu_Coda$temp)
dev.off() ## get local etc

## on local:
getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/traces_mixingSpatialAbu_omega.pdf"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git/"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere

##  
par(mfrow=c(1,3))

## "null" model, for comparison
preds_spat_noEnv = computePredictedValues(m_spat_noEnv)
MF_spat_noEnv = evaluateModelFit(hM=m_spat_noEnv, predY=preds_spat_noEnv)
## PA_full model for comparison
preds_spat_PA = computePredictedValues(m_spat_PA)
MF_spat_PA = evaluateModelFit(hM=m_spat_PA, preds_spat_PA)
## Abu_full model
preds_spat_abu = computePredictedValues(m_spat_Abu)
MF_spat_abu = evaluateModelFit(hM=m_spat_Abu, preds_spat_abu)

par(mfrow = c(1,3))
hist(MF_spat_noEnv$RMSE)
hist(MF_spat_PA$RMSE)
hist(MF_spat_abu$RMSE)

## much more powerful than the P/A model, but 
## again, it appears the model without covariates is the most powerful??
## confusing

## rerun, parallel, more species
## extra model with no spatial random effect, to gauge importance 
## plus 
## but wait, figure out the carbon emission data with Betti
## for visualization, take a look at this site: 
https://kateto.net/network-visualization

supportLevel = 0.75
## null, spatial
OmegaCor_spat_noEnv = computeAssociations(m_spat_noEnv) 
plotOrder_spat_noEnv <- corrMatOrder(OmegaCor_spat_noEnv[[1]]$mean,order="AOE")
toPlot_spat_noEnv = ((OmegaCor_spat_noEnv[[1]]$support>supportLevel) + (OmegaCor_spat_noEnv[[1]]$support<(1-supportLevel))>0)*OmegaCor_spat_noEnv[[1]]$mean
## environmental, spatial, PA
OmegaCor_spat_PA = computeAssociations(m_spat_PA) 
plotOrder_spat_PA <- corrMatOrder(OmegaCor_spat_PA[[1]]$mean,order="AOE")
toPlot_spat_PA = ((OmegaCor_spat_PA[[1]]$support>supportLevel) + (OmegaCor_spat_PA[[1]]$support<(1-supportLevel))>0)*OmegaCor_spat_PA[[1]]$mean

## environmental, spatial, abu
OmegaCor_spat_abu = computeAssociations(m_spat_Abu) 
plotOrder_spat_abu <- corrMatOrder(OmegaCor_spat_abu[[1]]$mean,order="AOE")
toPlot_spat_abu = ((OmegaCor_spat_abu[[1]]$support>supportLevel) + (OmegaCor_spat_abu[[1]]$support<(1-supportLevel))>0)*OmegaCor_spat_abu[[1]]$mean

par(mfrow = c(1,3))
corrplot(toPlot_spat_noEnv[plotOrder_spat_noEnv,plotOrder_spat_noEnv], method = "color", type="lower",diag=FALSE,
         col=colorRampPalette(c("blue","white","red"))(200),tl.cex=0.4,title=paste("random effect level:",m_spat_noEnv$rLNames[1]), mar=c(0,0,1,0))
corrplot(toPlot_spat_PA[plotOrder_spat_noEnv,plotOrder_spat_noEnv], method = "color", type="lower",diag=FALSE,
         col=colorRampPalette(c("blue","white","red"))(200),tl.cex=0.4,title=paste("random effect level:",m_spat_PA$rLNames[1]), mar=c(0,0,1,0))
corrplot(toPlot_spat_abu[plotOrder_spat_noEnv,plotOrder_spat_noEnv], method = "color", type="lower",diag=FALSE,
         col=colorRampPalette(c("blue","white","red"))(200),tl.cex=0.4,title=paste("random effect level:",m_spat_PA$rLNames[1]), mar=c(0,0,1,0))

## why do we have such a large network with the full models? 

## right now I cannot tell the effect of our choice of distributions
## (log normal for "null" model, probit for PA, normal for abu)
## from the effect of including the covariates
## we need another "control" model that is not a "hurdle" model,
## i.e. log normal in a model with all environmental covariates 

## next steps:

## learn what is possible concerning CO2 flux from Betty
## if it is available, add some index of CO2 flux into 
## our environmental covariates
## rerun all three models, plus an additional log-normal distribution
## model on the abundance-including-zeroes matrix 
## save these models and keep moving through the analysis pipeline
## to the point of finding networks with modules that are associated
## with high emissions, high SOC

## which reminds me, where is the SOC data? I guess this is 
## just the carbon data from soil. I don't think there is a 
## large reservoir of non-organic carbon in these soils

## talked to Betty, and the carbon emission data 

conda activate spatialDirt

## side note, can we get a nice panda viewer into our 
## environment?

#conda install conda-forge::pandasgui ## nope, did this break our install again?
## or?
#conda install conda-forge::tabloo

soil 

python3

import pandas as pd
import os
import tabloo
import numpy as np

## the carbon data is here:
CO2efflux = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/SoilDataBetty/COefflux.csv")

## this should probably be added to our environmental matrix:
envData = pd.read_csv('sulariEnv.csv', index_col="SampleID")
envData['Date'] = pd.to_datetime(envData['Date'], yearfirst=True, errors='coerce')

tabloo.show(envData)

## there were some weird sites...
envData.loc['S89']
envData.loc['S78']

## might be important here, what are our repeated plots?
repeats = pd.read_csv("repeatedPlotsEnv.csv")
repeats.sort_values(by="PlotID")

## remove the weird samples we found yesterday? = S89
#envData.drop('S89',inplace=True)
#envData.drop('S78',inplace=True)
## think about that ^, leave them in now

envData.head()

CO2efflux.head()

envData.shape
CO2efflux.shape ## we have lots of extra data - what is this?

envData['PlotID'].isin(CO2efflux['plotID'])

envData['PlotID'].isin(CO2efflux['plotID']).sum() ## 115 rows of our envData have CO2 data

## what is missing?:
filt = ~(envData['PlotID'].isin(CO2efflux['plotID']))
envData.loc[filt]
## P0216 P0102 P0116 P0165 P0157
## P0216 P0116 P0165 are also lacking moisture and temperature data... 

## are any of these our repeated sites, as in using for spatiotemporal study?
plotsmissingCO2 = envData.loc[filt].PlotID
envData[envData['PlotID'].isin(plotsmissingCO2)]
plotsmissingCO2.isin( repeats.PlotID) ## nope, that is good.


CO2efflux['plotID'].isin(envData['PlotID']).head() 

CO2efflux['plotID'].isin(envData['PlotID']).sum() ## same, 115 rows in the CO2 are also from sites also in our envdata

## what co2 data are supposedly not in our environmental data?:
filt2 = ~CO2efflux['plotID'].isin(envData['PlotID']) 

tabloo.show(CO2efflux[filt2]) ## most of these are "PS" plots, not in our envData

## but there is PS0180:
"P0180" in CO2efflux['plotID'].values

"P0180" in envData['PlotID'].values

## sulari says this plot has always been missing, no DNA extracted etc.

tabloo.show(envData)

## not sure, but move on with the rest, need to get a CO2 emission column in 
## our environmental dataframe

## whoops, deleted rownames, that's a pain:
git checkout 1a25869d56537f2 -- sulariEnv.csv

## to get the CO2 emissions, we need to match date and plot ID

envData = pd.read_csv('sulariEnv.csv', index_col="SampleID")
envData['Date'] = pd.to_datetime(envData['Date'], yearfirst=True, errors='coerce')

flow = []
soil_T = []
for i,j in envData.iterrows():
  a = j['PlotID']
  b = j['Date'].strftime("%Y-%m-%d")
  c = CO2efflux.query(f'plotID == "{a}" & date == "{b}"')["flow"]
  if c.empty: c = np.nan
  else: c = c.iloc[0]
  d = CO2efflux.query(f'plotID == "{a}" & date == "{b}"')["soil_T"]
  if d.empty: d = np.nan
  else: d = d.iloc[0]
  flow.append(c)
  soil_T.append(d)

envData['co2flow'] = flow
envData['soil_T'] = soil_T

## sanity check:
envData.loc['S1']
CO2efflux[CO2efflux['plotID'] == 'P0104']

envData.loc['S120']
CO2efflux[CO2efflux['plotID'] == 'P0213']

tabloo.show(envData)

## looks good. write this out as our new env data:

envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv')

envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv')

## now, we can we include this in out new models? 


## back to R and hmsc...

## to review we want several models:

## a "null" without spatial effects or any environmental covariates, log normal residuals
## a spatial model without environmental covariates, log normal residuals
## a non-spatial model with environmental covariates, log normal residuals
## two full spatial model with environmental covariates:
  ## 1. hurdle model ## which is actually 2 models
  ## 2. log-normal, for comparison with the "control models"

## so five (4) models. 

## can we set these up before we leave today? Should be able to reuse the above
## code: 


denbiDirt

conda activate r_env

soil

R

library(Hmsc)
library(phyloseq)

rm(list=ls()) ## merge the spatial and env data:

xycoords = read.csv("sulariSpatial.csv", row.names=1)
envData <- read.csv("sulariEnv.csv", row.names="SampleID")
aa <- merge(x=envData, y=xycoords, by=0, )
row.names(aa) <- aa$Row.names
aa[1] <- NULL
aa <- aa[rownames(envData),]
envData <- aa; rm(aa)
## now with co2 flux:
varsOfInterest = c("PlotID","soil_respiration","co2flow","Land_type","pH","N","C","Temperature","waterCont","xx","yy")
## side note: why do we have two types of temperature data?:
#plot(envData$soil_T, envData$Temperature)
XData<-data.frame(envData[,varsOfInterest])
XData$Land_type <- as.factor(XData$Land_type)
## we can only use complete cases
ccases <- complete.cases(XData) ## 109 samples, we lose 11
XData = XData[ccases,] 
load("psCleanedUp.rda")
psCleanedUp = prune_samples(!(rownames(sample_data(psCleanedUp)) %in% c("C1.1","C1.2","C2.1","C2.2")), psCleanedUp)
psCleanedUp = prune_taxa( colSums(otu_table(psCleanedUp)) > 0, psCleanedUp)
bactRaw <- otu_table(psCleanedUp)@.Data
bactRaw = bactRaw[ccases,]  ## complete cases only
XData['BacteriaDepth'] <- log(rowSums(bactRaw))
bact.pa<-ifelse(bactRaw>0,1,0)
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat

##################### thresholds!! ###########
## larger, prev model had 1315 spp, x2 in hurdle model, threshold.prev=0.05, threshold.abu=0.002
## we'll do a much harsher subset for the moment. If sampling is cheap, soften it
## need to reduce to at least half, maybe more
threshold.prev = 5/112 ## present in at least 5 samples, out of 112 samples 
threshold.abu = 0.01 ## only OTU that reaches at least 1% abundance of one sample
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) 
cond2=apply(bact.rel,2,max)>=threshold.abu 
bactData <- bactRaw[, cond1 & cond2 ] 
sum(bactData > 0)
sum(bact.pa)
dim(bactData) ## 166 OTUs remain, out of 
dim(bact.pa) ## 3313 OTUs
################################################
Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
Yabu <- Y
Yabu[Y==0] <- NA ## remove zeroes from the model
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Ypa = 1*(Y>0)
all(rownames(XData) == rownames(Y)) ## check check
sample.id = row.names(Y)
samplePlotid = XData$PlotID
studyDesign = data.frame("sample"=as.factor(sample.id), "site_spatial"=as.factor(samplePlotid))
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID 
plot_coords$PlotID <- NULL
varsOfmodel1 <- c("soil_respiration","co2flow","Land_type","pH","N","C","Temperature","waterCont")
XData1 <- XData[,c(varsOfmodel1)]
XFormula1 = ~ soil_respiration + co2flow + Land_type + pH + N + C + Temperature
XData0 <- XData[,"BacteriaDepth",drop=FALSE]
XFormula0 = ~ BacteriaDepth
rL.sample = HmscRandomLevel(units = sample.id)
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) #try 3. Not sure how to optimize this. But there may multiple spatial scales.


## 
## for the null abundance model with no environmental covariates, they use a 
## lognormal poisson

## 1. "null" model without spatial effects or any environmental covariates, log normal residuals
m_ReadAbundancesOnly = Hmsc(Y=Y,
         XData = XData0,  XFormula = XFormula0,
         distr={"lognormal poisson"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample)},
         YScale=TRUE 
)
save(m_ReadAbundancesOnly, file="m_ReadAbundancesOnly.rda")

## 2. a spatial model without environmental covariates, log normal residuals
m_ReadAbundancesSpatial = Hmsc(Y=Y,
         XData = XData0,  XFormula = XFormula0,
         distr={"lognormal poisson"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)},
         YScale=TRUE 
)
save(m_ReadAbundancesSpatial, file="m_ReadAbundancesSpatial.rda")

## 3. a non-spatial model with environmental covariates, log normal residuals
m_environmentalOnly = Hmsc(Y=Y,
         XData = XData1,  XFormula = XFormula1,
         distr={"lognormal poisson"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample)},
         YScale=TRUE 
)
save(m_environmentalOnly, file="m_environmentalOnly.rda")

## 4. a full spatial model with zeros and environmental covariates, log normal residuals
m_fullModel_logNorm = Hmsc(Y=Y,
         XData = XData1,  XFormula = XFormula1,
         distr={"lognormal poisson"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)},
         YScale=TRUE 
)
save(m_fullModel_logNorm, file="m_fullModel_logNorm.rda")


## 5. a hurdle model, full spatial model with environmental covariates which is actually 2 models:
  ## 5.1. hurdle probit/PA: 

m_hurdle_PA = Hmsc(Y=Ypa,
         XData = XData1,  XFormula = XFormula1,
         distr={"probit"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)},
         YScale=TRUE 
)
save(m_hurdle_PA, file="m_hurdle_PA.rda")

## 5.2. hurdle abundance 
m_hurdle_abu = Hmsc(Y=Yabu,
         XData = XData1,  XFormula = XFormula1,
         distr={"normal"} ,
         studyDesign=studyDesign, 
         ranLevels={list("sample"=rL.sample, "site_spatial" = rL.site_spatial)},
         YScale=TRUE 
)
save(m_hurdle_abu, file="m_hurdle_abu.rda")

## tomorrow try sampling these 
## we need to run them together, as each individually 
## uses little memory.

## which means a script for each: 

## 1
#### m_ReadAbundancesOnly_sampling.r ####

library(Hmsc)
print("starting model:")
print("m_ReadAbundancesOnly")
print(paste("start time is", Sys.time()))
load("m_ReadAbundancesOnly.rda")
thin = 20
samples = 1000
nChains = 4
nP = 2
#m_ReadAbundancesOnly = sampleMcmc(m_ReadAbundancesOnly, 
m_ReadAbundancesOnly2 = sampleMcmc(m_ReadAbundancesOnly, 
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP
                     )
#save(m_ReadAbundancesOnly, file="m_ReadAbundancesOnly_sampled.rda")
save(m_ReadAbundancesOnly2, file="m_ReadAbundancesOnly_sampled2.rda")
print(paste("finish time is", Sys.time()))

##############################################################################

nohup Rscript m_ReadAbundancesOnly_sampling.r &>> m_ReadAbundancesOnly_sampling.log & 

## 15517

## 2
#### m_ReadAbundancesSpatial_sampling.r ####

library(Hmsc)
print("starting model:")
print("m_ReadAbundancesSpatial")
print(paste("start time is", Sys.time()))
load("m_ReadAbundancesSpatial.rda")
thin = 20
samples = 1000
nChains = 4
nP = 2
#m_ReadAbundancesSpatial = sampleMcmc(m_ReadAbundancesSpatial, 
m_ReadAbundancesSpatial2 = sampleMcmc(m_ReadAbundancesSpatial, 
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP
                     )
#save(m_ReadAbundancesSpatial, file="m_ReadAbundancesSpatial_sampled.rda")
save(m_ReadAbundancesSpatial2, file="m_ReadAbundancesSpatial_sampled2.rda")
print(paste("finish time is", Sys.time()))

#################################################

conda activate r_env

nohup Rscript m_ReadAbundancesSpatial_sampling.r &> m_ReadAbundancesSpatial_sampling.log &




m_environmentalOnly.rda

## 3
#### m_environmentalOnly_sampling.r ####

library(Hmsc)
print("starting model:")
print("m_environmentalOnly")
print(paste("start time is", Sys.time()))
load("m_environmentalOnly.rda")
thin = 20
samples = 1000
nChains = 4
nP = 2
#m_environmentalOnly = sampleMcmc(m_environmentalOnly, 
m_environmentalOnly2 = sampleMcmc(m_environmentalOnly, 
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP
                     )
#save(m_environmentalOnly, file="m_environmentalOnly_sampled.rda")
save(m_environmentalOnly2, file="m_environmentalOnly_sampled2.rda")
print(paste("finish time is", Sys.time()))

#################################################

nohup Rscript m_environmentalOnly_sampling.r &> m_environmentalOnly_sampling.log &

## 15695

## m_fullModel_logNorm

## 4 
#### m_fullModel_logNorm_sampling.r ####

library(Hmsc)
print("starting model:")
print("m_fullModel_logNorm")
print(paste("start time is", Sys.time()))
load("m_fullModel_logNorm.rda")
thin = 20
samples = 1000
nChains = 8
#nChains = 4
nP = 2
m_fullModel_logNorm = sampleMcmc(m_fullModel_logNorm,
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP
                     )
save(m_fullModel_logNorm, file="m_fullModel_logNorm_sampled.rda")
print(paste("finish time is", Sys.time()))

#################################################

nohup Rscript m_fullModel_logNorm_sampling.r &> m_fullModel_logNorm_sampling.log &

## 

## 5.1 m_hurdle_PA
#### m_hurdle_PA_sampling.r ####

library(Hmsc)
print("starting model:")
print("m_hurdle_PA")
print(paste("start time is", Sys.time()))
load("m_hurdle_PA.rda")
thin = 20
samples = 1000
nChains = 4
nP = 2
#m_hurdle_PA = sampleMcmc(m_hurdle_PA,
m_hurdle_PA2 = sampleMcmc(m_hurdle_PA,
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP
                     )
save(m_hurdle_PA2, file="m_hurdle_PA_sampled2.rda")
#save(m_hurdle_PA, file="m_hurdle_PA_sampled.rda")
print(paste("finish time is", Sys.time()))

#################################################

#nohup Rscript m_hurdle_PA_sampling.r &> m_hurdle_PA_sampling.log &
## 15900

conda activate r_env

nohup Rscript m_hurdle_PA_sampling.r &> m_hurdle_PA_sampling.log &

## 5.2 m_hurdle_abu
#### m_hurdle_abu_sampling.r ####

library(Hmsc)
print("starting model:")
print("m_hurdle_aba_2ndSample")
#print("m_hurdle_aba")
print(paste("start time is", Sys.time()))
load("m_hurdle_abu.rda")
thin = 20
samples = 1000
nChains = 4
nP = 2
#m_hurdle_abu = sampleMcmc(m_hurdle_abu,
m_hurdle_abu2 = sampleMcmc(m_hurdle_abu,
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP
                     )
#save(m_hurdle_abu2, file="m_hurdle_abu_sampled.rda")
save(m_hurdle_abu2, file="m_hurdle_abu_sampled2.rda")
print(paste("finish time is", Sys.time()))

#################################################

nohup Rscript m_hurdle_abu_sampling.r &> m_hurdle_abu_sampling2.log &


## some of these models are finishing up. 

## it takes even longer to predictive R2 out of these,
## maybe start them? 

## also, we may want deeper sampling...ugh...

## so much work and computing.

## start by checking the mixing of the models that have finished:

## on denbi:

soil; conda activate r_env; R

library(Hmsc)

load('m_ReadAbundancesOnly_sampled.rda')

m_ReadAbundancesOnly

m_ReadAbundancesOnly_coda = convertToCodaObject(m_ReadAbundancesOnly)

m_ReadAbundancesOnly.beta = effectiveSize(m_ReadAbundancesOnly_coda$Beta)  

hist(m_ReadAbundancesOnly.beta, main="effective sample size for beta", breaks=30) 
## not horrible, but mean is somewhere around 500
## might be good to run more sampling, but wait till the other models come in

## except this is taking forever, the full hurdle models are still going. 

## might as well keep  working on the other models, get 
## deeper sampling. let's rerun the above sampling, code edited accordingly.

## models 1-4 are resampling. Wait on 5.1 and 5.2, maybe not necessary?

## some models are coming in. Let's back them up locally:

## on local:
getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/m_ReadAbundancesOnly_sampled2.rda"
putItHere="/media/vol/fichtelberg/hmscBackups/spatial/"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere

## also the older models:

ls *sampled.rda

tar -cvzf sampledSpatiolFirstRound.tgz *sampled.rda
getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/sampledSpatiolFirstRound.tgz"
putItHere="/media/vol/fichtelberg/hmscBackups/spatial/"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere

getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/sampledSpatiolFirstRound.tgz"
cd "/media/vol/fichtelberg/hmscBackups/spatial/"

tar -xzvf sampledSpatiolFirstRound.tgz

## okay backed up

## plan: get cooccurrence matrices, add column with respiration and CO2 emission rates
## build coccurence network, then try the visualization pipelines in NetCoMi

## not sure exactly how to proceed with the hurdle model here. Most analyses 
## just contrast the results of the two submodels, and don't really synthesize 
## them into a single result.

## we can play with the models we have, while we wait for the full sampling to come in.
## models are here, on denbi:



## a couple things we want to do  

#### m_ReadAbundancesOnly_sampling.r ####
#### m_ReadAbundancesSpatial_sampling.r ####
#### m_environmentalOnly_sampling.r ####
#### m_fullModel_logNorm_sampling.r ####
#### m_hurdle_PA_sampling.r ####
#### m_hurdle_abu_sampling.r ####

## we have the following already sampled:
m_ReadAbundancesOnly_sampled.rda
m_fullModel_logNorm_sampled.rda
m_ReadAbundancesSpatial_sampled.rda
m_environmentalOnly_sampled.rda

## we have deep (2 x 4 chains) of 
m_ReadAbundancesOnly_sampled.rda
m_ReadAbundancesOnly_sampled2.rda
m_environmentalOnly_sampled.rda
m_environmentalOnly_sampled2.rda
m_fullModel_logNorm_sampled.rda
m_fullModel_logNorm_sampled2.rda

## this leaves:
m_ReadAbundancesSpatial_sampled.rda
## without a secondary sampling 

ps au | grep m_ReadAbundancesSpatial_sampling.r
## doesn't seem to be anything running... weird. 
## rerun it above


## we have no products from:
#### m_hurdle_PA_sampling.r ####
#### m_hurdle_abu_sampling.r ####

## these are still running, right?
## the record of these were lost in a synch

## including unsampled models themselves, hope these are the right versions:
git log --all --full-history -- "m_hurdle_PA.rda"
git checkout cfe086f8e18bf60b22af1919c2761c584b8cca92~1 m_hurdle_PA.rda
git log --all --full-history -- "m_hurdle_abu.rda"
git checkout cfe086f8e18bf60b22af1919c2761c584b8cca92~1 m_hurdle_abu.rda

git log --all --full-history -- "m_hurdle_PA_sampling.r"
git log --all --full-history -- "m_hurdle_PA_sampling.log"
git log --all --full-history -- "m_hurdle_abu_sampling.r"
git log --all --full-history -- "m_hurdle_abu_sampling.r"

## there they are
git checkout  8572b393b533a4bb1cab7e4d3d45a3e370e4316a~1 m_hurdle_PA_sampling.r
git checkout  8572b393b533a4bb1cab7e4d3d45a3e370e4316a~1 m_hurdle_PA_sampling.log
git checkout 8572b393b533a4bb1cab7e4d3d45a3e370e4316a~1 m_hurdle_abu_sampling.r
git checkout 8572b393b533a4bb1cab7e4d3d45a3e370e4316a~1 m_hurdle_abu_sampling.log

ps ax | grep "m_hurdle_abu_sampling.r" ## yep, still running

ps ax | grep "m_hurdle_PA_sampling.r" ## yep, still running

## okay, so we keep those running, and as soon as other models come in, start the 
## deeper sampling on the hurdles

## for the moment, we can play with what we have
## on denbi:

conda activate r_env; soil; R

library(Hmsc)

## let's start with the test of the 
## effects of spatial effects only. 

load("m_ReadAbundancesOnly_sampled.rda")
load("m_ReadAbundancesSpatial_sampled.rda")
#load("m_fullModel_logNorm_sampled.rda")
#load("m_environmentalOnly_sampled.rda")

## so, can we check ordinations to see what 
## differences, if any, are changed by 
## including spatial effects, etc.

## p182-3 the book, they cover ordination using the latent variables

## in the book, they do it this way:

for (j in 1:2){
  m = models[[2]][[j]]
  biPlot(m, etaPost = getPostEstimate(m, “Eta”),
  lambdaPost = getPostEstimate(m, “Lambda”), colVar = 2)
}

m_abu = m_ReadAbundancesOnly

aa = getPostEstimate(m_abu, "Eta")  ## site scores
bb = getPostEstimate(m_abu, "Lambda") ## species scores

str(aa)
str(bb)

biPlot(m_abu, etaPost = aa, lambdaPost = bb) ## works, but what the heck are we looking at? Too many species. 
biPlot(m_abu, etaPost = aa, lambdaPost = bb, xlim=c(0.06,0.065), ylim=c(0.07,0.08))

## we should be able to color this by read abundance:
biPlot(m_abu, etaPost = aa, lambdaPost = bb, xlim=c(-1,0.5), ylim=c(-0.05,0.5), )
biPlot(m_abu, etaPost = aa, lambdaPost = bb, factors = c(1,2), "BacteriaDepth")

## can we extract these points ourselves? first two latent variables are presumably most important:
dev.new()
plot(t(bb$mean[1:2,]))
## promising, but how do we know OTU names? 
## ah, there they are:
text(bb$mean[1,], bb$mean[2,], m_abu$spNames, pos=1, cex=1)

## looks right. Not scaled, but otherwise the same ordination as 
## produced by their biplot function

## let's see if plots group by read abundance: 
col = m_abu$XData["BacteriaDepth"]
col = round((col - min(col))/diff(range(col)) * (length(colors) - 1) + 1)
colors = colorRampPalette(c("blue", "white", "red"))
colors <- colors(100)
cols = colors[col$BacteriaDepth]

## we can try different combinations of LVs:
plot(aa$mean[,1:2], col=cols)

dev.new()
plot(aa$mean[,1:3], col=cols)

dev.new()
plot(aa$mean[,2:3], col=cols)

## I don't see a pattern. mostly noise. I wonder why the ~high r2 on the spatiotemporal 
## on this model of read abundances only...
## there are two large groupings as far as sites along the first LV, 
## worth checking this out later, let's see if this explained by our 
## evironmental data when we introduce it

## let's apply this to our spTemp data and update the notebook with it. 

## another backup to do:

m_hurdle_abu.rda

getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/m_hurdle_abu.rda"
putItHere="/media/vol/fichtelberg/hmscBackups/spatial/"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere

## let's catch up the spatial-only model with the sptemp model

## let's use the logNormal poisson-process of the full spatial model
## and adapt the scripts for the hurdle model later

R

library(Hmsc)

load('m_fullModel_logNorm.rda')

load('m_fullModel_logNorm_sampled.rda')

load("m_fullModel_logNorm_sampled.rda")

load('m_fullModel_logNorm_sampled2.rda')

m_fullModel_logNorm

m_fullModel_logNorm2

comboFullModel <- c(m_fullModel_logNorm,m_fullModel_logNorm2)

## looking closer, it looks this model is missing its spatial
## component, which is probably why it sampled so quickly.
## ugh. reformulate it above and restart the sampling. Delete the old
## models. 

## we can just run the script once, with 8 chains.

rm m_fullModel_logNorm_sampled2.rda

## the log for the sampling is there, but not finished

## head hurts, pretty sure we restarted this sampling but it is nowhwere...

ps aux | grep m_fullModel_logNorm_sampling.r

ps ax | grep "/home/ubuntu/miniconda3/envs/r_env/lib/R/bin/exec/R"

ps ax | grep "file"

## yep, looks like the sampling is going for last three submodels, still need
## the deeper sampling of the second hurdle model

## check all these and make sure they have the random effects we expect:

m_ReadAbundancesSpatial_sampled2.rda

load('m_environmentalOnly_sampled.rda')

load('m_environmentalOnly_sampled2.rda')
m_environmentalOnly
m_environmentalOnly2

comboEnvOnly <- c(m_environmentalOnly, m_environmentalOnly2)

load("m_ReadAbundancesSpatial_sampled2.rda")


load('m_environmentalOnly_sampled.rda')

load('m_fullModel_logNorm_sampled.rda')
load('m_fullModel_logNorm_sampled2.rda')

m_environmentalOnly
m_fullModel_logNorm
m_fullModel_logNorm2

## lots of models floating around. We should have 
## ultimately havew two sampled files for 

m_ReadAbundancesOnly.rda
m_ReadAbundancesSpatial.rda
m_environmentalOnly.rda
m_fullModel_logNorm.rda
m_hurdle_PA.rda
m_hurdle_abu.rda

ls m_ReadAbundancesOnly_sampled.rda
ls m_ReadAbundancesSpatial_sampled.rda
ls m_environmentalOnly_sampled.rda

ls m_fullModel_logNorm_sampled.rda

ls m_hurdle_PA_sampled.rda

ls m_hurdle_abu_sampled.rda

## It think this means we have to delete these:
m_fullModel_logNorm_sampled.rda
m_fullModel_logNorm_sampled2.rda
## the new one should come in with 8 chains. 

## we also got the second readabu+spatial model sampling:
getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/m_ReadAbundancesSpatial_sampled2.rda"
putItHere="/media/vol/fichtelberg/hmscBackups/spatial/"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere

## okay, where are we? we have finished cross-validation in the sptemp models
## maybe run through the model comparison with these, over in the other script...

## but we need to keep the processors busy. 

## There is still some primary sampling to do, I think with the hurdle abundance model

less m_hurdle_abu_sampling.r

less m_hurdle_PA_sampling.r

ps ax | grep "m_hurdle_abu_sampling.r" ## done, 

ps ax | grep "m_hurdle_PA_sampling.r" ## yep, still running

ps ax -eo pid,lstart,cmd | grep "m_hurdle_PA_sampling.r"

ps ax -eo pid,lstart,cmd | grep "m_hurdle_abu_sampling.r"

library(Hmsc)
load("m_hurdle_abu_sampled.rda")

## yes, only 4 chains. Start the sampling for the other 4. Code modified above.

## and as far as cross validation?  This has not been started. 

## better get on it. 

## looking at our existing script for cross validation, I don't see why 
## it shouldn't work for these models also (originall written for the sptemp models)
## the main problem is that we didn't save these as rds files. So renaming is a problem. 

## last time I tried to fix this I destroyed the models. Make sure everything is 
## backed up first:

getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/m_hurdle_abu_sampled.rda"
putItHere="/media/vol/fichtelberg/hmscBackups/spatial/"
scp -P 30267 ubuntu@129.70.51.6:$getFile $putItHere

## now, let's save out an RDS copy of each of these.
## only the ones that have been completely sampled
## make sense to do at this point. 

## that is these:

m_ReadAbundancesOnly_sampled.rda
m_ReadAbundancesOnly_sampled2.rda

m_ReadAbundancesSpatial_sampled.rda
m_ReadAbundancesSpatial_sampled2.rda

m_environmentalOnly_sampled.rda
m_environmentalOnly_sampled2.rda

## so each of these pairs needs to be loaded, concatenated, and saved out as an RDS combo file

library(Hmsc)

load("m_ReadAbundancesOnly_sampled.rda")
load("m_ReadAbundancesOnly_sampled2.rda")
m_ReadAbundancesOnly_combo <- c(m_ReadAbundancesOnly,m_ReadAbundancesOnly2)
saveRDS(m_ReadAbundancesOnly_combo, file=("m_ReadAbundancesOnly_combo_sampled.rds"))

rm(list=ls())

load("m_ReadAbundancesSpatial_sampled.rda")
load("m_ReadAbundancesSpatial_sampled2.rda")
m_ReadAbundancesSpatial_combo <- c(m_ReadAbundancesSpatial,m_ReadAbundancesSpatial2)
saveRDS(m_ReadAbundancesSpatial_combo, file=("m_ReadAbundancesSpatial_combo_sampled.rds"))

rm(list=ls())

load("m_environmentalOnly_sampled.rda")
load("m_environmentalOnly_sampled2.rda")
m_environmentalOnly_combo <- c(m_environmentalOnly, m_environmentalOnly2)
saveRDS(m_environmentalOnly_combo, file=("m_environmentalOnly_combo_sampled.rds"))

rm(list=ls())

## now, can we run these through our script?

m_ReadAbundancesOnly_combo_sampled.rds
m_ReadAbundancesSpatial_combo_sampled.rds
m_environmentalOnly_combo_sampled.rds

nohup Rscript getCrossValFit.r "m_ReadAbundancesOnly_combo_sampled.rds"  &> m_ReadAbundancesOnly_combo_crossval.log &  ## 92204
## not sure how much to load the processors right now. 
## plenty of RAM, but processors are really busy. 
## looking at this, seems like the cross-validation is 
## relatively processor heavy but not using a lot of RAM. 
## Let's go ahead and start the other two, see what happens:

nohup Rscript getCrossValFit.r "m_ReadAbundancesSpatial_combo_sampled.rds"  &> m_ReadAbundancesSpatial_combo_crossval.log &  ## 92302
nohup Rscript getCrossValFit.r "m_environmentalOnly_combo_sampled.rds"  &> m_environmentalOnly_combo_crossval.log &  ## 92396

## and now, the denbi computer is buzzing, hissing, steaming, popping again. 
## poor thing.

## some backups needed for new files:
m_ReadAbundancesOnly_combo_crossValPreds.rds
m_hurdle_PA_sampled.rda
m_environmentalOnly_combo_crossValPreds.rds

## on local office pc:

#newfiles=( \
#m_ReadAbundancesOnly_combo_crossValPreds.rds \
#m_hurdle_PA_sampled.rda \
#m_environmentalOnly_combo_crossValPreds.rds \
#)

## and again, 31.8.24:

cd "/media/vol/fichtelberg/hmscBackups/spatial/"

getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/m_hurdle_abu_sampled2.rda"
putItHere="/media/vol/fichtelberg/hmscBackups/spatial/"
scp -vi /home/daniel/.ssh/ubuntu_e -P 30267 ubuntu@129.70.51.6:$getFile $putItHere 

## 3.10.24
## okay, the instance is scheduled to shut down
## let's see if it can. Check files:


tail m_fullModel_logNorm_sampled_c?.log

## no errors reported, all seem to have finished    

ls -ltrh m_fullModel_logNorm_sampled_c*

## they don't seem that large, but let's put them on my desktop and the lab computer:

ls -ltrh m_fullModel_logNorm_sampled_c*

mkdir m_fullModel_logNorm_sampled

cp m_fullModel_logNorm_sampled_c*  m_fullModel_logNorm_sampled/
cp m_fullModel_logNorm.rda m_fullModel_logNorm_sampled/

tar -czf m_fullModel_logNorm_sampled.tar.gz m_fullModel_logNorm_sampled

## and from office comp:

cd "/media/vol/fichtelberg/hmscBackups/spatial/"
getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/m_fullModel_logNorm_sampled.tar.gz"
putItHere="/media/vol/fichtelberg/hmscBackups/spatial/"

scp -vi /home/daniel/.ssh/ubuntu_e -P 30267 ubuntu@129.70.51.6:$getFile $putItHere 

## somehow, looks like I forgot to back all this up onto the lab computer:
## let's put them 

getFile="/media/vol/fichtelberg/hmscBackups/"
putItHere="/media/vol1/daniel/sulariArne/soilAnalysis/"
scp  -r $getFile test@132.180.112.115:$putItHere 

## in general, we should probably back up all rds and rda files. 
getFile="/vol/data/fichtelgebirgeSoils/denbiHMSCbackup.tar.gz"
putItHere="/media/vol/fichtelberg/hmscBackups/"
scp -ri /home/daniel/.ssh/ubuntu_e -P 30267 ubuntu@129.70.51.6:$getFile $putItHere 

## and put this big archive on the labcomp from the officecomp:
putItHere="/media/vol1/daniel/sulariArne/soilAnalysis/hmscBackups/spatial/"
getFile="/media/vol/fichtelberg/hmscBackups/denbiHMSCbackup.tar.gz"
scp $getFile test@132.180.112.115:$putItHere 

## this one was missing, get it on both towers:
getFile="/vol/data/fichtelgebirgeSoils/spatialAnalysis/m_hurdle_PA_sampled.rda"
putItHere="/media/vol/fichtelberg/hmscBackups/spatial/"
scp -ri /home/daniel/.ssh/ubuntu_e -P 30267 ubuntu@129.70.51.6:$getFile $putItHere 

## and put it on the labcomp from the officecomp:
putItHere="/media/vol1/daniel/sulariArne/soilAnalysis/hmscBackups/spatial/"
getFile="/media/vol/fichtelberg/hmscBackups/spatial/m_hurdle_PA_sampled.rda"
scp $getFile test@132.180.112.115:$putItHere 

## and can we catch up the laptop, for remote work?


## back on denbi

## and the sampling of the log normal abundance failed
## after a month. Jeezus

## can this be debugged?
## it looks like from the logs that only one chain failed somehow.
## so maybe let's run a lot of chains, individually?

## so rerun the same script from line 8986, with some modifications.
## let's switch over to the rds/user input model

soil
conda activate r_env

#### m_fullModel_logNorm_sampling_1atatime.r ####

library(Hmsc)
chainName = commandArgs(trailingOnly=TRUE)[1]
print("starting model:")
print("m_fullModel_logNorm")
print(paste("start time is", Sys.time()))
load("m_fullModel_logNorm.rda")
## fix this for real
thin = 20
## fix this for real
samples = 1000
nChains = 1
nP = 2
m_fullModel_logNorm = sampleMcmc(m_fullModel_logNorm,
                     samples = samples, 
                     thin=thin, 
                     transient = ceiling(0.5*samples*thin), 
                     nChains = nChains, 
                     nParallel = nP
                     )
filename <- paste0("m_fullModel_logNorm_sampled_c", chainName, ".rds")
saveRDS(m_fullModel_logNorm, file=filename)
print(paste("finish time is", Sys.time()))

#################################################

for i in {1..8}; do
  echo $i
  nohup Rscript m_fullModel_logNorm_sampling_1atatime.r $i  &> m_fullModel_logNorm_sampled_c${i}.log &
done

## pids

1358
1359
1360
1361
1362
1363
1364
1365

## what is the way forward here? 

## once we have a posterior for both hurdle models for the 
## spatial-only models, we can switch over to them for 
## exploration.

## but still don't, so I guess we keep building the analysis
## using the smaller spatiotemp model.

## we have our results from the big models, hopefully. 
## let's check them out, bring them up to speed with 
## work that has been done with the spatiotemporal dataset,
## if possible.

## where are the results from this model?:

## on the desktop pc:

ls -lh /media/vol/fichtelberg/hmscBackups/spatial

## the split up chains for the lognormal model spatial model:
ls /media/vol/fichtelberg/hmscBackups/spatial/m_fullModel_logNorm_sampled

## on the lab server:
cd /media/vol1/daniel/sulariArne/soilAnalysis/hmscBackups

## the hope is that these all sampled fully. 
## we may take one that we trust most for predictions.
## but how different are they?

## so bring them through the same old pipelines:

## also, we don't have cross validation for the logNorm full model...forgot about that.
## maybe the lab computer can handle it??

## check soon, get our ducks in rows first. 

## for the large spatial sampling scheme, we have the following models:


## 1
#### m_ReadAbundancesOnly_sampling.r ####
m_ReadAbundancesOnly_sampled.rda
m_ReadAbundancesOnly_sampled2.rda
m_ReadAbundancesOnly_combo_crossValPreds.rds

## 2
#### m_ReadAbundancesSpatial_sampling.r ####
m_ReadAbundancesSpatial_sampled.rda
m_ReadAbundancesSpatial_sampled2.rda
m_ReadAbundancesSpatial_combo_crossValModelFit.rds
m_ReadAbundancesSpatial_combo_crossValPreds.rds

## 3
#### m_environmentalOnly_sampling.r ####
m_environmentalOnly_sampled.rda
m_environmentalOnly_sampled2.rda
m_environmentalOnly_combo_crossValPreds.rds

## 4 
#### m_fullModel_logNorm_sampling.r ####
m_fullModel_logNorm_sampled/
m_fullModel_logNorm_sampled.tar.gz

## 5.1 m_hurdle_PA
#### m_hurdle_PA_sampling.r ####
m_hurdle_PA_sampled.rda
m_hurdle_PA_sampled2.rda

## 5.2 m_hurdle_abu
#### m_hurdle_abu_sampling.r ####
m_hurdle_abu_sampled.rda
m_hurdle_abu_sampled2.rda

## so we need the cross-validations for: 
m_hurdle_PA_sampled
m_hurdle_abu_sampled
m_fullModel_logNorm_sampled

## m_fullModel_logNorm_sampled results need to be concatenated for that,
## and every thing else. 

## First, let's try a cross-validation on the lab computer:
## for example, with:
m_hurdle_PA_sampled

## on the desktop computer. Probably won't work, but let's try. 

library(Hmsc)

pathHMSCarchives <- "/media/vol/fichtelberg/hmscBackups/spatial/"
load(paste0(pathHMSCarchives,"m_hurdle_PA_sampled.rda"))
load(paste0(pathHMSCarchives,"m_hurdle_PA_sampled2.rda"))

m_hurdle_PA_sampled <- c(m_hurdle_PA,m_hurdle_PA2)

saveRDS(m_hurdle_PA_sampled, file="m_hurdle_PA_combo_sampled.rds")

## back on bash
nohup Rscript getCrossValFit.r "m_hurdle_PA_combo_sampled.rds"  &> m_hurdle_PA_combo_sampled_crossval.log &  

## seems to work okay. not taking much RAM, set them up on the lab computer
## so rerun, and set up the other two models, also


## on nanocomp:

## m_hurdle_PA_sampled
## m_hurdle_abu_sampled
## m_fullModel_logNorm_sampled


library(Hmsc)

pathHMSCarchives <- "/media/vol1/daniel/sulariArne/soilAnalysis/hmscBackups/spatial/"
load(paste0(pathHMSCarchives,"m_hurdle_PA_sampled.rda"))
load(paste0(pathHMSCarchives,"m_hurdle_PA_sampled2.rda"))
m_hurdle_PA_sampled <- c(m_hurdle_PA,m_hurdle_PA2)
saveRDS(m_hurdle_PA_sampled, file="m_hurdle_PA_combo_sampled.rds")

rm(list=ls())

pathHMSCarchives <- "/media/vol1/daniel/sulariArne/soilAnalysis/hmscBackups/spatial/"
load(paste0(pathHMSCarchives,"m_hurdle_abu_sampled.rda"))
load(paste0(pathHMSCarchives,"m_hurdle_abu_sampled2.rda"))
m_hurdle_abu_sampled_combo <- c(m_hurdle_abu, m_hurdle_abu2)
saveRDS(m_hurdle_abu_sampled_combo, file="m_hurdle_abu_combo_sampled.rds")

rm(list=ls())

## m_fullModel_logNorm_sampled is in eight pieces, needs to be combined:
step=0
m_fullModel_logNorm_sampled_chains <- list()
pathlognorm <- "/media/vol1/daniel/sulariArne/soilAnalysis/hmscBackups/spatial/m_fullModel_logNorm_sampled/"
for (i in list.files(pathlognorm, pattern="rds")){
    step=step+1
    print(i)
    m_fullModel_logNorm_sampled_chains[[step]] <- readRDS(paste0(pathlognorm,i))
}
## okay, how can we concatenate these things?
m_fullModel_logNorm_sampled_combo = c(m_fullModel_logNorm_sampled_chains[[1]],
                                      m_fullModel_logNorm_sampled_chains[[2]],
                                      m_fullModel_logNorm_sampled_chains[[3]],
                                      m_fullModel_logNorm_sampled_chains[[4]],
                                      m_fullModel_logNorm_sampled_chains[[5]],
                                      m_fullModel_logNorm_sampled_chains[[6]],
                                      m_fullModel_logNorm_sampled_chains[[7]],
                                      m_fullModel_logNorm_sampled_chains[[8]])

saveRDS(m_fullModel_logNorm_sampled_combo, file="m_fullModel_logNorm_combo_sampled.rds")

## and now, try running these things, still on the nanoComp:
m_hurdle_PA_combo_sampled.rds
m_fullModel_logNorm_combo_sampled.rds
m_hurdle_abu_combo_sampled.rds


nohup Rscript getCrossValFit.r "m_hurdle_PA_combo_sampled.rds"  &> m_hurdle_PA_combo_sampled_crossval.log &  ## 85633
nohup Rscript getCrossValFit.r "m_fullModel_logNorm_combo_sampled.rds"  &> m_fullModel_logNorm_combo_sampled_crossval.log & ## 86089
nohup Rscript getCrossValFit.r "m_hurdle_abu_combo_sampled.rds"  &> m_hurdle_abu_combo_sampled_crossval.log & ## 86308  



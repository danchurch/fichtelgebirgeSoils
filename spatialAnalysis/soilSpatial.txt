## git our local copy of the repo in order for office comp.
## working off the work tower, so need to get git synced up now to
## avoid confusion.

## we need RSA with SHA-2 signature algorithm

man ssh-keygen
ssh-keygen -t rsa -f fuj2git

## now, what do we need to get the push functionality...

git clone https://github.com/danchurch/fichtelgebirgeSoils.git

## test

touch thisIsNotReal.txt

## and of course can't push

git config --global user.email "danchurchthomas@gmail.com"
git config --global user.name "danchurch"

git remote add origin https://github.com/danchurch/fichtelgebirgeSoils.git
git branch -M main
git remote set-url origin git@github.com:danchurch/fichtelgebirgeSoils.git
git push -u origin main

## and we're in business with github

## maybe let's get a conda environment going for this.

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash

conda config --set auto_activate_base false

## get the mamba solver:

conda update -n base conda
conda install -n base conda-libmamba-solver
conda config --set solver libmamba

## get the standard channels
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

## this new conda env comes with python3.12

## let's see if this works for our spatial analysis


conda activate
conda create -n "spatialDirt" 
 
## let's think about spatial turnover in Sulari's community data

## first step would be to get a map. 

## we want to see where we sample, and visualize respiration 
## values across the landscape

conda deactivate

conda remove -n spatialDirt --all

conda create -n "spatialDirt" 

conda activate "spatialDirt" 
conda config --env --add channels conda-forge
conda config --env --set channel_priority strict

conda install python=3 geopandas

conda activate spatialDirt 

pip install rasterio

## we also need to be R up to speed...
## maybe do this outside of conda

conda deactivate

sudo R 

install.packages("BiocManager")
BiocManager::install("phyloseq")


## I think that took care of most of the complex installs

## oh wait, let's get the jupyter notebook setup going...

## how do we make sure that the jupyter behaves, stays in the 
## right python?

conda activate spatialDirt 
pip install notebook 

which jupyter ## looks like that work. Gets easier every year.

## and it looks like it is even keeping the R kernel from 
## my general environment. 

## to get a bash kernel on there? https://github.com/takluyver/bash_kernel

pip install bash_kernel
python -m bash_kernel.install

##### bayesian setup #####

## last time we worked with pymc3 we needed
## a separate conda env. Let's see if 
## things have gotten better. 

## first back up the env, just in case

conda activate spatialDirt

conda env export > spatialDirt.yml

## installing bambi should also install pymc, so
## try the bambi conda installs as per: 
https://github.com/bambinos/bambi#quickstart

pip install bambi

pip install "preliz[full,lab]"

## get the data from the newest martin book.

## put outside our repo

cd /home/daniel/Documents/manualsBooks/bayesian
git clone https://github.com/aloctavodia/BAP3.git

## seems like that worked in our spatialDirt environment

## try everything out for a bit, then update yaml backup for the repo


conda activate spatialDirt 

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
from matplotlib_scalebar.scalebar import ScaleBar
import pymc as pm


spatDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
## ^different on laptop
os.chdir(spatDir)

## we just want a map of points right now:

## on officeComp
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"
## on laptop
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"

envData = pd.read_csv(sulariEnvCSV)
## get rid of spaces
envData.rename({"Sample ID":"SampleID"}, axis="columns", inplace=True)
## we need to clean up the plot.ID. Sulari recorded season in the plot IDs,
## using letter codes. Also she has one double sampling, with an underscore
envData['Plot.ID'] = envData['Plot.ID'].str.slice(0,5)

## let's get rid of decimals in the names, weird for python work:
envData.rename({"Plot.ID":"PlotID", 
       "soil.respiration":"soil_respiration",
              "Land.type":"Land_type"}, 
                axis="columns", inplace=True)

envData.head()

## so we don't have to repeat:
envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index=False)

envData.head()


envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
 

## if we need to drop controls
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

## her otu table is really large:
##### R ###
library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")
logMin50ps
comdat <- as.data.frame(otu_table(logMin50ps))
write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")
############

## back in python

comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", index_col=0)

comData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

comData.head()

## the map below looks funny...looks like every forest plot has 
## a grassland under it...

dupsFilter = envData.Latitude.duplicated()
dupped = envData[dupsFilter].sort_values(by="Latitude")
dupped.groupby('Plot.ID').nunique()
envData.iloc[0:5,0:8]

## nope...looks okay

## we want a geodf...

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

## to convert to UTM? looks like we are in zone 33
## looks like:
## EPSG:32633

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

sulariPlot_utm.tail()

## can we import our georeferenced tif of the study area?
## lat/lon
#fichtelMap = rasterio.open("studyAreaClipped_modified.tif")
## UTM

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color code our points according to land type?
cdik={
"Arableland":"b",
"Grassland":"y",
"Forest":"g",
}
sulariPlotsDF['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]
sulariPlot_utm['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]

fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    facecolor=sulariPlotsDF['landColors'],
    markersize=400) 


grassPatch = Patch(color='y', label='grassland',)
forestPatch = Patch(color='g', label='forest')
farmPatch = Patch(color='b', label='arable land')
ax.legend(handles=[grassPatch, forestPatch, farmPatch], 
          loc="lower left",
          fontsize=15,
)

## if we want to compare just grassland and forest

plt.close('all')
onlyGrassForest = sulariPlot_utm[sulariPlot_utm['Land.type'].apply(lambda x: x in ["Forest", "Grassland"])]
fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
onlyGrassForest.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    linewidths=2,
    facecolor=onlyGrassForest['landColors'],
    markersize=200) 
ax.ticklabel_format(style='plain', axis='y', useOffset=False)
grassPatch = Patch(color='y', label='grassland',)
forestPatch = Patch(color='g', label='forest')
ax.legend(handles=[grassPatch, forestPatch], loc='lower left')
ax.add_artist(ScaleBar(1, location='lower right')) 
ax.set_xlim([265500, 286930])
ax.set_ylim([5547227, 5570000])
plt.savefig('forestVsGrasslandMapUTM.png', dpi=600, format='png')

## Look at the turnover data:

## lat/long
aa = pd.DataFrame({'xx':envData.Longitude, 'yy':envData.Latitude})
physDist = sp.distance.pdist(aa, metric='euclidean')
bcDist = sp.distance.pdist(comData, metric='brayCurtis')
fig, ax = plt.subplots()
ax.scatter(physDist, bcDist)
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title(label="Turnover with Lat/Lon", loc='center')

## utms
aa = pd.DataFrame({'xx':sulariPlot_utm.geometry.x, 'yy':sulariPlot_utm.geometry.y})
physDist = sp.distance.pdist(aa, metric='euclidean')
bcDist = sp.distance.pdist(comData, metric='brayCurtis')
fig, ax = plt.subplots()
ax.scatter(physDist, bcDist)
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title(label="Turnover with UTM", loc='center')

plt.close('all')
## subset by landtype
for lt in [ "Arableland" ,"Grassland" ,"Forest"]: 
    print(lt)
    edf = envData[envData['Land.type'] == lt]
    cdf = comData.loc[edf.index]
    aa = pd.DataFrame({'xx':edf.Longitude, 'yy':edf.Latitude})
    aa = aa.iloc[0:120,:]
    physDist = sp.distance.pdist(aa, metric='euclidean')
    bcDist = sp.distance.pdist(cdf, metric='brayCurtis')
    fig, ax = plt.subplots()
    ax.scatter(physDist, bcDist)
    ax.set_title(lt)
    ax.set_title(label= (lt + " in degrees"), loc='center')
    X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
    ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')

## well that looks pretty much like I hypothesized
## good stuff.

sulariPlot_utm.head()

plt.close('all')
plt.rc('ytick', labelsize=15)
plt.rc('xtick', labelsize=15)
lts = [ "Arableland" ,"Grassland" ,"Forest"]
#lts = [ "Grassland" ,"Forest"]
fig, axes = plt.subplots(nrows=1, ncols=len(lts), sharey=True)
axes = axes.flatten()
for nu,lt in enumerate(lts):
    edf = sulariPlot_utm[sulariPlot_utm['Land.type'] == lt]
    cdf = comData.loc[edf.index]
    aa = pd.DataFrame({'xx':edf.geometry.x, 'yy':edf.geometry.y})
    physDist = sp.distance.pdist(aa, metric='euclidean')
    bcDist = sp.distance.pdist(cdf, metric='brayCurtis')
    axes[nu].scatter(physDist, bcDist)
    X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
    linMod =  LinearRegression().fit(X, Y)
    axes[nu].plot( X, linMod.predict(X), c='k')
    axes[nu].set_title(label=lt, size=20, loc='center')
    axes[nu].set_xlabel('meters', size=20)
    print(lt, stats.linregress(physDist,bcDist))

fig.suptitle("Turnover in prokaryotic community", size=40)
axes[0].set_ylabel('Bray-Curtis dissimilarity', size=20)
axes[1].tick_params(left=False, labelleft=False, right=True, labelright=True, color='red', axis='y')
plt.subplots_adjust(wspace = 0)


###################################
##
## outputs from stats.regress:
##
## Arable Land
## slope=2.810886008879358e-06
## intercept=0.5742255266887248
## rvalue=0.07906122967379002
## pvalue=0.02033328808278514
## stderr=1.209265073980233e-06
## intercept_stderr=0.013364930035836518
## 
## Grassland
## slope=4.135586933082137e-07
## intercept=0.5590568736859212
## rvalue=0.012708447690178782
## pvalue=0.7298157896065798
## stderr=1.1969812747713313e-06
## intercept_stderr=0.013504959623407586
## 
## Forest
## slope=5.843245351182221e-06
## intercept=0.6024952517397398
## rvalue=0.20458396890349276
## pvalue=1.918646699083231e-08
## stderr=1.0284330215725841e-06
## intercept_stderr=0.01259259776931045
######################################

## add in the correlation coefficients and pvalues to grant app graphic. 

###### SAC curves ##########

## we have to stop avoiding gamma diversity calculations...

## do this in vegan? why not.

R

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
setwd(spatDir)


library(vegan)
library(phyloseq)

comM <- read.csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv', 
                    row.names=1)

## why is this so big, btw?

sum(colSums(comM) > 0) ## 4363. Why do we have a bunch of empty colums? I think these were low abundance ASVs, below our cutoffs.

## get rid of them to save memory:

library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")

logMin50ps

comdat <- as.data.frame(otu_table(logMin50ps))
comdat = comdat[,colSums(comdat) > 0] 

write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")


## get rid of controls
notControls=!(row.names(comM) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comM = comM[notControls,]

comM[1:4,1:4]


sp1 <- specaccum(comM)

plot(sp1, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue")

specpool(comM)

sp2 <- specaccum(comM, "random")

summary(sp2)

plot(sp2, ci.type="poly", col="red", lwd=2, ci.lty=0, ci.col="pink")


data(BCI)

sp1 <- specaccum(BCI)


sp2 <- specaccum(BCI, "random")

sp2

summary(sp2)

plot(sp1, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue")
boxplot(sp2, col="yellow", add=TRUE, pch="+")
## Fit Lomolino model to the exact accumulation
mod1 <- fitspecaccum(sp1, "lomolino")
coef(mod1)
fitted(mod1)
plot(sp1)

aa <- specaccum(comM, method = "exact")

?specaccum

anaSAC <- data.frame(aa$richness, aa$sd)
colnames(anaSAC) <- c('richness', 'sd')
anaSpeciesEstimators = specpool(comM)
print(anaSpeciesEstimators)
 
## okay, but we need to separate out by land types.
## wish we were in python...

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
setwd(spatDir)
library(vegan)
library(phyloseq)

comData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", row.names=1)
envData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv", row.names=1)

all(row.names(comData) == row.names(envData))

lt <- 'Forest'
justThisLandtype=row.names(envData[envData['Land_type'] == lt,])
aa <- comData[ justThisLandtype,]
sp1 <- specaccum(aa)
plot(sp1, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue")


## so loop this:

for (lt in c('Arableland','Grassland', 'Forest')){
    print(lt)
    justThisLandtype=row.names(envData[envData['Land_type'] == lt,])
    comm.i <- comData[ justThisLandtype,]
    specAccum.i <- specaccum(comm.i)
    SACdf.i <- data.frame(specAccum.i$richness, specAccum.i$sd)
    colnames(SACdf.i) <- c('richness', 'sd')
    speciesEstimators.i = specpool(comm.i)
    print(speciesEstimators.i)
    write.csv(SACdf.i, file=paste(lt, "SAC.csv", sep="_"))
    write.csv(speciesEstimators.i, file=paste(lt, "specEst.csv", sep="_"))
}

## interesting, this is pretty much exactly what Brendan found
## in the amazon. Despite lower alpha diversity, higher beta 
## diversity in forest soils. 
## and this equates to a higher total diversity across the
## survey (gamma). 

## take over to python for plotting

## we have an old function for this, wonder if it still works:


os.chdir("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis")

sacs = [ i for i in os.listdir() if "_SAC.csv" in i  ]

def plotSACs(habtype, color='black', ax=None):
    if ax is None: fig, ax = plt.subplots()
    sacs = [ i for i in os.listdir() if "_SAC.csv" in i  ]
    sacName = (habtype +'_SAC.csv')
    assert( (habtype +'_SAC.csv') in sacs)
    specEstName = (habtype + "_specEst.csv")
    sac_i = pd.read_csv(sacName, index_col=0)
    specEst_i = pd.read_csv(specEstName, index_col=0).loc['All']
    specEst_i.index = specEst_i.index.str.replace(".","_")
    X = sac_i.index
    ax.plot(X, sac_i['richness'], color=color)
    ax.fill_between(x=X,
                     y1=sac_i.richness - sac_i.sd,
                     y2=sac_i.richness + sac_i.sd,
                    alpha=0.4,
                    color=color,
                    )

plt.close('all')
fig, ax = plt.subplots(figsize=(10,10))
plotSACs('Arableland', ax=ax, color='#862d2d')
plotSACs('Forest', ax=ax, color='#006600')
plotSACs('Grassland', ax=ax, color='#FF7F00')

Arableland_patch = Patch(color='#862d2d', label='Arableland', alpha=0.4)
Forest_patch = Patch(color='#006600', label='Forest', alpha=0.4)
Grassland_patch = Patch(color='#FF7F00', label='Grassland', alpha=0.4)

ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])
ax.set_title('Species accumulution curves by\nland-use/Habitat')
ax.set_xlabel('Sites sampled')
ax.set_ylabel('Prokaryotic ASVs')


[Arableland_patch, Forest_patch, Grassland_patch]

## repeat alpha diversity
## using our >50 reads OTU table, can we calculate alpha diveristy by land type?
## back to old fashioned vegan/R

## we just want species richness. So we need to rarefy and compare 
## forest v. farm v. grassland data

notControls=!(row.names(comData) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comData = comData[notControls,]
envData = envData[notControls,]

data(BCI)

S <- specnumber(BCI) # observed number of species

S <- specnumber(comData) # observed number of species

## pretty much same as:
aa <- comData
aa[aa > 0] <- 1
rowSums(aa)

S <- specnumber(comData) # observed number of species

(raremax <- min(rowSums(BCI)))

Srare <- rarefy(BCI, raremax)

plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")

abline(0, 1)

rarecurve(BCI, step = 20, sample = raremax, col = "blue", cex = 0.6)


## so we are looking for a rarified species richness for each site.

## from this we will generate 3 mean +/- error values of species richness 
## one for each land use.

## this kind of analysis is really ok for count data.
## we've done all kinds of transformations, to try 
## to reduce sequencer error. 

## so I think we need to back up to phyloseq, to use our 
## sequencing depth information

library(phyloseq)


## transformed:
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")

## not transformed:
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

logMin50ps

(p = plot_richness(ps, x = "Land.type"))

estimate_richness(ps)

estimate_richness

## but I am thinking about this incorrectly. 
## these richness estimates are way high, 
## because of PCR, sequencer error, etc. 

## we attempting to reign in these errors 
## a bit through our transformations, let's
## honor this. 

## so back and use our communty matrix, 

savehistory("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/latelyInR.txt")

## to make it into "count data", multiply 
## to get rid of decimals:


sum(comData < .001 & comData > 0) ## 508 observations smaller than .001.
sum(comData < .0001 & comData > 0) ## 0 observations, so let's multiply by 10000
comDataFakeCounts = ceiling(comData * 10000) 
min(comDataFakeCounts[ comDataFakeCounts > 0 ]) ## our smallest non-zero observation is 8 

S <- specnumber(comDataFakeCounts) # observed number of species

(raremax <- min(rowSums(comDataFakeCounts))) ## 10001

rowSums(comDataFakeCounts)

Srare <- rarefy(comDataFakeCounts, raremax) ## this is what we need. 

## this is an estimate of how many species are present in each 
## sample, after coming down to a minimum abundance


## kind of interesting but not useful. Shows we sequenced deeply enough:
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")

## can we do all that without transforming to "counts"?
S2 <- specnumber(comData) # observed number of species, same as fake counts

(raremax2 <- min(rowSums(comData))) ## .9896

rowSums(comData)



## compare to:

bb <- specnumber(comData) # observed number of species
all(bb == Srare) ## yes. the same
## So I guess I am brilliant, I just reinvented their command. 
## big waste of time. 

## maybe also shows there isn't really a need to rarefy, at least
## on the transformed data

## not sure, but now let's trust these numbers.

## now, subset by land type, and get means?

head(envData)

all(row.names(envData) == row.names(comData))

## I'd guess we need a vector of group names (by land_type):
hist(Srare, 20) ## looks more or less normal.
mean(Srare) ## 301.3833
sd(Srare) ## 26.4

print("mean alpha diversity of all sites = ", mean(Srare))

print(paste("mean alpha diversity of all sites =", mean(Srare), "ASVs"))

cat(paste("mean alpha diversity of all sites =", mean(Srare), "ASVs"))

cat(paste("mean alpha diversity of all sites =", mean(Srare), "+/-", round(sd(Srare)), "ASVs"))


all(names(Srare) == row.names(envData))

## break this down by groups:

tapply(Srare, envData$Land_type, mean)
tapply(Srare, envData$Land_type, sd)

boxplot(Srare ~ envData$Land_type)

## anova
res.aov <- aov(Srare ~ envData$Land_type)
summary(res.aov) ## F= 2.85, p = 0.0618

## so maybe differences in species richness due to land type,
## maybe not.  Not a large effect, anyway. 
## update notebook, give it a break.

## t-test for difference between forest and grassland?

## to remove the farm samples 

noFarms <- envData$Land_type != "Arableland"

envData$Land_type[noFarms]
Srare[noFarms]


head(envData)

t.test(Srare[noFarms] ~ envData$Land_type[noFarms])

t_test(weight ~ group)

## we should probably do this in a bayesian way...
## get the ordinations done, then order the new book, 
## PCNMs can also be started without tests.

## but generally not sure how to handle the multivariate
## tests in a bayesian way. 

## two possibilities:
## BetaBayes: https://doi.org/10.3390/d14100858, somehow related to GDS
## BERA: https://doi.org/10.1080/00273171.2019.1598837
## BERA is apparently related to RDA. More reading is in order on both. 

## in the meantime...re-run the ordinations:

## the usual pipeline
## let's get ordinations with vegan and plot with python 

library(vegan)

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
setwd(spatDir)

comData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", row.names=1)
envData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv", row.names=1)
## get rid of controls
notControls=!(row.names(comData) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comData <- comData[notControls,]
envData <- envData[notControls,]

comData[1:4,1:4]

comNMS <- metaMDS(comData, try=40)

write.csv(comNMS$points, file='comNMS.csv')

## check this out in python:

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import matplotlib.colors 
import os
import scipy.spatial as sp
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
import pymc as pm
import preliz as pz
import arviz as az
import rasterio.plot

## data
nmsPts = pd.read_csv("comNMS.csv", index_col=0)
sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 
controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
envData.drop(controls, axis='rows', inplace=True)
comData.drop(controls, axis='rows', inplace=True)

## need some colors for land type
colorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
landCols = [ colorDict[i] for i in envData['Land_type'] ]
plt.close('all')
fig, ax = plt.subplots()
ax.scatter(x=nmsPts["MDS1"],
           y=nmsPts["MDS2"], 
           c=landCols,
          )
Arableland_patch = Patch(color='#862d2d', label='Arableland')
Forest_patch = Patch(color='#006600', label='Forest')
Grassland_patch = Patch(color='#FF7F00', label='Grassland')
ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])

## try markers for seasonality?
seasonDict = {
     'S': "o", 
    'SP': "v", 
    'W1': "D", 
    'W2': "P", 
     'A': "s",
}

## we might also check pH, and seasonality, and microbial biomass

## seasonality:
seasonShapes = [ seasonDict[i] for i in envData['season'] ]

## matplot lib doesn't change markers on the fly...
## if we want to change markers for each season:

plt.close('all')
fig, ax = plt.subplots()
for i in envData.season.unique():
  print(i)
  env_i = envData[envData['season'] == i]
  plots_i = env_i.index.to_list()
  nmsPts_i = nmsPts.loc[plots_i]
  cols_i = [ colorDict[i] for i in env_i['Land_type'] ]
  ax.scatter(x=nmsPts_i["MDS1"],
             y=nmsPts_i["MDS2"], 
             c=cols_i,
        marker=seasonDict[i],
            )

## I don't see any evidence of seasonality affecting these
## community structures

## ph ordinations

## color by pH, land by symbol, respiration by size

landTypeShapesDict = {
'Arableland': "o", 
'Forest'    : "v", 
'Grassland' : "s", 
}

pHmin = envData['pH'].min() ## 3.647
pHmax = envData['pH'].max() ## 7.312
norm=matplotlib.colors.Normalize(pHmin, pHmax)
plt.close('all')
fig, ax = plt.subplots()
for i in envData.Land_type.unique():
  print(i)
  env_i = envData[envData['Land_type'] == i]
  plots_i = env_i.index.to_list()
  nmsPts_i = nmsPts.loc[plots_i]
  sizes = env_i['soil_respiration']*50
  ax.scatter(x=nmsPts_i["MDS1"],
             y=nmsPts_i["MDS2"], 
             #s=140,
             s=sizes,
             c=env_i['pH'],
             cmap='Spectral',
             edgecolors='black',
             marker=landTypeShapesDict[i],
             norm=norm,
            )

fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.Normalize(pHmin, pHmax), cmap='Spectral'),
             ax=ax, orientation='vertical', label='pH')


## this is a good graph. needs a legend. 

## as usual, legends are beyond my ability. Do them manually later if we want the figure.


### test out bayesian setup, try comparison of two groups ###

dist = pz.Beta()
pz.maxent(dist, 0.1, 0.7, 0.9)

## test out the pymc3 setup:
np.random.seed(123)
trials = 4
theta_real = 0.35
data = pz.Binomial(n=1, p=theta_real).rvs(trials)

np.random.seed(123)
trials = 4
theta_real = 0.35
data = stats.bernoulli.rvs(p=theta_real, size=trials)

with pm.Model() as our_first_model:
    θ = pm.Beta('θ', alpha=1., beta=1.)
    γ = pm.Bernoulli('γ', p=θ, observed=data)
    trace = pm.sample(1000, random_seed=123)

## works. I can hardly remember how to do this bayesian stuff,
## but we can work from old examples....

## let's redo our only statistical model/test so far, the comparison 
## of alpha diversity between the land types

## start with oswaldo's tips example:

tips = pd.read_csv("/home/daniel/Documents/manualsBooks/bayesian/BAP3/code/data/tips.csv")

tips.tail()

categories = np.array(["Thur", "Fri", "Sat", "Sun"])
tip = tips["tip"].values
idx = pd.Categorical(tips["day"], categories=categories).codes

## arviz has cool plotting capabilities I have not even begun to learn:
az.plot_forest(tips.pivot(columns="day", values="tip").to_dict("list"),
               kind="ridgeplot",
               hdi_prob=1,
               colors="C1",
               figsize=(12, 4))

## for indexing with Arviz:

coords = {"days": categories, "days_flat":categories[idx]}

with pm.Model(coords=coords) as comparing_groups:
    μ = pm.HalfNormal("μ", sigma=5, dims="days")
    σ = pm.HalfNormal("σ", sigma=1, dims="days")
    y = pm.Gamma("y", mu=μ[idx], sigma=σ[idx], observed=tip, dims="days_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))


_, axes = plt.subplots(2, 2, figsize=(10, 5), sharex=True, sharey=True)

az.plot_ppc(idata_cg, num_pp_samples=100,
            colors=["C1", "C0", "C0"],
            coords={"days_flat":[categories]}, flatten=[], ax=axes)

az.plot_trace(idata_cg)

plt.show()

## seems fine
az.summary(idata_cg, kind="stats").round(2)



## okay, how do we adapt this to our data? we want to compare 
## alpha diversity of three groups - crop, forest, and grassland.

sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 
controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
envData.drop(controls, axis='rows', inplace=True)
comData.drop(controls, axis='rows', inplace=True)

## we observed above that we can trust the raw species 
## richness counts from our community matrix, no need
## to rarify back or anything. 

## so how to get this in pandas/python?

aa = comData.copy()
aa[aa > 0] = 1
specRich = aa.sum(axis="columns")

comData.head()

aa.head()

aa.sum(axis="columns")

aa.sum(axis="columns").loc['S14']

aa.sum(axis="columns").loc['S102']


specRich = aa.sum(axis="columns")



pd.to_numeric(specRich, downcast='integer')

## maybe a df with all the info we need:

specRichLT = (pd.concat([pd.to_numeric(specRich, downcast='integer'), envData['Land_type']], axis='columns')
                     .rename({0:"spRich"},axis="columns"))

## looks right. so this should be our species richness. Might need this later:
specRichLT.to_csv("specRich.csv")

specRichLT.head()

## we want to get a distribution for the mean values of alpha diversity for
## each group 
 
specRichLT.head()

## can we visualize this with arviz first?:

az.plot_forest(specRichLT.pivot(columns="Land_type", values="spRich").to_dict("list"),
               kind="ridgeplot",
               hdi_prob=1,
               colors="C1",
               figsize=(12, 4))

## oswaldo's confusing code for creating an index, adapted for our data:
categories = np.array(["Arableland", "Grassland", "Forest"])
spr = specRichLT["spRich"].values
idx = pd.Categorical(specRichLT["Land_type"], categories=categories).codes
coords = {"Land_type": categories, "land_type_flat":categories[idx]}

with pm.Model(coords=coords) as comparing_groups:
    μ = pm.Normal("μ", mu=300, sigma=50, dims="Land_type")
    σ = pm.HalfNormal("σ", sigma=10, dims="Land_type")
    y = pm.Normal("Species richness", mu=μ[idx], sigma= σ[idx], observed=spr, dims="land_type_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))

    #y = pm.Gamma("y", mu=μ[idx], sigma=σ[idx], observed=spr, dims="land_type_flat") ## better for outliers?

plt.close('all')


with pm.Model(coords=coords) as comparing_groups:
    μ = pm.Normal("μ", mu=300, sigma=50, dims="Land_type")
    σ = pm.HalfNormal("σ", sigma=20, dims="Land_type")
    y = pm.Normal("Species richness", mu=μ[idx], sigma= σ[idx], observed=spr, dims="land_type_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))

plt.close('all')
fig, axes = plt.subplots(3, 1, sharex=True)
az.plot_ppc(idata_cg, num_pp_samples=100, coords={"land_type_flat":[categories]}, flatten=[], ax=axes)
fig.tight_layout()

## works, but the outliers are forcing a lot of variation into the posterior

## can we do this with cauchy?

with pm.Model(coords=coords) as model_t:
    μ = pm.Normal("μ", mu=300, sigma=50, dims="Land_type")
    σ = pm.HalfNormal("σ", sigma=20, dims="Land_type")
    ν = pm.Exponential('ν', 0.1, dims="Land_type") ## exponential gets flatter with lower values, mean gets pulled away from zero
    y = pm.StudentT('Species richness', nu=ν[idx], mu=μ[idx], sigma= σ[idx], observed=spr, dims="land_type_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))

## in the notebook, they give two models for this
## the second uses the gamma distribution.
## not run - why gamma? not sure. 

plt.close('all')

fig, axes = plt.subplots(3, 1, sharex=True)
az.plot_ppc(idata_cg, num_pp_samples=100, coords={"land_type_flat":[categories]}, flatten=[], ax=axes)
axes[0].set_xlim(200,400)
axes[1].get_legend().remove()
axes[2].get_legend().remove()
fig.tight_layout()


#### map of respiration values ####

## we still don't have a general analysis strategy,
## but it always helps to look at a map at the trait 
## of interest.

## we want to see rates of respiration across the 
## study. 

## it would be great to gave a vectorized land use map...
## does this exist somewhere?

## first, plot the respiration values:

envData.soil_respiration

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )
sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color edges by landtype:

landColorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
landCols = [ landColorDict[i] for i in sulariPlot_utm['Land_type'] ]
## 
plt.close('all')
fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
ax.ticklabel_format(useOffset=False, style='plain')
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    cmap='YlOrRd',
    #cmap='RdPu',
    column='soil_respiration',
    edgecolors=landCols,
    linewidth=2,
    markersize=sulariPlot_utm['soil_respiration']*50,
     )

ax.set_ylim(5547500, 5570000)
ax.set_xlim(265000, 287000)
respMin = envData['soil_respiration'].min() ## 0.488057256
respMax = envData['soil_respiration'].max() ## 13.70117879
fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.Normalize(respMin, respMax), cmap='YlOrRd'),
             ax=ax, orientation='vertical', label='resp')
Arableland_patch = Patch(color='#862d2d', label='Arableland')
Forest_patch = Patch(color='#006600', label='Forest')
Grassland_patch = Patch(color='#FF7F00', label='Grassland')
ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])

#### find species that are associated with high respiration (and low resp?) ##

## try deseq or bayesian equivalent. 
## or indicator species?
## cooccurrence networks?

## in general, we want species that are associated with high resp
## even in forests and croplands, which are the land types with 
## lower respiration rates generally. 

## we could cluster the communities and see if a cluster not related to
## to land type emerges. It is possible that a "highly respiring" community
## exists. But seems unlikely, across all land types. 

## take the highest respiring sites from each land type, and look for 
## species that occur only in them? or run deseq on each land 
## type alone and take the species positively associated with respiration

## see if there are any common species. We'll start with deseq, but 
## I think for publication I'd like to build some bayesian models for
## differential abundance


##### deseq ##### 

## need deseq. Let's install it in the overall environment, not just spatialDirt env


if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("DESeq2")

## we need raw abundances

R

library('phyloseq')
library('DESeq2')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

psLandCont <- ps ## make a duplicate phyloseq obj to play with

sample_data(psLandCont)[c('C1.1','C1.2','C2.1','C2.2'),'Land.type']  <- "control" ## add info

tail(sample_data(psLandCont)) ## looks okay

## okay, now following the tutorial above:

diagdds = phyloseq_to_deseq2(psLandCont, ~ Land.type)
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")

resultsNames(diagdds) ## Forest vs. Grassland not mentioned, but still possible

## let's see how the results look:
res <- results(diagdds, contrast=c("Land.type","Forest","Arableland"))

## edit down to highly significant results:

alpha = 0.1 ## significance cutoff
res <- res[complete.cases(res),] ## necessary because not all ASVs in all landtypes? 
## but then are those ASVs that are present only in one or two land types excluded here?
## these would be among the most important, I would think...think about this later
sigtab = res[(res$padj < alpha), ] ## cut off below the alpha using adjusted pvalues
## add taxonomy:
sigtab = cbind(as(sigtab, "data.frame"), as(tax_table(psLandCont)[rownames(sigtab), ], "matrix"))

## to check ASVs associated with respiration?

R

library('phyloseq')
library('DESeq2')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

## just like before make a duplicate phyloseq obj to play with
## and get rid of controls while we're at it
psNoControl = prune_samples(!(rownames(sample_data(ps)) %in% c("C1.1","C1.2","C2.1","C2.2")), ps)

## get rid of NAs in basalResp
basalRespNotNA <- !is.na(sample_data(psNoControl)$soil.respiration)
psNoControl = prune_samples(basalRespNotNA, psNoControl)
## okay, same old code as before:
diagdds = phyloseq_to_deseq2(psNoControl, ~ soil.respiration) ## set "treatment" of interest
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")
resultsNames(diagdds)
res <- results(diagdds)

alpha = 0.01 ## significance cutoff
res <- res[complete.cases(res),] ## necessary because not all ASVs in all landtypes
sigtab = res[(res$padj < alpha), ] ## cut off below the alpha using adjusted pvalues

sigtab$log2FoldChange

View(sigtab)



write.csv(sigtab, file="respDiffSeq.csv")

## these are all the strongly positively associated ASVs with resp:

highResp <- tax_table(ps)[c('ASV5', 'ASV13', 'ASV85', 'ASV110', 'ASV371', 'ASV462', 'ASV621',
       'ASV1089', 'ASV1419', 'ASV1795', 'ASV1831', 'ASV1905', 'ASV2058',
       'ASV2184', 'ASV2773')]

write.csv(highResp, file="hiRespTax.csv")


## there is a geobacter (ASV371) in there, maybe two (ASV2058). Weird.

ASV1831

## let's look at the rep sequence of these weird ones when we have time.


## since our "gene length" is exactly the same for all reads, I think we 
## can interpret baseMean as abundance 

## is this true? for instance, ASV2 has baseMean of 590. Seems like this 
## would be the abundance of ASV among the samples of lowest respiration values.

 
sample_data(psNoControl)$Basal.respiration

sample_variables(psNoControl)

## find some of these ASVs associated with increasing respiration - are they co-occurring? Are they where we expect them to be?

##### diffseq with land type as covariate #####


## if we want to check using covariate of land type, something like this?:

#diagdds = phyloseq_to_deseq2(psNoControl, ~ soil.respiration + Land.type) ## set "treatment" of interest
## ^ I think order matters here, covariate of interest should be last? 
## as per https://support.bioconductor.org/p/100828/

diagdds = phyloseq_to_deseq2(psNoControl, ~ Land.type + soil.respiration) ## set "treatment" of interest
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")
resultsNames(diagdds)
res <- results(diagdds)

alpha = 0.01 ## significance cutoff
res <- res[complete.cases(res),] ## necessary because not all ASVs in all landtypes
sigtab = res[(res$padj < alpha), ] ## cut off below the alpha using adjusted pvalues
sigtab$log2FoldChange

## these should ASVs that are changing with respiration 
## regardless of land type...

aa <- sigtab[ sigtab$log2FoldChange > 0, ]
bb <- aa[ aa$baseMean > 1, ]

tax_table(ps)[row.names(bb)]

View(sigtab)

## with this approach, the asvs that come across as most important here are:

ASV1164 Chloroflexi
ASV1734 acidobacteriota, Bryobacter
ASV2626 alphaprot, Esterales 
ASV4902 Actinbacteriota, Solirubrobacteraceae
ASV6938 Alphaproteo, Sphingomonas

## what habitat do these microbes prefer?
## plotted below, seems like only ASV1164 is useful new info

## to be sure, maybe subset (split) by land type and run? We lose
## statistical power, but could be interesting.

## we need a way to rapidly map ASVs.

## we also need to figure out if these are cooccurring

## we have a bunch of old, crude tools...start with these, and think about 
## a proper bayesian model for finding which species are responding

## the bracod paper seems very promising, even though it has zero citations...
## hasn't been updated in a year but could be worse. Interesting, it 
## focuses on the species as predictors of a host trait. That fits our 
## setup just fine, with species as predictors of respiration. Can't
## tell what the backend is. 


## the other option seems to be BORAL. This has been cited a bunch, in nature 
## etc. Looks like a good package to know about. More generalized, etc.
## but also looks like we do 
## uses old gibbs samplers, I think. Weird. 

## let's give bracod a run tomorrow, see if it is easy to set up and use

## in the meantime, how do we map the observations of an ASV? 

## should be do-able in python with the data we have:


sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 
controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
envData.drop(controls, axis='rows', inplace=True)
comData.drop(controls, axis='rows', inplace=True)
plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )
sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration',
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)
sulariPlotsDF.to_crs('EPSG:32633', inplace=True)
landColorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

fig,ax = plt.subplots()

## how can we generalize this so it can handle multiple ASVs? 

def mapOneASV(asv, ax=None, color="b", jitter=0, showLand=False):
    if ax is None: ax = plt.gca()
    jitX = sulariPlotsDF['geometry'].x.apply(lambda x: x + np.random.normal(loc=0, scale=jitter))
    jitY = sulariPlotsDF['geometry'].y.apply(lambda x: x + np.random.normal(loc=0, scale=jitter))
    asvPlotPoints = gpd.points_from_xy( jitX, jitY, crs="EPSG:32633" )
    asvGEO = gpd.GeoDataFrame(pd.concat([comData[asv], sulariPlotsDF['Land_type']], axis=1), geometry=asvPlotPoints, crs="EPSG:32633")
    if showLand: 
        asvGEO['landCols'] = [ landColorDict[i] for i in asvGEO['Land_type'] ]
    else: 
        asvGEO['landCols'] = "k"
    rasterio.plot.show(fichtelMap, ax=ax)
    asvGEO.plot(
        marker="o",
        ax=ax,
        linewidth=1,
        edgecolor=asvGEO['landCols'],
        facecolor=color,
        markersize=asvGEO[asv]*10000)
    return(asvGEO)

plt.close('all')

## then we can make maps as we see fit, with 0-inf otus.

mapOneASV("ASV2", color="b", jitter=0, showLand=True)

asv1 = mapOneASV("ASV1", color="r", jitter=0)

fig,ax = plt.subplots()

mapOneASV("ASV2", color="b", jitter=0)

mapOneASV("ASV1", color="r", jitter=0)

mapOneASV("ASV3", color="k", jitter=0)

## side note, ASV1 seems to be in a lot of plots...
## I thought this was E. coli from our mock community. Is this index bleed?

## how to check...

## this would mean diving back into the phyloseq pipeline and tracking 
## the prevalence of these MC otus...ugh, what a pain. 

## anyway, fairly low levels. If we are sticking to community level
## questions, probably not important.

## let's map some of the ASVs that look important for 

ASV1831 "Gemmataceae"         "Zavarzinella"
ASV371  "Geobacteraceae"      "Geobacter"

aa = mapOneASV("ASV1831", color="b", jitter=0)

aa = mapOneASV("ASV371", color="y", jitter=100)

aa[aa["ASV371"] > 0].index.values

envData.loc(bb)

envData.loc["S36"]
envData.loc["S70"]

## let's map the five most plentiful 
respASV = pd.read_csv("respDiffSeq.csv", index_col=0)

respASVtax = pd.read_csv("hiRespTax.csv", index_col=0)


respASV.head()

respASV.index


respASV.query("log2FoldChange > 1" ).index


## these are not so rare:

respASV.query("log2FoldChange > 1 & baseMean > 7")

## look at udeabacter


udea = mapOneASV("ASV621", color="b", jitter=0, showLand=True)

## interesting, but are any of these non-grassland species?
## subset to just forest sites, then run the deseq
## check tomorrow, have to work on some other things...

## here are some species that still appeared sensitive to 
## respiration, after accounting for land type, using deseq2 above

## ASV1164 Chloroflexi  forest
## ASV1734 acidobacteriota, Bryobacter
## ASV2626 alphaprot, Esterales 
## ASV4902 Actinbacteriota, Solirubrobacteraceae
## ASV6938 Alphaproteo, Sphingomonas

fig,ax = plt.subplots()

plt.close('all')

mapOneASV("ASV1164", color="b", jitter=0, showLand=True) ## definitely forest associated

aa = mapOneASV("ASV1734", color="b", jitter=0, showLand=True) ## definitely forest associated
aa = mapOneASV("ASV2626", color="b", jitter=0, showLand=True) ## only found in one plot above thresholds? So this probably disappears after our transformations
aa = mapOneASV("ASV4902", color="b", jitter=0, showLand=True) ## same: only found in one plot above thresholds? So this probably disappears after our transformations
aa = mapOneASV("ASV6938", color="b", jitter=0, showLand=True) ## also only one point. Oh jeez. 

(aa.iloc[:,0] > 0).sum()

## so the only the Chloroflexi from forests seems informative here.
## let's try subsetting by landtype:

## I think we need some multivariate approaches here. Networks and bayesian lms, like maybe bradco

## not sure if it will work with our current conda env?:

pip install BRACoD ## nope

## try a new env:

conda deactivate

conda create -n BRACoD python=3.6

conda activate BRACoD

pip install BRACoD

## theano not working...needs this?
conda install mkl-service

## that seems to have installed okay...test it out with sample data

## following https://github.com/ajverster/BRACoD

python

import BRACoD
import numpy as np

sim_counts, sim_y, contributions = BRACoD.simulate_microbiome_counts(BRACoD.df_counts_obesity)

sim_y ## our response variable 

contributions.shape

sim_relab = BRACoD.scale_counts(sim_counts)

help(BRACoD.run_bracod)

trace = BRACoD.run_bracod(sim_relab, sim_y, n_sample = 1000, n_burn=1000, njobs=4)

## too many cores? try defaults?
trace = BRACoD.run_bracod(sim_relab, sim_y, n_sample = 1000, n_burn=1000)


## whoah, this is slow...why?
## not working, freezes up. No real use of memory/cores


BRACoD.convergence_tests(trace, sim_relab)

conda env export > my_bracod_env.yaml


## that appears to be dead. 

## okay, that rules out bracod for the moment. Let's pick a cooccurence network 
## analysis method, try an install:

install.packages("devtools")

install.packages("BiocManager")

BiocManager::install("limma")

# Install NetCoMi
devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))


## try some sample features:

library(NetCoMi)

data("amgut1.filt")

amgut1.filt[0:10,0:10]


data("amgut2.filt.phy")

amgut2.filt.phy ## phyloseq obj


net_spring <- netConstruct(amgut1.filt,
                           filtTax = "highestFreq",
                           filtTaxPar = list(highestFreq = 50),
                           filtSamp = "totalReads",
                           filtSampPar = list(totalReads = 1000),
                           measure = "spring",
                           measurePar = list(nlambda=10, 
                                             rep.num=10,
                                             Rmethod = "approx"),
                           normMethod = "none", 
                           zeroMethod = "none",
                           sparsMethod = "none", 
                           dissFunc = "signed",
                           verbose = 2,
                           seed = 123456)



props_spring <- netAnalyze(net_spring, 
                           centrLCC = TRUE,
                           clustMethod = "cluster_fast_greedy",
                           hubPar = "eigenvector",
                           weightDeg = FALSE, normDeg = FALSE)


#?summary.microNetProps


summary(props_spring, numbNodes = 5L)

p <- plot(props_spring, 
          nodeColor = "cluster", 
          nodeSize = "eigenvector",
          title1 = "Network on OTU level with SPRING associations", 
          showTitle = TRUE,
          cexTitle = 2.3)

legend(0.7, 1.1, cex = 2.2, title = "estimated association:",
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"), 
       bty = "n", horiz = TRUE)

## or with pearson:

net_pears <- netConstruct(amgut2.filt.phy,  
                          measure = "pearson",
                          normMethod = "clr",
                          zeroMethod = "multRepl",
                          sparsMethod = "threshold",
                          thresh = 0.3,
                          verbose = 3)

## let's try it on our data.

## we can use a centered log transformation, pearson correlation as per the examples 
## on the NetCoMi github

load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

net_pears <- netConstruct(ps,  
                          measure = "pearson",
                          normMethod = "clr",
                          zeroMethod = "multRepl", ## don't understand totally...
                          sparsMethod = "threshold", ## don't understand totally...
                          thresh = 0.3,
                          verbose = 3)

## killed. To much memory required?


props_pears <- netAnalyze(net_pears, 
                          clustMethod = "cluster_fast_greedy")

plot(props_pears, 
     nodeColor = "cluster", 
     nodeSize = "eigenvector",
     title1 = "Network on OTU level with Pearson correlations", 
     showTitle = TRUE,
     cexTitle = 2.3)

legend(0.7, 1.1, cex = 2.2, title = "estimated correlation:", 
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"), 
       bty = "n", horiz = TRUE)


## can we port this over to the lab computer?

nanoComp

## let's try using the house R install:

## make a working directory:

cd /media/vol1/daniel/sulariArne/soilAnalysis


#library('DESeq2')

if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install('phyloseq')

library('phyloseq')

setwd("/media/vol1/daniel/sulariArne/soilAnalysis")

download.file("https://github.com/danchurch/fichtelgebirgeSoils/raw/main/sulariData/sulariPhyloseqObject.rda", destfile="sulariPhyloseqObject.rda")

load("sulariPhyloseqObject.rda")

ps ## looks ok


BiocManager::install("limma")

BiocManager::install("zCompositions")

install.packages("devtools") 

devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))


## dev.tools failed. 
## the following packages failed. Work on it tomorrow. 

install.packages("systemfonts")
## which needs...
sudo apt install libfontconfig1-dev

install.packages(       "xml2")
## which needs...
sudo apt install libxml2-dev

install.packages("textshaping")
## which needs...
sudo apt install libharfbuzz-dev libfribidi-dev

install.packages(  "rversions") ## easy
install.packages( "urlchecker") ## easy

install.packages(    "openssl")
## which needs
sudo apt install libssl-dev

install.packages(       "ragg")
## which needs
sudo apt install libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev

install.packages("credentials") ## easy
install.packages(      "httr2") ## easy
install.packages(       "httr") ## easy
install.packages(       "gert") ## easy
install.packages(         "gh") ## easy
install.packages(    "usethis") ## easy
install.packages(    "pkgdown") ## easy
install.packages(   "roxygen2") ## easy

## and...
install.packages(   "devtools")

devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))

## failed again. Lots of failed dependencies:
‘SpiecEasi’, ‘mixedCCA’, ‘qgraph’, ‘SPRING’, ‘WGCNA’ are not available for package ‘NetCoMi’


library(devtools)
install_github("zdk123/SpiecEasi")

## which needs:

## which needs fortran ??
sudo apt install gfortran
## and the following are not there:
ld -llapack --verbose
ld -lblas --verbose
sudo apt install liblapack-dev libopenblas-dev

install.packages("mixedCCA")
install.packages("qgraph") 
install.packages("SPRING")
install.packages("WGCNA")

## try again 
devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))

library('NetCoMi') ## no errors...finally...

R

library('phyloseq')
library('NetCoMi')
setwd("/media/vol1/daniel/sulariArne/soilAnalysis")

## does the plotter work on x11 forwarding?

plot(1) ## looks okay, for base plotter

#download.file("https://github.com/danchurch/fichtelgebirgeSoils/raw/main/sulariData/sulariPhyloseqObject.rda", destfile="sulariPhyloseqObject.rda")

load("sulariPhyloseqObject.rda")

## to run our data, we want a pretty rigorous minimum abundance threshold. 
## for all of our vegan stuff, we use a minimum abundance of 50 reads 
## per observation. 

## to be clear, we want all sample counts below 50 to be come zero

dropLow <- function(x) {
                           if (x < 0) {x = 0}
                          }

dropLow <- function(x) {
                           x <- x - 50
                           if (x < 0) {x = 0}
                           x
                          }

dropLow(100)
ps.atLeast50 <- transform_sample_counts(ps.filter.fam, dropLow)

## nope, can't handle if statements. Let' pull out the matrix and work 
## on it directly:


setwd("/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis")

load("../sulariData/sulariPhyloseqObject.rda")
library('vegan')
library('phyloseq')
library('NetCoMi')

ps.atLeast50 <- ps
aa <- otu_table(ps)
bb <- aa - 50 
bb[bb < 0] <- 0
otu_table(ps.atLeast50) <- bb
## can we trim out empty otus now? 
ps.atLeast50 <- prune_taxa( taxa_sums(ps.atLeast50) > 0, ps.atLeast50 )

otu_table(ps)[0:10,0:10]

otu_table(ps.atLeast50)[0:10,0:10]

dim(otu_table(ps)) 
dim(otu_table(ps.atLeast50)) ## down to just 4280 taxa, out of 36140

## ok, looks right
## now pass that to network software:

## just curious, can it handle the full ps? want to see how this changes the network.

#net_pears_fullPS <- netConstruct(ps,  
#                          measure = "pearson",
#                          normMethod = "clr",
#                          zeroMethod = "multRepl", ## don't understand totally...
#                          sparsMethod = "threshold", ## don't understand totally...
#                          thresh = 0.3,
#                          verbose = 3)

## nope, dies

## try smaller object:

net_pears_PSatleast50 <- netConstruct(ps.atLeast50,  
                          measure = "pearson",
                          normMethod = "clr",
                          zeroMethod = "multRepl", ## don't understand totally...
                          sparsMethod = "threshold", ## don't understand totally...
                          thresh = 0.3,
                          verbose = 3)

## that was computationally expensive:
save(net_pears_PSatleast50, file = "net_pears_PSatleast50.rda") ## 139 mb, pretty darn big..

props_pears <- netAnalyze(net_pears_PSatleast50, 
                          clustMethod = "cluster_fast_greedy")


?netConstruct

## huh, we can use a custom count matrix

## which means we can add columns. 
## so let's make a matrix that includes land-type, and 

?netAnalyze

## not run
plot(props_pears, 
     nodeColor = "cluster", 
     nodeSize = "eigenvector",
     title1 = "Network on OTU level with Pearson correlations", 
     showTitle = TRUE,
     cexTitle = 2.3)

legend(0.7, 1.1, cex = 2.2, title = "estimated correlation:", 
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"), 
       bty = "n", horiz = TRUE)

## plan for the day:

## rerun network analysis with environment variables land-use and respiration

## if it works, update notebook with it:



### on another note, we need to run jupyter on the lab computer...

## means 
## - install jupyter
## - clone repo to lab comp
##  maybe best to do this as we did on my own computer, within a conda env

## can we use our current spatialDirt yaml for this?

conda env export > spatialDirt_15.4.24.yaml

## update repo, clone onto lab
## on the lab comp, try to use it to make a new environment:

cd /media/vol1/daniel/sulariArne/soilAnalysis

wget https://raw.githubusercontent.com/danchurch/fichtelgebirgeSoils/main/spatialDirt_15.4.24.yaml 

conda update -n base conda

conda install -n base conda-libmamba-solver

conda config --set solver libmamba

## get the standard channels

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

conda env create --name spatialDirt --file=spatialDirt_15.4.24.yaml

## lots of errors...let's see...installing kernels as above

## is the R kernel on there?

## try: 
## https://stackoverflow.com/questions/44056164/jupyter-client-has-to-be-installed-but-jupyter-kernelspec-version-exited-wit

## in a sudo R sesh on lab comp:

install.packages('IRkernel')

system.file('kernelspec', package = 'IRkernel')

## gives us:
"/usr/local/lib/R/site-library/IRkernel/kernelspec"


## which we can give to our notebook
jupyter kernelspec install "/usr/local/lib/R/site-library/IRkernel/kernelspec" \
  --name "R" \
  --user 

jupyter kernelspec list

cd /media/vol1/daniel/miniconda3/envs/spatialDirt



## as per this site:
https://stackoverflow.com/questions/69244218/how-to-run-a-jupyter-notebook-through-a-remote-server-on-local-machine

jupyter notebook --no-browser --port=8080

ssh -L 8080:localhost:8080 test@132.180.112.115

## almost works. wants passwords and tokens and stuff:
https://jupyter-server.readthedocs.io/en/latest/operators/public-server.html

## for now use tokens...


## we need to synch up with git

## on the nanocomp

## as per above, the nanocomp computer repo needs 
## to be given permissions, etc.

cd /media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis

git config --global user.email "danchurchthomas@gmail.com"

git config --global user.name "danchurch"

git remote add origin https://github.com/danchurch/fichtelgebirgeSoils.git
git branch -M main
git remote set-url origin git@github.com:danchurch/fichtelgebirgeSoils.git

git push -u origin main

## nope, don't think I put keys for lab comp on there

cd ~/.ssh
ssh-keygen -t rsa -f nanoComp2git

## and as per here:
https://stackoverflow.com/questions/13509293/git-fatal-could-not-read-from-remote-repository
## the ssh-agent had to be started up and informed about the new key:

eval `ssh-agent -s`
ssh-add ~/.ssh/nanoComp2git

## and this had to be added to the bashrc. weird. never had to do that before...

## anyway, github seems synched. 
## and does that mean we can carefully work remotely with the jupyter notebook now?

## when we actually get comfortable with this, we can get back to editing this
## text file local. For now, both vim and jupyter are on the nanocomp computer:


cd /media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis

conda activate spatialDirt 

jupyter notebook --no-browser --port=8080

## run this to activate the tcp forwarding
ssh -L 8080:localhost:8080 test@132.180.112.115

## then open browser to: 
http://localhost:8080/notebooks/spatialAnalysisSulariData.ipynb

## it's going to be confusing working on both computers. 
## just slow it down, save/pull/push.

## calculating the network statistics takes for ever with netcommi

## can we multithread? its only using one core...


## I think we need to revisit the network software 

## we need some how a vector of the ASVs most responsive to 
## respiration, added to a cooccurrence matrix.

## we need to mkae our own adjacency matrix, I guess. 

## anyway, to get the candidates for responsiveness to 
## respiration...seems like we can't avoid a full multivariate
## treatment of the community matrix?

## let's take a look at the BORAL tool...

##### boral install and test ####

## boral github repo is here:
https://github.com/emitanaka/boral

## try the install on local machine:

install.packages('boral') 

install.packages('R2jags') 

install.packages('rjags') 

sudo apt install jags ## and work back up

library(boral)

## and pretty much zero documentation...

#### HMSC install and test #####

## let's try HMSC package:

install.packages("devtools") # if not yet installed
install.packages("usethis") # if not yet installed

library(devtools)

install_github("hmsc-r/HMSC")

## cran could also work, for stable version. 

## there is a book. Maybe we order it? 

## but for the moment, try to find some online examples?

## they have a series of vignettes they want you to work through

https://cran.r-project.org/web/packages/Hmsc/index.html

## start with the first:

https://cran.r-project.org/web/packages/Hmsc/vignettes/vignette_1_univariate.pdf


library(Hmsc)

set.seed(1)

## make a simple linear model:
## using a maximum likelihood fit:

n = 50
x = rnorm(n)
alpha = 0
beta = 1
sigma = 1
L = alpha + beta*x
y = L + rnorm(n, sd = sigma)

plot(x, y, las=1)


df = data.frame(x,y)
m.lm = lm(y ~ x, data=df)
summary(m.lm)

## compare this to HMSC

Y = as.matrix(y)

XData = data.frame(x = x)

m = Hmsc(Y = Y, XData = XData, XFormula = ~x)

nChains = 2
thin = 5
samples = 1000
transient = 500*thin
verbose = 500*thin

?sampleMcmc

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
nChains = nChains, verbose = verbose)

mpost = convertToCodaObject(m)

summary(mpost$Beta)

preds = computePredictedValues(m)

?computePredictedValues

evaluateModelFit(hM=m, predY=preds)

plot(mpost$Beta)

effectiveSize(mpost$Beta)

## if these indicators are close to one, the 
## chains behaved similarly during sampling
gelman.diag(mpost$Beta,multivariate=FALSE)$psrf


## evaluating at a maxlikelihood linear model:

nres.lm = rstandard(m.lm)
preds.lm = fitted.values(m.lm)
par(mfrow=c(1,2))
hist(nres.lm, las = 1)
plot(preds.lm,nres.lm, las = 1)
abline(a=0,b=0)

plot(m.lm)

## checking out a bayesian lm:

preds.mean = apply(preds, FUN=mean, MARGIN=1)

nres = scale(y-preds.mean)

par(mfrow=c(1,2))

hist(nres, las = 1)
plot(preds.mean,nres, las = 1)

abline(a=0,b=0)

## generalized linear models (link function, different dists) possible, just a couple
## for univariate models:

y = 1*(L+ rnorm(n, sd = 1)>0)

plot(x,y, las = 1)

### example hierachical model

## 100 samples, 10 plots, one response variable, checking for plot effects
## additive effects, in other words looking for different intercepts
## due to the different plots:

## make fake data:
n = 100
x = rnorm(n)
alpha = 0
beta = 1
sigma = 1
L = alpha + beta*x
np = 10
sigma.plot = 1
sample.id = 1:n
plot.id = sample(1:np, n, replace = TRUE)
ap = rnorm(np, sd = sigma.plot)
a = ap[plot.id]
y = L + a + rnorm(n, sd = sigma)
plot.id = as.factor(plot.id)
plot(x,y,col = plot.id, las = 1)
XData = data.frame(x = x)
Y = as.matrix(y)
studyDesign = data.frame(sample = as.factor(sample.id), plot = as.factor(plot.id))
rL = HmscRandomLevel(units = studyDesign$plot)
m = Hmsc(Y=Y, XData=XData, XFormula=~x,
       studyDesign=studyDesign, ranLevels=list("plot"=rL))
m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
                nChains = nChains, nParallel = nChains, verbose = verbose)
preds = computePredictedValues(m)
MF = evaluateModelFit(hM=m, predY=preds)
MF$R2 ## ~.7, keeps changing
## that is classical R2, I guess, all data used to make the model
## and all data used to evaluate

## they offer rapid cross-validation predictive R2 
partition = createPartition(m, nfolds = 2, column = "sample")

partition

## not sure how this works, but model is refit using the likelihood from half 
## the data I guess:
preds = computePredictedValues(m, partition = partition, nParallel = nChains)

## seems like this would create two diff posteriors? no way, too weird. I just don't
## understand

MF = evaluateModelFit(hM = m, predY = preds)
MF$R2 ## ~0.7 still pretty good



## spatial autorrelation can be incorporated as an additional variable

## make a random dataset with y-variable autocorrelation that does
## not correspond with autocorrelation in x:

sigma.spatial = 2
alpha.spatial = 0.5
sample.id = rep(NA,n)
for (i in 1:n){
sample.id[i] = paste0("location_",as.character(i))
}
sample.id = as.factor(sample.id)
xycoords = matrix(runif(2*n), ncol=2)
rownames(xycoords) = sample.id
colnames(xycoords) = c("x-coordinate","y-coordinate")
a = MASS::mvrnorm(mu=rep(0,n),
Sigma = sigma.spatial^2*exp(-as.matrix(dist(xycoords))/alpha.spatial))
y = L + a + rnorm(n, sd = sigma)
Y=as.matrix(y)
colfunc = colorRampPalette(c("cyan", "red"))
ncols = 100
cols = colfunc(100)
par(mfrow=c(1,2))
for (i in 1:2){
if (i==1) value = x
if (i==2) value = y
value = value-min(value)
value = 1+(ncols-1)*value/max(value)
plot(xycoords[,1],xycoords[,2],col=cols[value],pch=16,main=c("x","y")[i], asp=1)
}

studyDesign = data.frame(sample = sample.id)

## tell the model we are expecting a spatial level to our model with sData setting?
rL = HmscRandomLevel(sData = xycoords)

m = Hmsc(Y=Y, XData=XData, XFormula=~x,
         studyDesign=studyDesign, ranLevels=list("sample"=rL))

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
         nChains = nChains, nParallel = nChains, verbose = verbose)


## not sure how they hande the estimation of the spatial effects,
## need to check under the hood a bit. But also need to keep moving with this
## analysis. 

## the second tutorial: simple multivariate data:

https://cran.r-project.org/web/packages/Hmsc/vignettes/vignette_2_multivariate_low.pdf

library(Hmsc)
library(corrplot)

set.seed(1) 

## five species example:

n = 100
x1 = rnorm(n)
x2 = rnorm(n)
XData = data.frame(x1=x1,x2=x2)
alpha = c(0,0,0,0,0)
beta1 = c(1,1,-1,-1,0)
beta2 = c(1,-1,1,-1,0)
sigma = c(1,1,1,1,1)

L = matrix(NA,nrow=n,ncol=5)

Y = matrix(NA,nrow=n,ncol=5)

for (j in 1:5){
L[,j] = alpha[j] + beta1[j]*x1 + beta2[j]*x2
Y[,j] = L[,j] + rnorm(n, sd = sigma[j])
}

## build model

m = Hmsc(Y = Y, XData = XData, XFormula = ~x1+x2)

nChains = 2
thin = 10
samples = 1000
transient = 500*thin
verbose = 0

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
               nChains = nChains, nParallel = nChains, verbose = verbose)

mpost = convertToCodaObject(m)

## check for convergence 
effectiveSize(mpost$Beta)

gelman.diag(mpost$Beta, multivariate=FALSE)$psrf
## sample sizes should 

par(mfrow=c(1,2))
hist(effectiveSize(mpost$Beta), main="ess(beta)")
hist(gelman.diag(mpost$Beta, multivariate=FALSE)$psrf, main="psrf(beta)")

preds = computePredictedValues(m)

evaluateModelFit(hM = m, predY = preds)

partition = createPartition(m, nfolds = 2)

preds = computePredictedValues(m, partition = partition, nParallel = nChains)

evaluateModelFit(hM = m, predY = preds)

postBeta = getPostEstimate(m, parName = "Beta")

plotBeta(m, post = postBeta, param = "Support", supportLevel = 0.95)

## so we can pick out the individual species response to environmental parameter
## in a statistically sound way, no need for pvalue correction, etc.
## this is a great improvement...

### including species-species interations 

## biotic interactions are included as latent variables in model, using 
## a species-species covariance matrix

studyDesign = data.frame(sample = as.factor(1:n))

rL = HmscRandomLevel(units = studyDesign$sample)

m = Hmsc(Y = Y, XData = XData, XFormula = ~x1+x2,
      studyDesign = studyDesign, ranLevels = list(sample = rL))

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
       nChains = nChains, nParallel = nChains, verbose = verbose)

mpost = convertToCodaObject(m)

par(mfrow=c(2,2))
hist(effectiveSize(mpost$Beta), main="ess(beta)")
hist(gelman.diag(mpost$Beta, multivariate=FALSE)$psrf, main="psrf(beta)")
hist(effectiveSize(mpost$Omega[[1]]), main="ess(omega)")
hist(gelman.diag(mpost$Omega[[1]], multivariate=FALSE)$psrf, main="psrf(omega)")

postBeta = getPostEstimate(m, parName="Beta")
plotBeta(m, post=postBeta, param="Support", supportLevel = 0.95)
## you would expect these to be the same as above

## and but now you also have a posterior for a latent variable 
## that is describing species-species interactions

OmegaCor = computeAssociations(m)
supportLevel = 0.95
toPlot = ((OmegaCor[[1]]$support>supportLevel)
   + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean

corrplot(toPlot, method = "color",
         col = colorRampPalette(c("blue","white","red"))(200),
         title = paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))


## and there is no correlation between species abundances,
## because the data was not generated with any covariances
## among the species abundances.

## make a simpler model, drop one covariate...what happens?

m = Hmsc(Y=Y, XData=XData, XFormula=~x1,
         studyDesign=studyDesign, ranLevels=list(sample=rL))

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
               nChains = nChains, nParallel = nChains, verbose = verbose)

postBeta = getPostEstimate(m, parName="Beta")

plotBeta(m, post=postBeta, param="Support", supportLevel = 0.95)

OmegaCor = computeAssociations(m)

?computeAssociations

supportLevel = 0.95

toPlot = ((OmegaCor[[1]]$support>supportLevel)
         + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean

corrplot(toPlot, method = "color",
         col=colorRampPalette(c("blue","white","red"))(200),
         title=paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))

## the resulting checkerboard shows what happens when an important 
## predictor is left out. The residual covariance after that is left
## after the remaining environmental predictor is incorporated into 
## the latent variable that is intended to represent biotic interactions.
## So some species are covarying, but only because they both "like" 
## environmental conditions that weren't in the model, not because 
## they are in symbioses etc. This shows the limitations of this kind of analysis. 

## explanatory R2:
preds = computePredictedValues(m)

evaluateModelFit(hM = m, predY = preds)

## the latent variable is actually doing a good job of 

## predictive/cross-validated R2
## making up the missing predictor. 
oo




## but this breaks down when you split the data
## for training/prediction. 
## They don't describe this in the tutorial, but
## assume this because the latent variables 
## are given uninformative priors and the 
## likelihood isn't weighted as much with the 
## smaller amound of data/evidenc.

preds = computePredictedValues(m, partition = partition, nParallel = nChains)

evaluateModelFit(hM = m, predY = preds)

## possible to do predictions of a species, especially
## if you have data from the other species. So cross validation
## leaving one species out at a time also possible:

preds = computePredictedValues(m, partition=partition,
      partition.sp=c(1,2,3,4,5), mcmcStep=10, nParallel = nChains)


## this is mostly for when you have a small species matrix

##### Ordinations ####

rL$nfMin=2
rL$nfMax=2

m = Hmsc(Y=Y, XData=XData, XFormula=~1,
    studyDesign=studyDesign, ranLevels=list(sample=rL))
m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
    nChains = nChains, nParallel = nChains, verbose = verbose)

etaPost=getPostEstimate(m, "Eta")
lambdaPost=getPostEstimate(m, "Lambda")

biPlot(m, etaPost = etaPost, lambdaPost = lambdaPost, factors = c(1,2), "x2")

## interesting, but my results don't match the tutorial, so not sure how to 
## interpret

## essentially, the idea is similar to eigen-decomposition/PCA type 
## analysis, but the method is different. 

## they just introduced two vaguely defined variables the explain the 
## general mass of the data, without any previous hypotheses
## stated, but what are the priors and likelihoods on these? don't 
## understand. Some sort of gaussian or dirichilet process I guess. Beyond me. 

## then these two latent variables act pretty much like when we create 
## a composite variable using PCAs

## we subtract the variance we can explain from environmental conditions
## and biotic interactions by giving these to the model, and the remaining
## residual variation only is explained by these 

## you can mix up the link functions and residual models:

nChains = 2
thin = 10
samples = 1000
transient = 500*thin
verbose = 0
set.seed(2)
n = 100
x1 = rnorm(n)
x2 = rnorm(n)
alpha = c(0,0,0,0)
beta1 = c(1,1,-1,-1)
beta2 = c(1,-1,1,-1)
sigma = c(1,NA,NA,1)
XData = data.frame(x1=x1,x2=x2)
L = matrix(NA,nrow=n,ncol=4)
Y = matrix(NA,nrow=n,ncol=4)
for (j in 1:4){
  L[,j] = alpha[j] + beta1[j]*x1 + beta2[j]*x2
  }
Y[,1] = L[,1] + rnorm(n, sd = sigma[1])
Y[,2] = 1*((L[,2] + rnorm(n, sd = 1))>0)
Y[,3] = rpois(n, lambda = exp(L[,3]))
Y[,4] = rpois(n, lambda = exp(L[,4] + rnorm(n, sd = sigma[4])))

m = Hmsc(Y = Y, XData = XData, XFormula = ~x1+x2,
       distr = c("normal","probit","poisson","lognormal poisson"))

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
    nChains = nChains, nParallel = nChains, verbose = verbose)



mpost = convertToCodaObject(m)
effectiveSize(mpost$Beta)
gelman.diag(mpost$Beta, multivariate=FALSE)$psrf

preds = computePredictedValues(m, expected = FALSE)

evaluateModelFit(hM = m, predY = preds)

postBeta = getPostEstimate(m, parName="Beta")

plotBeta(m, post=postBeta, param="Support", supportLevel = 0.95)

## cool, works. But not sure how we would assign priors/likelihood models for
## all of the thousand species we're going to examine...
## maybe in the next tutorial:

## HSCMC tutorial #3:
## https://cran.r-project.org/web/packages/Hmsc/vignettes/vignette_3_multivariate_high.pdf

library(Hmsc)
library(corrplot)
library(ape)
library(MASS)
library(fields) ## for image.plot?
library(knitr)
set.seed(1)

ns = 50
## this will incorporate phylogeny, which we will randomly make up:
phy = ape::rcoal(n=ns, tip.label = sprintf('species_%.3d',1:ns), br = "coalescent")
plot(phy, show.tip.label = FALSE, no.margin = TRUE)

## model two traits - forest preference and thermal optimum
## assume there is a phylogenetic effect - more closely related 
## species are more likely to have the same preference for forest
## or more similar thermal optimum:

## make up the data so, this hurts my poor little brain:

C = vcv(phy, model = "Brownian", corr = TRUE) ## ape function for traits that are evolving
spnames = colnames(C)
traits = matrix(NA,ncol =2,nrow = ns)
## fill in the traits,  mvrnorm from MASS package
for (i in 1:2){
    traits[,i] = matrix(mvrnorm(n = 1, mu = rep(0,ns), Sigma=C))
    }
rownames(traits) = spnames
colnames(traits) = c("habitat.use","thermal.optimum")
traits = as.data.frame(traits)
par(fig = c(0,0.6,0,0.8), mar=c(6,0,2,0))
plot(phy, show.tip.label = FALSE)
par(fig = c(0.6,0.9,0.025,0.775), mar=c(6,0,2,0), new=T)
plot.new()
image.plot(t(traits),axes=FALSE,legend.width = 3,legend.shrink=1,
#imagePlot(t(traits),axes=FALSE,legend.width = 3,legend.shrink=1,
col = colorRampPalette(c("blue","white","red"))(200))
text(x=1.1, y=0.72, srt = 90, "H", cex=0.9, pos = 4)
text(x=1.4, y=0.72, srt = 90, "T", cex=0.9, pos = 4)

## neat. maybe do something similar with respiration and forest/grassland

## make some environmental and community data:

n = 200
habitat = factor(sample(x = c("forest","open"), size = n, replace=TRUE))
climate = rnorm(n)
nc = 4
mu = matrix(0,nrow=nc,ncol=ns)
#expected niche of each species related to the "covariate" intercept
mu[1, ] = -traits$thermal.optimum^2/4-traits$habitat.use
#expected niche of each species related to the covariate forest
#(open area as reference level, so included in intercept)
mu[2, ] = 2*traits$habitat.use
#expected niche of each species related to the covariate climate
mu[3, ] = traits$thermal.optimum/2
#expected niche of each species related to the covariate climate*climate
mu[4, ] = -1/4
beta = mu + 0.25*matrix(rnorm(n = ns*nc), ncol=ns)
X = cbind(rep(1,ns), as.numeric(habitat=="forest"), climate, climate*climate)
L = X%*%beta
Y = L + mvrnorm(n=n, mu=rep(0,ns), Sigma=diag(ns))
colnames(Y) = spnames


## didn't really understand all of the code, but the goal 
## is to create a community matrix of species that 
## affected by the two traits. 

Y[1:5,1:5] ## species are columns

## build a model that includes:
## the climate and habitat data, 
## a climate^2 term (allows for unimodal, non-linear climate niche curve)
## also a term for the phylogenetic signal

XData = data.frame(climate = climate, habitat = habitat)
XFormula = ~habitat + poly(climate,degree = 2,raw = TRUE)
TrFormula = ~habitat.use + thermal.optimum
studyDesign = data.frame(sample = sprintf('sample_%.3d',1:n), stringsAsFactors=TRUE)
rL = HmscRandomLevel(units = studyDesign$sample)
rL$nfMax = 15
m = Hmsc(
                Y = Y, 
            XData = XData, 
         XFormula = XFormula,
           TrData = traits, 
        TrFormula = TrFormula,
        phyloTree = phy,
      studyDesign = studyDesign, 
        ranLevels = list(sample = rL))


## not run - apparently this will take 2 hours.
## lets set up the labcomputer for this.

m = Hmsc(Y = Y, XData = XData, XFormula = XFormula,
         TrData = traits, TrFormula = TrFormula,
         phyloTree = phy,
         studyDesign = studyDesign, ranLevels = list(sample = rL))

nChains = 2
thin = 10
samples = 1000
transient = 500
verbose = 0

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
               nChains = nChains, nParallel = nChains, verbose = verbose)

## that actually ran for like ten minutes

## so now we have posterior probability for the community

## they look for four parameters:

## species niches (environmental predictors of species abundance)
## influence of traits on species niches (organismal traits predicting their response to environment)
## species-species interactions 
## effect of phylogeny

## the omega (s x s) matrix is 50 x 50 x 2500 elements. The 2500 is from the traces of the posterior, I think
## but I thought that part of the point of latent variables was to avoid building this massive matrix?

## anyway...figure out later. They subset to one 100 random species pairs

## check convergences

mpost = convertToCodaObject(m)

par(mfrow=c(3,2))
ess.beta = effectiveSize(mpost$Beta)
psrf.beta = gelman.diag(mpost$Beta, multivariate=FALSE)$psrf
hist(ess.beta)
hist(psrf.beta)
ess.gamma = effectiveSize(mpost$Gamma)
psrf.gamma = gelman.diag(mpost$Gamma, multivariate=FALSE)$psrf
hist(ess.gamma)
hist(psrf.gamma)
sppairs = matrix(sample(x = 1:ns^2, size = 100))
tmp = mpost$Omega[[1]]

for (chain in 1:length(tmp)){
tmp[[chain]] = tmp[[chain]][,sppairs]
}

ess.omega = effectiveSize(tmp)
psrf.omega = gelman.diag(tmp, multivariate=FALSE)$psrf
hist(ess.omega)
hist(psrf.omega)

print("ess.rho:")
effectiveSize(mpost$Rho)

print("psrf.rho:")
gelman.diag(mpost$Rho)$psrf

## convergence diagnostics look fine

## overall performance of the model, explanatory.
## for this, check the R2 on average for all 
## species predictions:

preds = computePredictedValues(m)
MF = evaluateModelFit(hM=m, predY=preds)

hist(MF$R2, xlim = c(0,1), main=paste0("Mean = ", round(mean(MF$R2),2)))

## hovering around .70  not bad

m$X

head(m$X)


## we can use our model to do Variance Partitioning:

VP = computeVariancePartitioning(m, group = c(1,1,2,2), groupnames = c("habitat","climate"))

?computeVariancePartitioning

plotVariancePartitioning(m, VP = VP)

kable(VP$R2T$Beta)

## we can get back to the environmental predictors, on a per-species basis: 
postBeta = getPostEstimate(m, parName = "Beta")

plotBeta(m, post = postBeta, param = "Support",
plotTree = TRUE, supportLevel = 0.95, split=.4, spNamesNumbers = c(F,F))

postGamma = getPostEstimate(m, parName = "Gamma")

plotGamma(m, post=postGamma, param="Support", supportLevel = 0.95)

OmegaCor = computeAssociations(m)

## what are these these objects?

str(OmegaCor)

str(OmegaCor[[1]][1])

## significance is judged by a "support" metric, not sure what this is
str(OmegaCor[[1]][2])

supportLevel = 0.95 ## what is this? some sort of credible interval?

toPlot = ((OmegaCor[[1]]$support>supportLevel)
        + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean

corrplot(toPlot, method = "color",
         col=colorRampPalette(c("blue","white","red"))(200),
         tl.cex=.6, tl.col="black",
         title=paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))

## no associations, because synthetic data, not included

summary(mpost$Rho)

## we can predict what a community might do over an environmental gradient:

Gradient = constructGradient(m,focalVariable = "climate",
                   non.focalVariables = list("habitat"=list(3,"open")))

Gradient$XDataNew

predY = predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew,
                ranLevels=Gradient$rLNew, expected=TRUE)

plotGradient(m, Gradient, pred=predY, measure="S", showData = TRUE)


## individual species response can be modeled:

par(mfrow=c(1,2))
plotGradient(m, Gradient, pred=predY, measure="S", index = 1, showData = TRUE)
plotGradient(m, Gradient, pred=predY, measure="Y", index = 2, showData = TRUE)

## trait values of the group changing with a gradient:
plotGradient(m, Gradient, pred=predY, measure="T", index = 3, showData = TRUE)


## "gradients" can be constructed for categorical variables?:

Gradient = constructGradient(m,focalVariable = "habitat",
                             non.focalVariables = list("climate"=list(1)))

Gradient$XDataNew

## ah, I get it. This is like a parameter sweep, where we hold everything else
## constant and just change the variable of interest.

## so in this case we have a trait for "preference for forest". We can select the species most 
## responsive to habitat and watch its response to being in either a forest or meadow:

predY = predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew,
                ranLevels=Gradient$rLNew, expected=TRUE)

plotGradient(m, Gradient, pred=predY, measure="Y", index=which.max(m$TrData$habitat.use),
             showData = TRUE, jigger = 0.2)

## predicted to drop, make sense.

## and we can see how the community mean for this trait will drop with a habitat-change:

plotGradient(m, Gradient, pred=predY, measure="T", index=2, showData = TRUE, jigger = 0.2)

## same story. The number of microbes with the "forest-preference trait" will increase
## when the community is subject to forests.

## to get closer to reality, they give the example of trying to model the simulated 
## community without all the predictors that were used to make the data
## so it's like real life, when we are hypothesizing what is driving the community
## dynamics, but don't really know.

## for instance, trying to explain the community composition with climate only:

XFormula.1 = ~poly(climate, degree = 2, raw = TRUE)
ma50 = Hmsc(Y=Y, XData=XData, XFormula = XFormula.1,
            TrData = traits, TrFormula = TrFormula,
            phyloTree = phy,
            studyDesign=studyDesign, ranLevels=list(sample=rL))

ma50 = sampleMcmc(ma50, thin = thin, samples = samples, transient = transient,
            nChains = nChains, nParallel = nChains, verbose = verbose)

print ("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! done !!!!!!!!!!!!!!!!")
print ("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! done !!!!!!!!!!!!!!!!")
print ("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! done !!!!!!!!!!!!!!!!")

## and so we have a lot more variance from randome effects

VP = computeVariancePartitioning(ma50, group = c(1,1,1), groupnames=c("climate"))
plotVariancePartitioning(ma50, VP = VP)

## which is now ascribed partially due to species associations, because 

OmegaCor = computeAssociations(ma50)

supportLevel = 0.95
toPlot = ((OmegaCor[[1]]$support>supportLevel)
  + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean

corrplot(toPlot, method = "color",
col=colorRampPalette(c("blue","white","red"))(200),
tl.cex=.6, tl.col="black",
title=paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))


## and other stuff about traits, controlling shrinkage on the hierarchical model 
## etc, etc. But generally, got the idea. 

## there is another tutorial, about spatial datasets. Seems pertinent:

library(Hmsc)
library(MASS)

set.seed(6)

## 100 sites, 5 species:

n = 100
ns = 5
beta1 = c(-2,-1,0,1,2)
alpha = rep(0,ns)
beta = cbind(alpha,beta1)
x = cbind(rep(1,n),rnorm(n))
Lf = x%*%t(beta)
xycoords = matrix(runif(2*n),ncol=2)
colnames(xycoords) = c("x-coordinate","y-coordinate")
rownames(xycoords) = 1:n

## exponentially decreasing autocorrelation model:
sigma.spatial = c(2)
alpha.spatial = c(0.35)
Sigma = sigma.spatial^2*exp(-as.matrix(dist(xycoords))/alpha.spatial)
eta1 = mvrnorm(mu=rep(0,n), Sigma=Sigma)
lambda1 = c(1,2,-2,-1,0) ## species spatial residuals
Lr = eta1%*%t(lambda1) ##  
L = Lf + Lr ## linear function of effects of environmental and spatial function
y = as.matrix(L + matrix(rnorm(n*ns),ncol=ns))
yprob = 1*((L +matrix(rnorm(n*ns),ncol=ns))>0)
XData = data.frame(x1=x[,2])

rbPal = colorRampPalette(c('cyan','red'))
par(mfrow=c(2,3))
Col = rbPal(10)[as.numeric(cut(x[,2],breaks = 10))]
plot(xycoords[,2],xycoords[,1],pch = 20,col = Col,main=paste('x'), asp=1)
for(s in 1:ns){
    Col = rbPal(10)[as.numeric(cut(y[,s],breaks = 10))]
    plot(xycoords[,2],xycoords[,1],pch = 20,col = Col,main=paste('Species',s), asp=1)
    }

## this diagram is useful for showing that autocorrelation is hard to pick up by eye 
## sometimes

####

## make a model spatial by adding a random effect with a spatial argument
 
studyDesign = data.frame(sample = as.factor(1:n))
rL.spatial = HmscRandomLevel(sData = xycoords) ## here
rL.spatial = setPriors(rL.spatial,nfMin=1,nfMax=1) #We limit the model to one latent variables for visualization
m.spatial = Hmsc(Y=yprob, XData=XData, XFormula=~x1,
                 studyDesign=studyDesign, ranLevels=list("sample"=rL.spatial),distr="probit")

## sample:
nChains = 2
thin = 10
samples = 1000
transient = 1000
verbose = 1

m.spatial = sampleMcmc(m.spatial, thin = thin, samples = samples, transient = transient,
                       nChains = nChains, nParallel = nChains, verbose = verbose,
                       updater=list(GammaEta=FALSE))

## they skip checking the convergences

#Explanatory power

preds.spatial = computePredictedValues(m.spatial)

MF.spatial = evaluateModelFit(hM=m.spatial, predY=preds.spatial)

?evaluateModelFit

MF.spatial

partition = createPartition(m.spatial, nfolds = 2, column = "sample")

cvpreds.spatial = computePredictedValues(m.spatial, partition=partition,
                          nParallel = nChains, updater=list(GammaEta=FALSE))

mpost.spatial = convertToCodaObject(m.spatial)

plot(mpost.spatial$Alpha[[1]])

mpost.spatial = convertToCodaObject(m.spatial)

plot(mpost.spatial$Alpha[[1]])

summary(mpost.spatial$Alpha[[1]])

## try the model without the spatial component (so no random effects)
m = Hmsc(Y=yprob, XData=XData, XFormula=~x1, studyDesign = studyDesign, distr="probit")

m = sampleMcmc(m, thin = thin, samples = samples, transient = transient,
                nChains = nChains, nParallel = nChains, verbose = verbose)


preds = computePredictedValues(m)

MF = evaluateModelFit(hM=m, predY=preds)

MF

partition = createPartition(m, nfolds = 2, column = "sample")
preds = computePredictedValues(m, partition=partition, nParallel = nChains)

MF = evaluateModelFit(hM=m, predY=preds)

MF



### ok great...anything else we need to know before we dive into the real data?

## don't think so. 

## first step, what format do we want our data in?

## should use the >50, log-transformed data. 

## we have a lot of predictors. Model might get too
## complex. 

## maybe start without spatial effects, find best predictors,
## then include the spatial effects

####### try out HMSC on sulari data ##############

## still on lab comp

R

library(phyloseq)
library(vegan)
library(Hmsc)
library(corrplot)
library(ape)
library(MASS)
library(fields) 
library(knitr)

## sulari  community data:
comData <- read.csv("../sulariData/comdat.csv", row.names=1)
envData <- read.csv("sulariEnv.csv", row.names=1)

tail(comData)[,1:5]
tail(envData)[,1:5]

## let's just start working through the high-dim tutorial, using our 
## data. I think we need a 16s tree of our sequences...

## we need to get the reduced ~5000 ASV sequences out into fasta form:


load("../sulariData/sularilogMin50ps.rda")
## get rid of zero columns
logMin50ps = prune_taxa( taxa_sums(logMin50ps) > 0, logMin50ps )

## export the sequences as a fasta file for the aligner:

?Biostrings::writeXStringSet

Biostrings::writeXStringSet(refseq(logMin50ps), "sulariAbundantASV16s.fna", append=FALSE,
                                  compress=FALSE, compression_level=NA, format="fasta")

## align them with ssu-aligner:

## try it on the local desktop

conda activate spatialDirt

#conda install bioconda::ssu-align

## align these with SSU align
## as per https://www.biostars.org/p/11377/, 
## to produce a single tree from both archea and bact,
## we need to designate a single model:

ssu-align -n bacteria sulariAbundantASV16s.fna sulariAbundantASV16s_ali
## then a strict mask, don't trust any ambiguous calls
ssu-mask --pf 0.9999 --pt 0.9999 sulariAbundantASV16s_ali

## to get a fasta output of the alignment (what we probably need)
ssu-mask --stk2afa sulariAbundantASV16s_ali

ssu-mask --stk2afa sulariAbundantASV16s_ali

grep ">" sulariAbundantASV16s_ali/sulariAbundantASV16s_ali.bacteria.afa | wc -l ## 4300 sequences
grep ">" sulariAbundantASV16s.fna | wc -l ## 4370 sequences

## we lost 70 sequences. Probably all of our archea. 
## not sure 
## not sure if the HMSC can handle missing asvs
## we can try anyway. 

## use fastree? phyml?

## start with phyml:

## but our alignment 

## tree building... 

## try fasttree:

mv /home/daniel/Downloads/FastTree /home/daniel/.local/bin

ssuAlignOut="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariAbundantASV16s_ali/sulariAbundantASV16s_ali.bacteria.afa"
ls $ssuAlignOut

FastTree -gtr -nt < $ssuAlignOut > sulariFastTree.nwk 

## installing arb to look at this tree:

wget http://download.arb-home.de/release/latest/arb-7.0.ubuntu2004-amd64.tgz
wget http://download.arb-home.de/release/latest/arb_install.sh

less /usr/arb/arb_UBUNTU.txt

## looks the binaries need:
sudo apt install gnuplot
sudo apt install gv
sudo apt install libmotif-common
sudo apt install xfig
sudo apt install transfig
sudo apt install xterm

sudo bash arb_install.sh

sudo bash arb_installubuntu4arb.sh

## okay, seems to run. can look at the tree that way, 
## looks like a nice tree
## no use for it right now.  

## the next step would be to see if we can get it into R:

R

library(phyloseq)
library(vegan)
library(Hmsc)
library(corrplot)
library(ape)
library(MASS)
library(fields) 
library(knitr)

setwd("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/")

aa = ape::read.tree("sulariFastTree.nwk")

plot(aa, show.tip.label = FALSE, no.margin = TRUE)

## well, we can make an ape phylo object, and a nice looking tree
## good sign. But to test this out, we need to get the 
## community data into an HMSC model   
## the asvs may cause problems.
## check tomorrow or monday, I guess.

## anyway, what's the point? The point is to find out if the 
## respiration has a phylogenetic signal, beyond species 
## level

## so keep trying to adapt the tutorials with our data

## review how they built their sim data:

## for their species abundance, use matrix with rows as sites,
## columns as species 

## not sure, should we give the pseudocounts, or what?

## honestly, I have no idea how the data are shaped 
## after log transformation. 

## the tutorial authors don't seem to care, funny. 
## they mention that the Y in their example "might"
## represent log transformed biomass. Not sure 
## what that means for us. 

## anyway, let's pretend it fits for us, keep moving.
## they are using to numerical matrices, a community 
## matrix and an environmental 

XData = data.frame(climate = climate, habitat = habitat)

XFormula = ~habitat + poly(climate,degree = 2,raw = TRUE)
TrFormula = ~habitat.use + thermal.optimum
studyDesign = data.frame(sample = sprintf('sample_%.3d',1:n), stringsAsFactors=TRUE)
rL = HmscRandomLevel(units = studyDesign$sample)
rL$nfMax = 15

m = Hmsc(Y = Y, XData = XData, XFormula = XFormula,
         TrData = traits, TrFormula = TrFormula,
         phyloTree = phy,
         studyDesign = studyDesign, ranLevels = list(sample = rL))


## our data:
comData <- read.csv("../sulariData/comdat.csv", row.names=1)
envData <- read.csv("sulariEnv.csv", row.names=1)
notControls=!(row.names(comData) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comData = comData[notControls,]
envData = envData[notControls,]

## they have a very simple set of environmental predictors

## I do not know if we can call this a zero-inflated dataset...
## there are lots of zeros...but how do you test a massive
## multivariate dataset like this?

length(comData)

dim(comData)

120*4370 ## 524,400
sum(comData > 0) ## 36,166
sum(comData == 0) ## 488,234
120*4370 ## 524,400

## so like 90% of our observations are zeros. 
## of course we introduced a lot of these 
## when we enforced a minimum abundance

## I guess we treat zeros as missing values. 
## how? 

## let's check out their R scripts...
 
threshold.prev = 0.1
threshold.abu = 0.005

dataDir="/home/daniel/Documents/projects/fichtelSoils/Odriozola_etal_2020/data"

Fungi <- read.csv(file.path(dataDir,"OtusFungi.csv"),header = TRUE,stringsAsFactors = F,sep=";")
Fungi<-Fungi[,-1] 
ny = dim(Fungi)[1]

# Keep species meeting above established prevalence and abundance thresholds

## here I think they create two community matrices, one presence
## absence and one where zeros are treated as missing samples
## the likelihood distribution for each species is then 
## a mixture model of two distributions, one that tries
## to model the creation of negative, non-observation 
## zeroes as a limit of the sampling, and real zeroes 
## that indicate rarity.  

Fungi.pa<-ifelse(Fungi>0,1,0)
Fungi.rel<-Fungi/rep(rowSums(Fungi),times=length(Fungi))

## subset the data a bit, to make it easier to deal with. 

## they use a prevalence threshold of 10% of PA sums (0.1) or greater, throwing out other PA columns
cond1=!(colSums(Fungi.pa)<=threshold.prev*ny)
## and only species contain at least one instance total relative abundance of 0.005 at a site
cond2=apply(Fungi.rel,2,max)>=threshold.abu

## after this the matrix is much smaller.
Fungidata <- Fungi[,cond1 & cond2]

Fungidata[1:5,1:5]

dim(Fungidata) ## only 452 species remain
## I'm not sure if we would do something similar. 

## but this is their community matrix:

Y <- as.matrix(Fungidata)

ny = dim(Y)[1]
ns = dim(Y)[2]


Env <- read.csv(file.path(dataDir,"env.csv"),sep=";")

FungiDepth<-log(rowSums(Fungi)) ## add in sequencing depth
Env$DecayTime<-as.factor(Env$Year)
levels(Env$DecayTime)<-list("<5"="2013","5-15"="2008","16-38"="1997",">38"="1975")

## however, think we have to fix the tree categorical variable:
Env$Tree<-as.factor(Env$Tree)

head(Env)

## 

XData<-data.frame(Env[,c(1,11,8,3,4,5,6,7)],FungiDepth)
## they take the log of the nitrogen values, why? Is this usually done?

XData$N = log(XData$N)
## standardize the diameter of the fallen wood.
## why only this variable? the rest are normalized later. 
XData$diam = scale(XData$diam)

head(XData)

## here the rest is scaled:
XData[,-c(1,2)]=scale(XData[,-c(1,2)])

head(XData)

## I think they use the trait slot for denoting 
## fungal or bacterial otu

XDataList = list()
for (j in 1:2){
   for (i in 1:ns){
      tmp = XData
      tmp$Depth = XData$FungiDepth
      XDataList[[i+(j-1)*ns]] = tmp
   }
}


XDataList[[1]] ## I think this isn't necessary for us, they are just giving the 
## different seq depth data for bact vs. fungi


## then they make the dataframe of relative abundances:

Yabu = Y
## zeros become missing values
Yabu[Y==0] = NA
Yabu = log(Yabu)

## another scaling/centering?
for (i in 1:ns){
   Yabu[,i] = Yabu[,i] - mean(Yabu[,i],na.rm=TRUE)
   Yabu[,i] = Yabu[,i] / sqrt(var(Yabu[,i],na.rm=TRUE))
}


Ypa = 1*(Y>0)

## and they put the P/A and the relative abundance  matrices side-by-side.

Y = cbind(Ypa,Yabu)

#### build model ###

sample.id = rep(NA,ny)

for(i in 1:ny){
   sample.id[i] = paste0("log_",as.character(i))
}

studyDesign = data.frame(Sample=as.factor(sample.id))

## set a random effect by site, explained 5-10 latent variables. 
## not sure what they mean by setPriors, I don't see any 
## specification of priors here. 
rL.Sample = setPriors(HmscRandomLevel(units = studyDesign$Sample), nfMin = 5, nfMax = 10)

## set formulas:

# REGRESSION MODEL FOR ENVIRONMENTAL COVARIATES.
XFormula = ~ water + pH + N + C + lig + diam + Tree + DecayTime + FungiDepth

## null model, to check for seq depth effects??:
XFormula1 = ~ Depth

thin = 100
samples = 1000
nChains = 4
set.seed(1)


## model. Probit, I guess this is makes sense for the PA in the X data
## but I thought this hurdle model was a mixture model...

m = Hmsc(Y=Y,
         XData = XData,  XFormula = XFormula,
         distr={"probit"} ,
         studyDesign=studyDesign, ranLevels={list(Sample=rL.Sample)})

## ah, here is the hurdle implementation
## here they change the distributions 
## the distr=probit above gives a value of 2,
## not sure how the dispersion parameter 
## set to zero came to be
## but now for the latter half of the 
## species (which are a repeat of the 
## the first, but with relative abundances
## instead of P/A). 
## the 1,1 setting indicates poisson 
## with log-link function

for (i in (ns+1):(2*ns)){
   m$distr[i,1:2] = c(1,1)
}


?sampleMcmc

ceiling(0.4*samples*thin)

m = sampleMcmc(m, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = 4,verbose=1)

## seems to work. Can we set up something similar for our data?
## first stab would be to ignore time and space and just include 
## the environmental variables and time

## we need to make sure the lab computer is set up:

## on lab computer:
library(Hmsc)
library(phyloseq)
library(vegan)
library(corrplot)
library(ape)
library(MASS)
library(fields) 
library(knitr)

## they worked from raw asv abundances, we'll
## do the same.

## that means sulari's original phyloseq object:

load("../sulariData/sulariPhyloseqObject.rda")
psNoControl = prune_samples(!(rownames(sample_data(ps)) %in% c("C1.1","C1.2","C2.1","C2.2")), ps)
## remove species that are now zero
psNoControl = prune_taxa( colSums(otu_table(psNoControl)) > 0, psNoControl)

row.names(otu_table(psNoControl))

dim(otu_table(psNoControl))

bactRaw <- otu_table(psNoControl)@.Data


## make pa matrix
bact.pa<-ifelse(bactRaw>0,1,0)


## I think this needs help
## do an element-wise multiplication
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scaleMat = matrix( rep(rowsumsInv, nASVs), ncol =nASVs)
bact.rel <- bactRaw * scaleMat
## sanity check
bactRaw[1:5,1:5]
bact.rel[1:5,1:5]

## check some ASVs
20/sum(bactRaw[1,])
241/sum(bactRaw[2,]) 
## looks sane


## let's use their thresholds.
## we'll need to check these later on our positive controls

#threshold.prev = 0.1 ## present in at least 10% of samples?
threshold.prev = 0.02 ## present in at least 2% of samples?
threshold.abu = 0.001 ## only OTU that reaches at least 0.1% abundance of one sample

ny = dim(bactRaw)[1]

cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples?
cond2=apply(bact.rel,2,max)>=threshold.abu ## in at least one sample, abundance reaches m%
bactData <- bactRaw[,cond1 & cond2]

Y = bactData
ny = dim(Y)[1]
ns = dim(Y)[2]

dim(bactRaw)
dim(Y)

dim(bactData) ## with 0.02 and 0.0001 we are down to 2900 asvs. Really strict. In the future, relax condition1

bactData[1:5,1:5] 
Y[1:5,1:5] ## this includes ASV1, which is probably spurious. Ugh.

## note to self, remove all members of mock community. They are the largest
## source of index bleed.

## do the ordinations looks similar?

## first get a model running, see how long it takes

## environmental covariates (XData matrix) and read depth

## for first model run, ignore spatial and temporal aspects of data
## and check effect of read depth?

dataDir="/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis"
Env <- read.csv(file.path(dataDir,"sulariEnv.csv"), row.names="SampleID")
Env <- Env[!(rownames(Env) %in% c("C1.1","C1.2","C2.1","C2.2")),]
BacteriaDepth<-log(rowSums(bactRaw))


#c("PlotID","Date","soil_respiration","srpuc","MBC","season","Land_type","pH","N","C","CNR","Temperature","Moisture","Latitude","Longitude")
varsOfInterest = c("soil_respiration","Land_type","pH","N","C","Temperature","Moisture")
XData<-data.frame(Env[,varsOfInterest],BacteriaDepth)
XData[,-2]=scale(XData[,-2])
XData$Land_type <- as.factor(XData$Land_type)

## sanity check:
head(Env[,varsOfInterest])
head(XData)
View(XData)
## looks okay

## we'll do the hurdle model, with one PA and on relative abundance, log-transformed

Yabu <- Y
Yabu[Y==0] <- NA
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.

## but here they do further transformations? they scale the data, center it on the mean and divide
## by the square of the variance...this is pretty transformed data by the end of it all

## I think this is just a z-score standardization
for (i in 1:ns){
   Yabu[,i] = Yabu[,i] - mean(Yabu[,i],na.rm=TRUE)
   Yabu[,i] = Yabu[,i] / c(sqrt(var(Yabu[,i],na.rm=TRUE)))
}

## so is this the same as doing?

Yabu2 <- Y
Yabu2[Y==0] <- NA
Yabu2 = log(Yabu2) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
bb <- scale(Yabu2)

Yabu[1:5,1:5]
bb[1:5,1:5]

all(Yabu[1:5,1:5] == bb[1:5,1:5], na.rm=TRUE)
all(Yabu == bb, na.rm=TRUE) ## nope, why? Looks the same
cc = which(Yabu != bb, arr.ind=TRUE)
Yabu[119,302] == bb[119,302] ## no, but...
Yabu[119,302] - bb[119,302] ## 2.775558e-17

## floating decimal difference. these are the same. ugh, waste of time.

## in the future, use:
Yabu <- Y
Yabu[Y==0] <- NA
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu)

## make our second, presence absence matrix:

Ypa = 1*(Y>0)

## bring them together for a final Y matrix
Y = cbind(Ypa,Yabu)

## below, found out that we have a lot of missing
## X values, especially from moisture. So for the moment,
## just to try out the model, let's drop the moisture 
## column, and keep only complete rows after that:

#XData = XData[,c("soil_respiration","Land_type","pH","N","C","Temperature","Moisture", "BacteriaDepth")]

## for the moment, gotta leave out moisture, missing too much data.
XData = XData[,c("soil_respiration","Land_type","pH","N","C","Temperature","BacteriaDepth")]

ccases <- complete.cases(XData) ## 112 samples, we only lose 8 

XData = XData[ccases,] 
Y = Y[ccases,] 

ny = dim(Y)[1]
ns = dim(Y)[2]/2

rownames(XData) == rownames(Y)

## in the future, I guess we can check out the watercontent variable in Betty's data 
## see how correlated it is, 

## for now, keep moving, want to try a model today

# STUDY DESIGN

sample.id = row.names(Y)
studyDesign = data.frame(Sample=as.factor(sample.id))
rL.Sample = setPriors(HmscRandomLevel(units = studyDesign$Sample), nfMin = 5, nfMax = 10) ## not sure how to optimize the number of laten variables?

# REGRESSION MODEL FOR ENVIRONMENTAL COVARIATES.
XFormula = ~ soil_respiration + Land_type + pH + N + C + Temperature
XFormula1 = ~ BacteriaDepth ## check this next. Sort of a null model, I guess.

m = Hmsc(Y=Y,
         XData = XData,  XFormula = XFormula,
         distr={"probit"} ,
         studyDesign=studyDesign, ranLevels={list(Sample=rL.Sample)})

## huh, NA's not allowed in the xdata

sum(complete.cases(XData)) ## 91 complete cases, out of 120. Shoot. That is a lot of data lost. 

colSums(is.na(XData)) ## Moisture is the problem.  Corrected above.

## change the priors for the abundance matrix:


head(m$distr)

tail(m$distr)

View(m$distr)

for (i in (ns+1):(2*ns)){
   m$distr[i,1:2] = c(1,1)
}

## try this:
thin = 1
samples = 1000
nChains = 4

#m = sampleMcmc(m, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = 4,verbose=1)


## looks good, no errors. Cut it off and run remotely. What do we need to do make a script for this?

## save out the model, unsampled

save(m, file="firstHMSCmodelCarbon4d.rds")


## script: ## mcmcSample.R
library(Hmsc)
thin = 1
samples = 1000
nChains = 4
setwd("/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis")
print(paste("start time is", Sys.time()))
load("firstHMSCmodelCarbon4d.rds")
m = sampleMcmc(m, samples = samples, thin=thin, adaptNf=ceiling(0.4*samples*thin), transient = ceiling(0.5*samples*thin), nChains = nChains, nParallel = 4,verbose=1)
save(m, file="firstHMSCmodelCarbon4d_sampled.rds")
print(paste("finish time is", Sys.time()))

## that only took ten minutes
## we can expand the above to include many more rare species, I think.

## now rerun it with more ASVs...

nohup Rscript mcmcSample.R &> mcmcSample.log & ## that ran for an hour and ten minutes. 

## very doable. Add in space and seasonality/time? 

## at least add it moisture. 

## that is on this "sciebo" site:

## for the moment, run through some of the tutorials to see if it actually worked. 

## or maybe the analysis that the paper used..

https://uni-muenster.sciebo.de/s/Ke1maWrj1cBHwVl?path=%2F

## soildata/master.csv, or something like that.

## read

dataDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
Env <- read.csv(file.path(dataDir,"sulariEnv.csv"), row.names="SampleID")
Env <- Env[!(rownames(Env) %in% c("C1.1","C1.2","C2.1","C2.2")),]

head(Env)

bettyData <- read.csv(file.path(dataDir,"master.csv"))

## we need only zero depths for sulari data

filt0 <- bettyData$depth == 0 

bettyData0 <- bettyData[filt0,]

dim(bettyData0)

head(bettyData0)

bettyData0$watercont 

bettyData0$plotID

## ugh, let's do this in python. 

conda activate spatialDirt

python3 

import pandas as pd
import os
import matplotlib.pyplot as plt; plt.ion()

envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
envData['Date'] = pd.to_datetime(envData['Date'], dayfirst=True, errors='coerce')
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2" ], axis=0, inplace=True)

envData.dtypes

bettyData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/master.csv', parse_dates=['date'])
filt0 = bettyData['depth'] == '0'
bettyData0 = bettyData[filt0]



envData.PlotID

bettyData0.plotID

## how do we link these up, joining by plotid isn't enough, because of the multiple sampling in time
## do the dates agree?


bettyData0.plotID[bettyData0.plotID.duplicated()] ## yep lots of dups/resamples

## go through each of sulari's samples, find matching plotID and date
## in betty's data, pull out the watercont

## one line:

#s="S1"
s="S2"
#envData.loc[s]
a = envData.loc[s]['PlotID']
b = envData.loc[s]['Date'].strftime("%Y-%m-%d")
c = bettyData0.query(f'plotID == "{a}" & date == "{b}"')['watercont']

## in a function:

def get_waterCont(sample):
    a = envData.loc[sample]['PlotID']
    b = envData.loc[sample]['Date'].strftime("%Y-%m-%d")
    c = (bettyData0.query(f'plotID == "{a}" & date == "{b}"')['watercont'])
    try:
        c = c.to_list()[0]
    except:
        c = float("NaN")
    return(c)


sample="S1"

sample="S10"

aa = get_waterCont(sample)

## looks okay.

aa = envData.reset_index()['SampleID'].apply(get_waterCont)
aa.index=envData.index




envData['waterCont'] = aa

envData.head()

## sanity check:

## missing data:
aa.isna().any() ## y

envData[aa.isna()] ## three samples:

## there are three sites where data are missing
P0216 P0116 P0165

envData[envData$PlotID %in% c('P0216','P0116','P0165'),]
envData[envData$PlotID %in% c('P0216','P0116','P0165'),]

## are Moisture and waterCont correlated?

help(envData.plot)

envData.plot("Moisture", "waterCont", kind="scatter")
## looks good, very correlated

## One weird outlier.
## that was confusing. save this as our new environmental 
## matrix:

envData.to_csv('sulariEnv.csv')

## okay, so we need a new model, that includes the spatial
## and maybe temporal components?

## for the analysis we need to, not necessarily in order:

## 1 - remove the MC species, can't trust them
## 2 - build spatiatemporal model without temporal component
## 3 - build with temporal component
## 4 - compare

## to remove the mock community species, back to R:

R

library('phyloseq')

sulariDataDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/"
load(paste0(sulariDataDir,"sulariPhyloseqObject.rda"))
psNoControl = prune_samples(!(rownames(sample_data(ps)) %in% c("C1.1","C1.2","C2.1","C2.2")), ps)
## remove species that are now zero
psNoControl = prune_taxa( colSums(otu_table(psNoControl)) > 0, psNoControl)

## and how do we find our ASVs from the MC control?

## These will be the most abundant asvs in our positive controls
## the taxonomy should line up, also, with our notes

sample_names(ps) 

## the single species controls are obvious:
otu_table(ps)["C1.1",1:10]

c1.1 <- sort(otu_table(ps)@.Data["C1.1",],decreasing=TRUE)
c1.1 <- c1.1[c1.1 > 0]

c1.2 <- sort(otu_table(ps)@.Data["C1.2",],decreasing=TRUE)
c1.2 <- c1.2[c1.2 > 0]

## the most abundant ASVs in these:

c1.1[1:15]

c1.1ASVs <- c("ASV1","ASV1315","ASV997","ASV29","ASV39","ASV43","ASV62","ASV4162")

c1.2[1:15]

c1.2ASVs <- c("ASV1","ASV1315","ASV997","ASV4162","ASV29","ASV43","ASV39","ASV62")

## what is the taxonomy of these?

tax_table(ps)[c1.1ASVs,]

tax_table(ps)[c1.2ASVs,]

## there are several possible MC contaminants in here
## ASV1315 is probably a split ASV off of ASV1
## ASV29 bacillus
## ASV39 Chryseobacterium

c2.1 <- sort(otu_table(ps)@.Data["C2.1",],decreasing=TRUE)
c2.1 <- c2.1[c2.1 > 0]

c2.2 <- sort(otu_table(ps)@.Data["C2.2",],decreasing=TRUE)
c2.2 <- c2.2[c2.2 > 0]

## and these are:

c2.1[1:20]

c2.1ASVs <- c("ASV29","ASV39","ASV43","ASV62","ASV70","ASV86","ASV78","ASV200","ASV244","ASV275","ASV365","ASV1","ASV943","ASV997","ASV4")

c2.2[1:20]
c2.2ASVs <- c("ASV29"," ASV39"," ASV43"," ASV70"," ASV62"," ASV86"," ASV78","ASV200","ASV244","ASV275 ASV365","ASV1","ASV943","ASV997")


tax_table(ps)[c2.1ASVs,]

tax_table(ps)[c2.2ASVs,]

## how common are these elsehwere? 

otu_table(ps)[,"ASV29"]

sum(otu_table(ps)[,"ASV29"] > 0) ## 30 sites, out 120. Again, don't think we can trust this. 
## Too bad, because bacillus subtilis is actually known from soil

## franconibacter is an unusual one, I think:
sum(otu_table(ps)[,"ASV943"] > 0) ## 2 samples, as expected 
otu_table(ps)[,"ASV943"] ## not found outside of mock community. Great. 

## so index bleed is mostly a problem with the really abundant microorganisms

## clean up the index bleed problem and rerun the notebook with the cleaner dat
## then start again on the models  

## which are the members of the mock community, as far as we can tell?:

## C1.x controls are all e. coli, ASV1 and probably also ASV1315, and ASV997

otu="ASV1"
sum(otu_table(ps)[,otu] > 0) ## 58 samples, over half
otu_table(ps)[,otu] ## ~1000 reads in the mock community standards. elsewhere <500 reads 

otu="ASV1315"
sum(otu_table(ps)[,otu] > 0) ## only in c1.1/2, didn't bleed
otu_table(ps)[,otu] ## ~500 reads

otu="ASV997"
sum(otu_table(ps)[,otu] > 0) ## 
otu_table(ps)[,otu] ## funny, highly concentrated in controls, but present in a cluster of 
## sites S33-S40 at lower levels (40-120 reads each), probably not enough to matter but can
## be removed here to be sure

## so probably get rid of ASV1 and ASV99

## also of interest in the controls are four ASVs that 
## are probably mock community species:

ASV29   "Bacillaceae"        "Bacillus"
ASV39   "Weeksellaceae"      "Chryseobacterium"
ASV43   "Clostridiaceae"     "Clostridium sensu stricto 11"
ASV62   "Enterobacteriaceae" "Klebsiella"

otu="ASV29"
sum(otu_table(ps)[,otu] > 0) ## 
otu_table(ps)[,otu] ## 

otu="ASV39"
sum(otu_table(ps)[,otu] > 0) ## 
otu_table(ps)[,otu] ## 

otu="ASV43"
sum(otu_table(ps)[,otu] > 0) ## 
otu_table(ps)[,otu] ## 

otu="ASV62"
sum(otu_table(ps)[,otu] > 0) ## 
otu_table(ps)[,otu] ## 

otus=c("ASV29", "ASV39", "ASV43", "ASV62","ASV86","ASV70","ASV200")
otu_table(ps)[,otus] ## 

## site 120 seems to be contaminated more than any other by the controls  
sum(otu_table(ps)["S120",])

## the paper I'm following used a minimum abundance of 0.005 of a site to be considered:

head(tax_table(ps))


sum(otu_table(ps)["S120",])*.005 ## anything above 337.76 would be kept

## so some index bleed by those MC members, but probably not enough to worry about

## check the others:

c2.1 <- sort(otu_table(ps)@.Data["C2.1",],decreasing=TRUE)
c2.1 <- c2.1[c2.1 > 0]
c2.2 <- sort(otu_table(ps)@.Data["C2.2",],decreasing=TRUE)
c2.2 <- c2.2[c2.2 > 0]

c2.1[1:20]

c2.1ASVs <- c("ASV29","ASV39","ASV43","ASV62","ASV70","ASV86","ASV78","ASV200","ASV244","ASV275","ASV365","ASV1","ASV943","ASV997","ASV4")

otu_table(ps)[,c("ASV1",c2.1ASVs)]

sum(otu_table(ps)[,"ASV1"] > 0)


tax_table(ps)["ASV70",]

tax_table(ps)["ASV943",] ## franconibacter

tax_table(ps)["ASV997",] ## plesiomonas 


c2.2[1:20]
c2.2ASVs <- c("ASV29"," ASV39"," ASV43"," ASV70"," ASV62"," ASV86"," ASV78","ASV200","ASV244","ASV275 ASV365","ASV1","ASV943","ASV997")

## for example, the Klebsiella/Enterobactor
otu="ASV62"
sum(otu_table(ps)[,otu] > 0) ## shows up in 38 samples
otu_table(ps)[,otu] ## but mostly in trivial amounts, < 60 reads

## Bacillus subtilis:
otu="ASV29"
otu_table(ps)[,otu] ## but mostly in trivial amounts, =< 100 reads
sum(otu_table(ps)[,otu] > 0) ## shows up in 30 samples

rowSums(otu_table(ps)) ## 

rowSums(otu_table(ps))*0.005 ## in general, a 0.5% threshold requires ~ 300 reads 
rowSums(otu_table(ps))*0.003 ## in general, a 0.3% threshold requires ~ 100-200 reads 

## so we could go to 0.003 (.3%) and get rid of these index-bleeds

## okay, so the only real index bleed is with E. coli. Minimum thresholds
## should take care of the rest.

c2.1ASVs

tax_table(ps)[c2.1ASVs,][,'Genus']
c2.1[1:20]

tax_table(ps)[c2.2ASVs,]

## need to update the notebook with the above.
## standardize the data we are using a bit.

## best approach here - include a section at the beginning of the 
## notebook with these poking/prodding data, 

## also the plotting function that we used with sulari's 
## thesis:

rankAb <- function(phyObj, sampleName, ylimit=500, ntax=NULL, textatX=100, textatY=(ylimit-40)){
    sampleNo0filter <- get_taxa(phyObj, sampleName) > 0
    if(is.null(ntax)) ntax=sum(sampleNo0filter)
    print(sum(sampleNo0filter)) ## let user know how many unique taxa are in sample
    sampleNo0 <- get_taxa(phyObj, sampleName)[sampleNo0filter]
    sampleNo0 <- sort(sampleNo0, decreasing=TRUE)
    sampleNo0 <- sampleNo0[1:ntax]
    taxaNames=tax_table(phyObj)[ names(sampleNo0), "Genus"]
    nuASV <- paste("number of unique ASVs = ",sum(sampleNo0filter), sep="")
    par(cex.axis = .75, mar=c(10,4,4,2))
    barplot(sampleNo0,
        ylim = c(0,ylimit),
        main=sampleName,
        cex.main=2,
        las=2,
        names.arg=taxaNames,
        mar=c(20,10,2,2))
    text(textatX, textatY,  nuASV, cex = 2, )
}
 
rankAb(ps,"C1.1",ylimit=3000, ntax = 30)

par(cex.lab=20)

par(mar = c(30, 4.4, 4.1, 1.9))

par(mar = c(0, 0, 0, 0))

rankAb(ps,"C2.1",ylimit=10000, ntax = 30)

rankAb(ps,"C2.2",ylimit=10000, ntax = 30)


| DSMZ name | (Sub)Phylum | Classified as | ASV# |
|  :--- | :---: | :---: | ---: |
|  Bacillus subtilis          | Bacillota            | Bacillus                                           | ASV29  |
|  Chryseobacterium luteum    | Bacteroidota         | Chryseobacterium                                   | ASV39  |
|  Clostridium roseum         | Bacillota            | Clostridium sensu stricto 11                       | ASV43  |
|  Enterobacter mori          | Gammaproteobacteria  | Klebsiella                                         | ASV62  |
|  Franconibacter helveticus  | Gammaproteobacteria  | Franconibactor                                     | ASV86, ASV943  |
|  Glycomyces tenuis          | Actinomycetetota     | Glycomyces                                         | ASV200  |
|  Pseudomonas fluorescens    | Gammaproteobacteria  | Pseudomonas                                        | ASV70, ASV275  |
|  Pseudomonas putida         | Gammaproteobacteria  | Pseudomonas                                        | ASV70, ASV275  |
|  Rhizobium endophyticum     | Alphaproteobacteria  | Allorhizobium-Neorhizobium-Pararhizobium-Rhizobium | ASV78  |
|  Streptomyces acidiscabies  | Actinomycetatota     | Streptomyces                                       | ASV244  |
|  Variovorax guangxiensis    | Betaproteobacteria   | Variovorax                                         | ASV365  |




## great. So how do we remove all this contamination? I think our original plan of 
## a minimum cutoff is better than a minimum % prevalence per sample as 
## used by Odriozola et al. 

## so enforce a minimum cutoff, and normalize for each site and take the log 
## essentially the same pipeline 
## also, just get rid of ASV1. 

## all this should be done in phylseq before exporting the community to downstream 
## software:


R

library('phyloseq')

sulariDataDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/"
load(paste0(sulariDataDir,"sulariPhyloseqObject.rda"))

psNoControl = prune_samples(!(rownames(sample_data(ps)) %in% c("C1.1","C1.2","C2.1","C2.2")), ps)
## remove species that are now zero
psNoControl = prune_taxa( colSums(otu_table(psNoControl)) > 0, psNoControl)

## remove ASV1 wherever it is found:

notASV1 <- taxa_names(psNoControl) != "ASV1"
psNoControl <- prune_taxa(notASV1, psNoControl)

## what is our minimum cutoff? Here we have a dilemma. 
## our last sample, 120, is extremely contaminated
## by our positive controls. In the worst case,
## ASV29 is >200 reads 

rowSums(otu_table(psNoControl))

otu_table(psNoControl)[0:10,0:10]

## in which case do we lose more OTUs?
## a simple minimum cutoff, or a relative minimum presence

## they enforce a relative prevalence threshold in this way:
## first, make a relative abundance matrix, line ~3400 above
## then
threshold.prev = 0.02 ## each OTU is retain only if present in at least 2% of samples
threshold.abu = 0.001 ## only OTU that reaches at least 0.1% abundance of one sample
bact.pa<-ifelse(bactRaw>0,1,0) ## make PA mat
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples?
cond2=apply(bact.rel,2,max)>=threshold.abu ## in at least one sample, abundance reaches m%
bactData <- bactRaw[,cond1 & cond2]

## with these thresholds, do the contaminants pass through?

mcBiggest <- c("ASV29","ASV39","ASV43","ASV62","ASV70","ASV86","ASV78","ASV200")
for (i in mcBiggest){
  print(i)
  print(bactData[,i])
}

otu_table(ps)[,mcBiggest]

bactData[,mcBiggest]

## yes, nearly all make it through.
## I think we have to enforce a minimum cutoff.

## I think the best strategy is to go by each line 
## and find the highest abundance of contamination 
## by the mock community.

## essentially a site-by-site cutoff, using the most abundant members of the 
## mock communitiy members to decide the cutoff 

## ASVE 78 is everywhere, btw. What is this?
tax_table(ps)["ASV78",] ## Rhizobium spp. This could be real. As far as I know, a non-nodule forming rhizobium
## https://doi.org/10.1016/j.syapm.2010.07.005
## 

## interesting, B. subtilis is reported in thousands of 
## soils samples across the world, but we don't really
## see it here except in samples that are obviously 
## contaminated. 

## anyway, what happens with a minimum cutoff of 225, 
## which is the highest level of contamination that 
## we see in S120, with ASV29, Bacillus subtilis

minCutoff <- 50

## make a copy of our otu_table from our original phyloseq object:

ot <- otu_table(ps)
## if it doesn't meet this cutoff, go to zero
ot <- ot - 225
ot[ot < 0] <- 0
## insert this into a new phyloseq object
psMinCutoff <- ps
otu_table(psMinCutoff) <- ot
## how many reads do we lose with this?

sample_names(psMinCutoff)

otu_table(psMinCutoff)[,mcBiggest]

otu_table(psMinCutoff)["c2.1",]

## remove controls and zeros

## remove species that are now zero
psMinCutoff = prune_taxa( colSums(otu_table(psMinCutoff)) > 0, psMinCutoff)

psMinCutoff ## down to 643 taxa

## this clears out all the major contaminants

otu_table(psMinCutoff)[,mcBiggest]

otu_table(psMinCutoff)[c("C1.1","C1.2"),]

par(mfrow=c(1,2))
rankAb(psMinCutoff,"C1.1",ylimit=500)
rankAb(ps,"C1.1",ylimit=500, ntax = 30)
par(mfrow=c(1,1))

## okay, so this gets rid of all contaminants. 
## but all rare species are lost, I assume

## for instance, do we find 

Candidatus Udaeobacter

onlySulari <- grep("S", filtFs)

grep("Udaeobacter", tax_table(ps), value=TRUE)

sum(grepl("Udaeobacter", tax_table(ps))) 

grepl("Udaeobacter", tax_table(ps)) 

## oh wow. Lots. 
filt <- grep("Udaeobacter", tax_table(ps))
filt <- grepl("Udaeobacter", tax_table(ps)) 

tax_table(ps)@.Data[filt]

head(tax_table(ps))

any(tax_table(ps)[,'Phylum'] == "Candidatus Udaeobacter")

grep("Candidatus Udaeobacter", tax_table(ps)[,'Phylum']) ## nope

## here's where they are:
filt <- grep("Candidatus Udaeobacter", tax_table(ps)[,'Genus'])
tax_table(ps)@.Data[filt,]

## lots of udeabacter, 

filt <- grep("Candidatus Udaeobacter", tax_table(ps)[,'Genus'])
udeas <- rownames(tax_table(ps)@.Data[filt,])
otu_table(ps)[,udeas]

## yes, looks like we may lose a lot of information if we use these
## strict cutoffs.

## it will be more reasonable to do some ASV/OTU clustering, 
## then enforce the minimums. Less loss of data. 

data("esophagus")
# for speed
esophagus = prune_taxa(taxa_names(esophagus)[1:25], esophagus)

plot_tree(esophagus, label.tips="taxa_names", size="abundance", title="Before tip_glom()")
plot_tree(tip_glom(esophagus, h=0.2), label.tips="taxa_names", size="abundance", title="After tip_glom()")

## great, so how do we bring in our tree for this phyloseq object?
## get the tree into R...

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"

/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis

aa <- read_tree(paste0(spatDir,"/sulariFastTree.nwk"))
phy_tree(ps) <- aa

print(paste("start time is", Sys.time()))
#bb <- tip_glom(ps, h=0.03) ## looks like family level in tree below
# bb <- tip_glom(ps, h=0.02) ## lost Franconibacter, presumably to the Enterobacter branch
#bb <- tip_glom(ps, h=0.02) ## still too much lumping
#bb <- tip_glom(ps, h=0.02) ## very close - two pseudomonas, one franconibacter, but streptomyces lumped into another ASV
#bb <- tip_glom(ps, h=0.01) ## same as .02
bb <- tip_glom(ps, h=0.005) ## and...looks good!
print(paste("finish time is", Sys.time()))

## the thing to do here would be to look at a positive controls, 
## and find the h-value that gets rid of splitting in
## E. coli, Franconibacter, any others 
## but one that keeps the pseudomonases separate
## to get a phylo tree of just the controls

cutoffAbu <- 100
cc = prune_samples((rownames(sample_data(bb)) %in% c("C1.1","C1.2","C2.1","C2.2")), bb)
cc = prune_taxa( colSums(otu_table(cc)) > 0, cc)
## retain only those OTUs with high abundances in both MC samples:
mcOTUs <- taxa_names(cc)[colSums(otu_table(cc)[c("C2.1","C2.2"),] > cutoffAbu) == 2]
dd = prune_taxa( c("ASV1",mcOTUs), cc)
sample_data(dd)$Plot.ID <- c("ss","ss","mc","mc")


plot(phy_tree(dd))

par(mfrow = c(1,2))
rankAb(cc,"C1.1", ylimit=5000, ntax=10)
rankAb(dd,"C2.1", ylimit=5000)
par(mfrow = c(1,1))

rankAb(dd,"C1.1", ylimit=5000)


plot_tree(dd, label.tips="taxa_names", color="Plot.ID", size="abundance")

plot_tree(dd, label.tips="Genus", color="Plot.ID", size="abundance")

plot_tree(dd, label.tips="Family", size="abundance")

plot_tree(dd, label.tips="Phylum", size="abundance")

plot_tree(dd, label.tips="Genus")

rankAb(dd,"C2.1",ylimit=500)

tax_table(dd)

## why are the taxa so different now, mostly unlabeled?

tax_table(ps)[,'Genus'])

## Franconibacter keeps disappearing...
filt <- grep("Franconibacter", tax_table(ps)[,'Genus'])
frankishBacteria <- rownames(tax_table(ps)@.Data[filt,])
otu_table(ps)[,frankishBacteria]

frankishBacteria 

## what else are we missing?

bactOfInt <- "Streptomyces" 
filt <- grep(bactOfInt, tax_table(ps)[,'Genus'])
bactOfInt <- rownames(tax_table(ps)@.Data[filt,])

bactOfInt

otu_table(ps)[,bactOfInt]

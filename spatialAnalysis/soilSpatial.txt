## git our local copy of the repo in order for office comp.
## working off the work tower, so need to get git synced up now to
## avoid confusion.

## we need RSA with SHA-2 signature algorithm

man ssh-keygen
ssh-keygen -t rsa -f fuj2git

## now, what do we need to get the push functionality...

git clone https://github.com/danchurch/fichtelgebirgeSoils.git

## test

touch thisIsNotReal.txt

## and of course can't push

git config --global user.email "danchurchthomas@gmail.com"
git config --global user.name "danchurch"

git remote add origin https://github.com/danchurch/fichtelgebirgeSoils.git
git branch -M main
git remote set-url origin git@github.com:danchurch/fichtelgebirgeSoils.git
git push -u origin main

## and we're in business with github

## maybe let's get a conda environment going for this.

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash

conda config --set auto_activate_base false

## get the mamba solver:

conda update -n base conda
conda install -n base conda-libmamba-solver
conda config --set solver libmamba

## get the standard channels
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

## this new conda env comes with python3.12

## let's see if this works for our spatial analysis


conda activate
conda create -n "spatialDirt" 
 
## let's think about spatial turnover in Sulari's community data

## first step would be to get a map. 

## we want to see where we sample, and visualize respiration 
## values across the landscape

conda deactivate

conda remove -n spatialDirt --all

conda create -n "spatialDirt" 

conda activate "spatialDirt" 
conda config --env --add channels conda-forge
conda config --env --set channel_priority strict

conda install python=3 geopandas

conda activate spatialDirt 

pip install rasterio

## we also need to be R up to speed...
## maybe do this outside of conda

conda deactivate

sudo R 

install.packages("BiocManager")
BiocManager::install("phyloseq")


## I think that took care of most of the complex installs

## oh wait, let's get the jupyter notebook setup going...

## how do we make sure that the jupyter behaves, stays in the 
## right python?

conda activate spatialDirt 
pip install notebook 

which jupyter ## looks like that work. Gets easier every year.

## and it looks like it is even keeping the R kernel from 
## my general environment. 

## to get a bash kernel on there? https://github.com/takluyver/bash_kernel

pip install bash_kernel
python -m bash_kernel.install

##### bayesian setup #####

## last time we worked with pymc3 we needed
## a separate conda env. Let's see if 
## things have gotten better. 

## first back up the env, just in case

conda activate spatialDirt

conda env export > spatialDirt.yml

## installing bambi should also install pymc, so
## try the bambi conda installs as per: 
https://github.com/bambinos/bambi#quickstart

pip install bambi

pip install "preliz[full,lab]"

## get the data from the newest martin book.

## put outside our repo

cd /home/daniel/Documents/manualsBooks/bayesian
git clone https://github.com/aloctavodia/BAP3.git

## seems like that worked in our spatialDirt environment

## try everything out for a bit, then update yaml backup for the repo


conda activate spatialDirt 

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import os, rasterio
import rasterio.plot
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
from matplotlib_scalebar.scalebar import ScaleBar
import pymc as pm


spatDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
## ^different on laptop
os.chdir(spatDir)

## we just want a map of points right now:

## on officeComp
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"
## on laptop
#sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sulariData/Envt_Matrix.csv"

envData = pd.read_csv(sulariEnvCSV)
## get rid of spaces
envData.rename({"Sample ID":"SampleID"}, axis="columns", inplace=True)
## we need to clean up the plot.ID. Sulari recorded season in the plot IDs,
## using letter codes. Also she has one double sampling, with an underscore
envData['Plot.ID'] = envData['Plot.ID'].str.slice(0,5)

## let's get rid of decimals in the names, weird for python work:
envData.rename({"Plot.ID":"PlotID", 
       "soil.respiration":"soil_respiration",
              "Land.type":"Land_type"}, 
                axis="columns", inplace=True)

envData.head()

## so we don't have to repeat:
envData.to_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index=False)

envData.head()


envData = pd.read_csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv', index_col='SampleID')
 

## if we need to drop controls
envData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

## her otu table is really large:
##### R ###
library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")
logMin50ps
comdat <- as.data.frame(otu_table(logMin50ps))
write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")
############

## back in python

comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", index_col=0)

comData.drop([ "C1.1", "C1.2", "C2.1", "C2.2"], inplace=True)

comData.head()

## the map below looks funny...looks like every forest plot has 
## a grassland under it...

dupsFilter = envData.Latitude.duplicated()
dupped = envData[dupsFilter].sort_values(by="Latitude")
dupped.groupby('Plot.ID').nunique()
envData.iloc[0:5,0:8]

## nope...looks okay

## we want a geodf...

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )

sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

## to convert to UTM? looks like we are in zone 33
## looks like:
## EPSG:32633

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

sulariPlot_utm.tail()

## can we import our georeferenced tif of the study area?
## lat/lon
#fichtelMap = rasterio.open("studyAreaClipped_modified.tif")
## UTM

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color code our points according to land type?
cdik={
"Arableland":"b",
"Grassland":"y",
"Forest":"g",
}
sulariPlotsDF['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]
sulariPlot_utm['landColors'] = [ cdik[i] for i in sulariPlotsDF['Land_type'] ]

fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    facecolor=sulariPlotsDF['landColors'],
    markersize=400) 


grassPatch = Patch(color='y', label='grassland',)
forestPatch = Patch(color='g', label='forest')
farmPatch = Patch(color='b', label='arable land')
ax.legend(handles=[grassPatch, forestPatch, farmPatch], 
          loc="lower left",
          fontsize=15,
)

## if we want to compare just grassland and forest

plt.close('all')
onlyGrassForest = sulariPlot_utm[sulariPlot_utm['Land.type'].apply(lambda x: x in ["Forest", "Grassland"])]
fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
onlyGrassForest.plot(
    marker="o",
    ax=ax,
    edgecolor='k',
    linewidths=2,
    facecolor=onlyGrassForest['landColors'],
    markersize=200) 
ax.ticklabel_format(style='plain', axis='y', useOffset=False)
grassPatch = Patch(color='y', label='grassland',)
forestPatch = Patch(color='g', label='forest')
ax.legend(handles=[grassPatch, forestPatch], loc='lower left')
ax.add_artist(ScaleBar(1, location='lower right')) 
ax.set_xlim([265500, 286930])
ax.set_ylim([5547227, 5570000])
plt.savefig('forestVsGrasslandMapUTM.png', dpi=600, format='png')

## Look at the turnover data:

## lat/long
aa = pd.DataFrame({'xx':envData.Longitude, 'yy':envData.Latitude})
physDist = sp.distance.pdist(aa, metric='euclidean')
bcDist = sp.distance.pdist(comData, metric='brayCurtis')
fig, ax = plt.subplots()
ax.scatter(physDist, bcDist)
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title(label="Turnover with Lat/Lon", loc='center')

## utms
aa = pd.DataFrame({'xx':sulariPlot_utm.geometry.x, 'yy':sulariPlot_utm.geometry.y})
physDist = sp.distance.pdist(aa, metric='euclidean')
bcDist = sp.distance.pdist(comData, metric='brayCurtis')
fig, ax = plt.subplots()
ax.scatter(physDist, bcDist)
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title(label="Turnover with UTM", loc='center')

plt.close('all')
## subset by landtype
for lt in [ "Arableland" ,"Grassland" ,"Forest"]: 
    print(lt)
    edf = envData[envData['Land.type'] == lt]
    cdf = comData.loc[edf.index]
    aa = pd.DataFrame({'xx':edf.Longitude, 'yy':edf.Latitude})
    aa = aa.iloc[0:120,:]
    physDist = sp.distance.pdist(aa, metric='euclidean')
    bcDist = sp.distance.pdist(cdf, metric='brayCurtis')
    fig, ax = plt.subplots()
    ax.scatter(physDist, bcDist)
    ax.set_title(lt)
    ax.set_title(label= (lt + " in degrees"), loc='center')
    X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
    ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')

## well that looks pretty much like I hypothesized
## good stuff.

sulariPlot_utm.head()

plt.close('all')
plt.rc('ytick', labelsize=15)
plt.rc('xtick', labelsize=15)
lts = [ "Arableland" ,"Grassland" ,"Forest"]
#lts = [ "Grassland" ,"Forest"]
fig, axes = plt.subplots(nrows=1, ncols=len(lts), sharey=True)
axes = axes.flatten()
for nu,lt in enumerate(lts):
    edf = sulariPlot_utm[sulariPlot_utm['Land.type'] == lt]
    cdf = comData.loc[edf.index]
    aa = pd.DataFrame({'xx':edf.geometry.x, 'yy':edf.geometry.y})
    physDist = sp.distance.pdist(aa, metric='euclidean')
    bcDist = sp.distance.pdist(cdf, metric='brayCurtis')
    axes[nu].scatter(physDist, bcDist)
    X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
    linMod =  LinearRegression().fit(X, Y)
    axes[nu].plot( X, linMod.predict(X), c='k')
    axes[nu].set_title(label=lt, size=20, loc='center')
    axes[nu].set_xlabel('meters', size=20)
    print(lt, stats.linregress(physDist,bcDist))

fig.suptitle("Turnover in prokaryotic community", size=40)
axes[0].set_ylabel('Bray-Curtis dissimilarity', size=20)
axes[1].tick_params(left=False, labelleft=False, right=True, labelright=True, color='red', axis='y')
plt.subplots_adjust(wspace = 0)


###################################
##
## outputs from stats.regress:
##
## Arable Land
## slope=2.810886008879358e-06
## intercept=0.5742255266887248
## rvalue=0.07906122967379002
## pvalue=0.02033328808278514
## stderr=1.209265073980233e-06
## intercept_stderr=0.013364930035836518
## 
## Grassland
## slope=4.135586933082137e-07
## intercept=0.5590568736859212
## rvalue=0.012708447690178782
## pvalue=0.7298157896065798
## stderr=1.1969812747713313e-06
## intercept_stderr=0.013504959623407586
## 
## Forest
## slope=5.843245351182221e-06
## intercept=0.6024952517397398
## rvalue=0.20458396890349276
## pvalue=1.918646699083231e-08
## stderr=1.0284330215725841e-06
## intercept_stderr=0.01259259776931045
######################################

## add in the correlation coefficients and pvalues to grant app graphic. 

###### SAC curves ##########

## we have to stop avoiding gamma diversity calculations...

## do this in vegan? why not.

R

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
setwd(spatDir)


library(vegan)
library(phyloseq)

comM <- read.csv('/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv', 
                    row.names=1)

## why is this so big, btw?

sum(colSums(comM) > 0) ## 4363. Why do we have a bunch of empty colums? I think these were low abundance ASVs, below our cutoffs.

## get rid of them to save memory:

library('phyloseq')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")

logMin50ps

comdat <- as.data.frame(otu_table(logMin50ps))
comdat = comdat[,colSums(comdat) > 0] 

write.csv(comdat, file="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv")


## get rid of controls
notControls=!(row.names(comM) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comM = comM[notControls,]

comM[1:4,1:4]


sp1 <- specaccum(comM)

plot(sp1, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue")

specpool(comM)

sp2 <- specaccum(comM, "random")

summary(sp2)

plot(sp2, ci.type="poly", col="red", lwd=2, ci.lty=0, ci.col="pink")


data(BCI)

sp1 <- specaccum(BCI)


sp2 <- specaccum(BCI, "random")

sp2

summary(sp2)

plot(sp1, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue")
boxplot(sp2, col="yellow", add=TRUE, pch="+")
## Fit Lomolino model to the exact accumulation
mod1 <- fitspecaccum(sp1, "lomolino")
coef(mod1)
fitted(mod1)
plot(sp1)

aa <- specaccum(comM, method = "exact")

?specaccum

anaSAC <- data.frame(aa$richness, aa$sd)
colnames(anaSAC) <- c('richness', 'sd')
anaSpeciesEstimators = specpool(comM)
print(anaSpeciesEstimators)
 
## okay, but we need to separate out by land types.
## wish we were in python...

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
setwd(spatDir)
library(vegan)
library(phyloseq)

comData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", row.names=1)
envData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv", row.names=1)

all(row.names(comData) == row.names(envData))

lt <- 'Forest'
justThisLandtype=row.names(envData[envData['Land_type'] == lt,])
aa <- comData[ justThisLandtype,]
sp1 <- specaccum(aa)
plot(sp1, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue")


## so loop this:

for (lt in c('Arableland','Grassland', 'Forest')){
    print(lt)
    justThisLandtype=row.names(envData[envData['Land_type'] == lt,])
    comm.i <- comData[ justThisLandtype,]
    specAccum.i <- specaccum(comm.i)
    SACdf.i <- data.frame(specAccum.i$richness, specAccum.i$sd)
    colnames(SACdf.i) <- c('richness', 'sd')
    speciesEstimators.i = specpool(comm.i)
    print(speciesEstimators.i)
    write.csv(SACdf.i, file=paste(lt, "SAC.csv", sep="_"))
    write.csv(speciesEstimators.i, file=paste(lt, "specEst.csv", sep="_"))
}

## interesting, this is pretty much exactly what Brendan found
## in the amazon. Despite lower alpha diversity, higher beta 
## diversity in forest soils. 
## and this equates to a higher total diversity across the
## survey (gamma). 

## take over to python for plotting

## we have an old function for this, wonder if it still works:


os.chdir("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis")

sacs = [ i for i in os.listdir() if "_SAC.csv" in i  ]

def plotSACs(habtype, color='black', ax=None):
    if ax is None: fig, ax = plt.subplots()
    sacs = [ i for i in os.listdir() if "_SAC.csv" in i  ]
    sacName = (habtype +'_SAC.csv')
    assert( (habtype +'_SAC.csv') in sacs)
    specEstName = (habtype + "_specEst.csv")
    sac_i = pd.read_csv(sacName, index_col=0)
    specEst_i = pd.read_csv(specEstName, index_col=0).loc['All']
    specEst_i.index = specEst_i.index.str.replace(".","_")
    X = sac_i.index
    ax.plot(X, sac_i['richness'], color=color)
    ax.fill_between(x=X,
                     y1=sac_i.richness - sac_i.sd,
                     y2=sac_i.richness + sac_i.sd,
                    alpha=0.4,
                    color=color,
                    )

plt.close('all')
fig, ax = plt.subplots(figsize=(10,10))
plotSACs('Arableland', ax=ax, color='#862d2d')
plotSACs('Forest', ax=ax, color='#006600')
plotSACs('Grassland', ax=ax, color='#FF7F00')

Arableland_patch = Patch(color='#862d2d', label='Arableland', alpha=0.4)
Forest_patch = Patch(color='#006600', label='Forest', alpha=0.4)
Grassland_patch = Patch(color='#FF7F00', label='Grassland', alpha=0.4)

ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])
ax.set_title('Species accumulution curves by\nland-use/Habitat')
ax.set_xlabel('Sites sampled')
ax.set_ylabel('Prokaryotic ASVs')


[Arableland_patch, Forest_patch, Grassland_patch]

## repeat alpha diversity
## using our >50 reads OTU table, can we calculate alpha diveristy by land type?
## back to old fashioned vegan/R

## we just want species richness. So we need to rarefy and compare 
## forest v. farm v. grassland data

notControls=!(row.names(comData) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comData = comData[notControls,]
envData = envData[notControls,]

data(BCI)

S <- specnumber(BCI) # observed number of species

S <- specnumber(comData) # observed number of species

## pretty much same as:
aa <- comData
aa[aa > 0] <- 1
rowSums(aa)

S <- specnumber(comData) # observed number of species

(raremax <- min(rowSums(BCI)))

Srare <- rarefy(BCI, raremax)

plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")

abline(0, 1)

rarecurve(BCI, step = 20, sample = raremax, col = "blue", cex = 0.6)


## so we are looking for a rarified species richness for each site.

## from this we will generate 3 mean +/- error values of species richness 
## one for each land use.

## this kind of analysis is really ok for count data.
## we've done all kinds of transformations, to try 
## to reduce sequencer error. 

## so I think we need to back up to phyloseq, to use our 
## sequencing depth information

library(phyloseq)


## transformed:
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sularilogMin50ps.rda")

## not transformed:
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

logMin50ps

(p = plot_richness(ps, x = "Land.type"))

estimate_richness(ps)

estimate_richness

## but I am thinking about this incorrectly. 
## these richness estimates are way high, 
## because of PCR, sequencer error, etc. 

## we attempting to reign in these errors 
## a bit through our transformations, let's
## honor this. 

## so back and use our communty matrix, 

savehistory("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/latelyInR.txt")

## to make it into "count data", multiply 
## to get rid of decimals:


sum(comData < .001 & comData > 0) ## 508 observations smaller than .001.
sum(comData < .0001 & comData > 0) ## 0 observations, so let's multiply by 10000
comDataFakeCounts = ceiling(comData * 10000) 
min(comDataFakeCounts[ comDataFakeCounts > 0 ]) ## our smallest non-zero observation is 8 

S <- specnumber(comDataFakeCounts) # observed number of species

(raremax <- min(rowSums(comDataFakeCounts))) ## 10001

rowSums(comDataFakeCounts)

Srare <- rarefy(comDataFakeCounts, raremax) ## this is what we need. 

## this is an estimate of how many species are present in each 
## sample, after coming down to a minimum abundance


## kind of interesting but not useful. Shows we sequenced deeply enough:
plot(S, Srare, xlab = "Observed No. of Species", ylab = "Rarefied No. of Species")

## can we do all that without transforming to "counts"?
S2 <- specnumber(comData) # observed number of species, same as fake counts

(raremax2 <- min(rowSums(comData))) ## .9896

rowSums(comData)



## compare to:

bb <- specnumber(comData) # observed number of species
all(bb == Srare) ## yes. the same
## So I guess I am brilliant, I just reinvented their command. 
## big waste of time. 

## maybe also shows there isn't really a need to rarefy, at least
## on the transformed data

## not sure, but now let's trust these numbers.

## now, subset by land type, and get means?

head(envData)

all(row.names(envData) == row.names(comData))

## I'd guess we need a vector of group names (by land_type):
hist(Srare, 20) ## looks more or less normal.
mean(Srare) ## 301.3833
sd(Srare) ## 26.4

print("mean alpha diversity of all sites = ", mean(Srare))

print(paste("mean alpha diversity of all sites =", mean(Srare), "ASVs"))

cat(paste("mean alpha diversity of all sites =", mean(Srare), "ASVs"))

cat(paste("mean alpha diversity of all sites =", mean(Srare), "+/-", round(sd(Srare)), "ASVs"))


all(names(Srare) == row.names(envData))

## break this down by groups:

tapply(Srare, envData$Land_type, mean)
tapply(Srare, envData$Land_type, sd)

boxplot(Srare ~ envData$Land_type)

## anova
res.aov <- aov(Srare ~ envData$Land_type)
summary(res.aov) ## F= 2.85, p = 0.0618

## so maybe differences in species richness due to land type,
## maybe not.  Not a large effect, anyway. 
## update notebook, give it a break.

## t-test for difference between forest and grassland?

## to remove the farm samples 

noFarms <- envData$Land_type != "Arableland"

envData$Land_type[noFarms]
Srare[noFarms]


head(envData)

t.test(Srare[noFarms] ~ envData$Land_type[noFarms])

t_test(weight ~ group)

## we should probably do this in a bayesian way...
## get the ordinations done, then order the new book, 
## PCNMs can also be started without tests.

## but generally not sure how to handle the multivariate
## tests in a bayesian way. 

## two possibilities:
## BetaBayes: https://doi.org/10.3390/d14100858, somehow related to GDS
## BERA: https://doi.org/10.1080/00273171.2019.1598837
## BERA is apparently related to RDA. More reading is in order on both. 

## in the meantime...re-run the ordinations:

## the usual pipeline
## let's get ordinations with vegan and plot with python 

library(vegan)

spatDir <- "/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis"
setwd(spatDir)

comData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", row.names=1)
envData=read.csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv", row.names=1)
## get rid of controls
notControls=!(row.names(comData) %in% c("C1.1", "C1.2", "C2.1", "C2.2"))
comData <- comData[notControls,]
envData <- envData[notControls,]

comData[1:4,1:4]

comNMS <- metaMDS(comData, try=40)

write.csv(comNMS$points, file='comNMS.csv')

## check this out in python:

python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
import matplotlib.colors 
import os
import scipy.spatial as sp
import numpy as np
from scipy import stats
from matplotlib.patches import Patch
import pymc as pm
import preliz as pz
import arviz as az
import rasterio.plot

## data
nmsPts = pd.read_csv("comNMS.csv", index_col=0)
sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 
controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
envData.drop(controls, axis='rows', inplace=True)
comData.drop(controls, axis='rows', inplace=True)

## need some colors for land type
colorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
landCols = [ colorDict[i] for i in envData['Land_type'] ]
plt.close('all')
fig, ax = plt.subplots()
ax.scatter(x=nmsPts["MDS1"],
           y=nmsPts["MDS2"], 
           c=landCols,
          )
Arableland_patch = Patch(color='#862d2d', label='Arableland')
Forest_patch = Patch(color='#006600', label='Forest')
Grassland_patch = Patch(color='#FF7F00', label='Grassland')
ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])

## try markers for seasonality?
seasonDict = {
     'S': "o", 
    'SP': "v", 
    'W1': "D", 
    'W2': "P", 
     'A': "s",
}

## we might also check pH, and seasonality, and microbial biomass

## seasonality:
seasonShapes = [ seasonDict[i] for i in envData['season'] ]

## matplot lib doesn't change markers on the fly...
## if we want to change markers for each season:

plt.close('all')
fig, ax = plt.subplots()
for i in envData.season.unique():
  print(i)
  env_i = envData[envData['season'] == i]
  plots_i = env_i.index.to_list()
  nmsPts_i = nmsPts.loc[plots_i]
  cols_i = [ colorDict[i] for i in env_i['Land_type'] ]
  ax.scatter(x=nmsPts_i["MDS1"],
             y=nmsPts_i["MDS2"], 
             c=cols_i,
        marker=seasonDict[i],
            )

## I don't see any evidence of seasonality affecting these
## community structures

## ph ordinations

## color by pH, land by symbol, respiration by size

landTypeShapesDict = {
'Arableland': "o", 
'Forest'    : "v", 
'Grassland' : "s", 
}

pHmin = envData['pH'].min() ## 3.647
pHmax = envData['pH'].max() ## 7.312
norm=matplotlib.colors.Normalize(pHmin, pHmax)
plt.close('all')
fig, ax = plt.subplots()
for i in envData.Land_type.unique():
  print(i)
  env_i = envData[envData['Land_type'] == i]
  plots_i = env_i.index.to_list()
  nmsPts_i = nmsPts.loc[plots_i]
  sizes = env_i['soil_respiration']*50
  ax.scatter(x=nmsPts_i["MDS1"],
             y=nmsPts_i["MDS2"], 
             #s=140,
             s=sizes,
             c=env_i['pH'],
             cmap='Spectral',
             edgecolors='black',
             marker=landTypeShapesDict[i],
             norm=norm,
            )

fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.Normalize(pHmin, pHmax), cmap='Spectral'),
             ax=ax, orientation='vertical', label='pH')


## this is a good graph. needs a legend. 

## as usual, legends are beyond my ability. Do them manually later if we want the figure.


### test out bayesian setup, try comparison of two groups ###

dist = pz.Beta()
pz.maxent(dist, 0.1, 0.7, 0.9)

## test out the pymc3 setup:
np.random.seed(123)
trials = 4
theta_real = 0.35
data = pz.Binomial(n=1, p=theta_real).rvs(trials)

np.random.seed(123)
trials = 4
theta_real = 0.35
data = stats.bernoulli.rvs(p=theta_real, size=trials)

with pm.Model() as our_first_model:
    θ = pm.Beta('θ', alpha=1., beta=1.)
    γ = pm.Bernoulli('γ', p=θ, observed=data)
    trace = pm.sample(1000, random_seed=123)

## works. I can hardly remember how to do this bayesian stuff,
## but we can work from old examples....

## let's redo our only statistical model/test so far, the comparison 
## of alpha diversity between the land types

## start with oswaldo's tips example:

tips = pd.read_csv("/home/daniel/Documents/manualsBooks/bayesian/BAP3/code/data/tips.csv")

tips.tail()

categories = np.array(["Thur", "Fri", "Sat", "Sun"])
tip = tips["tip"].values
idx = pd.Categorical(tips["day"], categories=categories).codes

## arviz has cool plotting capabilities I have not even begun to learn:
az.plot_forest(tips.pivot(columns="day", values="tip").to_dict("list"),
               kind="ridgeplot",
               hdi_prob=1,
               colors="C1",
               figsize=(12, 4))

## for indexing with Arviz:

coords = {"days": categories, "days_flat":categories[idx]}

with pm.Model(coords=coords) as comparing_groups:
    μ = pm.HalfNormal("μ", sigma=5, dims="days")
    σ = pm.HalfNormal("σ", sigma=1, dims="days")
    y = pm.Gamma("y", mu=μ[idx], sigma=σ[idx], observed=tip, dims="days_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))


_, axes = plt.subplots(2, 2, figsize=(10, 5), sharex=True, sharey=True)

az.plot_ppc(idata_cg, num_pp_samples=100,
            colors=["C1", "C0", "C0"],
            coords={"days_flat":[categories]}, flatten=[], ax=axes)

az.plot_trace(idata_cg)

plt.show()

## seems fine
az.summary(idata_cg, kind="stats").round(2)



## okay, how do we adapt this to our data? we want to compare 
## alpha diversity of three groups - crop, forest, and grassland.

sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 
controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
envData.drop(controls, axis='rows', inplace=True)
comData.drop(controls, axis='rows', inplace=True)

## we observed above that we can trust the raw species 
## richness counts from our community matrix, no need
## to rarify back or anything. 

## so how to get this in pandas/python?

aa = comData.copy()
aa[aa > 0] = 1
specRich = aa.sum(axis="columns")

comData.head()

aa.head()

aa.sum(axis="columns")

aa.sum(axis="columns").loc['S14']

aa.sum(axis="columns").loc['S102']


specRich = aa.sum(axis="columns")



pd.to_numeric(specRich, downcast='integer')

## maybe a df with all the info we need:

specRichLT = (pd.concat([pd.to_numeric(specRich, downcast='integer'), envData['Land_type']], axis='columns')
                     .rename({0:"spRich"},axis="columns"))

## looks right. so this should be our species richness. Might need this later:
specRichLT.to_csv("specRich.csv")

specRichLT.head()

## we want to get a distribution for the mean values of alpha diversity for
## each group 
 
specRichLT.head()

## can we visualize this with arviz first?:

az.plot_forest(specRichLT.pivot(columns="Land_type", values="spRich").to_dict("list"),
               kind="ridgeplot",
               hdi_prob=1,
               colors="C1",
               figsize=(12, 4))

## oswaldo's confusing code for creating an index, adapted for our data:
categories = np.array(["Arableland", "Grassland", "Forest"])
spr = specRichLT["spRich"].values
idx = pd.Categorical(specRichLT["Land_type"], categories=categories).codes
coords = {"Land_type": categories, "land_type_flat":categories[idx]}

with pm.Model(coords=coords) as comparing_groups:
    μ = pm.Normal("μ", mu=300, sigma=50, dims="Land_type")
    σ = pm.HalfNormal("σ", sigma=10, dims="Land_type")
    y = pm.Normal("Species richness", mu=μ[idx], sigma= σ[idx], observed=spr, dims="land_type_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))

    #y = pm.Gamma("y", mu=μ[idx], sigma=σ[idx], observed=spr, dims="land_type_flat") ## better for outliers?

plt.close('all')


with pm.Model(coords=coords) as comparing_groups:
    μ = pm.Normal("μ", mu=300, sigma=50, dims="Land_type")
    σ = pm.HalfNormal("σ", sigma=20, dims="Land_type")
    y = pm.Normal("Species richness", mu=μ[idx], sigma= σ[idx], observed=spr, dims="land_type_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))

plt.close('all')
fig, axes = plt.subplots(3, 1, sharex=True)
az.plot_ppc(idata_cg, num_pp_samples=100, coords={"land_type_flat":[categories]}, flatten=[], ax=axes)
fig.tight_layout()

## works, but the outliers are forcing a lot of variation into the posterior

## can we do this with cauchy?

with pm.Model(coords=coords) as model_t:
    μ = pm.Normal("μ", mu=300, sigma=50, dims="Land_type")
    σ = pm.HalfNormal("σ", sigma=20, dims="Land_type")
    ν = pm.Exponential('ν', 0.1, dims="Land_type") ## exponential gets flatter with lower values, mean gets pulled away from zero
    y = pm.StudentT('Species richness', nu=ν[idx], mu=μ[idx], sigma= σ[idx], observed=spr, dims="land_type_flat")
    idata_cg = pm.sample(random_seed=4591, chains=4)
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg, random_seed=4591))

## in the notebook, they give two models for this
## the second uses the gamma distribution.
## not run - why gamma? not sure. 

plt.close('all')

fig, axes = plt.subplots(3, 1, sharex=True)
az.plot_ppc(idata_cg, num_pp_samples=100, coords={"land_type_flat":[categories]}, flatten=[], ax=axes)
axes[0].set_xlim(200,400)
axes[1].get_legend().remove()
axes[2].get_legend().remove()
fig.tight_layout()


#### map of respiration values ####

## we still don't have a general analysis strategy,
## but it always helps to look at a map at the trait 
## of interest.

## we want to see rates of respiration across the 
## study. 

## it would be great to gave a vectorized land use map...
## does this exist somewhere?

## first, plot the respiration values:

envData.soil_respiration

plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )
sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration', 
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)

sulariPlot_utm = sulariPlotsDF.to_crs('EPSG:32633')

fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

## color edges by landtype:

landColorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
landCols = [ landColorDict[i] for i in sulariPlot_utm['Land_type'] ]
## 
plt.close('all')
fig, ax = plt.subplots()
rasterio.plot.show(fichtelMap, ax=ax)
ax.ticklabel_format(useOffset=False, style='plain')
sulariPlot_utm.plot(
    marker="o",
    ax=ax,
    cmap='YlOrRd',
    #cmap='RdPu',
    column='soil_respiration',
    edgecolors=landCols,
    linewidth=2,
    markersize=sulariPlot_utm['soil_respiration']*50,
     )

ax.set_ylim(5547500, 5570000)
ax.set_xlim(265000, 287000)
respMin = envData['soil_respiration'].min() ## 0.488057256
respMax = envData['soil_respiration'].max() ## 13.70117879
fig.colorbar(plt.cm.ScalarMappable(norm=matplotlib.colors.Normalize(respMin, respMax), cmap='YlOrRd'),
             ax=ax, orientation='vertical', label='resp')
Arableland_patch = Patch(color='#862d2d', label='Arableland')
Forest_patch = Patch(color='#006600', label='Forest')
Grassland_patch = Patch(color='#FF7F00', label='Grassland')
ax.legend(handles=[Forest_patch, Arableland_patch, Grassland_patch])

#### find species that are associated with high respiration (and low resp?) ##

## try deseq or bayesian equivalent. 
## or indicator species?
## cooccurrence networks?

## in general, we want species that are associated with high resp
## even in forests and croplands, which are the land types with 
## lower respiration rates generally. 

## we could cluster the communities and see if a cluster not related to
## to land type emerges. It is possible that a "highly respiring" community
## exists. But seems unlikely, across all land types. 

## take the highest respiring sites from each land type, and look for 
## species that occur only in them? or run deseq on each land 
## type alone and take the species positively associated with respiration

## see if there are any common species. We'll start with deseq, but 
## I think for publication I'd like to build some bayesian models for
## differential abundance


##### deseq ##### 

## need deseq. Let's install it in the overall environment, not just spatialDirt env


if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("DESeq2")

## we need raw abundances

R

library('phyloseq')
library('DESeq2')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

psLandCont <- ps ## make a duplicate phyloseq obj to play with

sample_data(psLandCont)[c('C1.1','C1.2','C2.1','C2.2'),'Land.type']  <- "control" ## add info

tail(sample_data(psLandCont)) ## looks okay

## okay, now following the tutorial above:

diagdds = phyloseq_to_deseq2(psLandCont, ~ Land.type)
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")

resultsNames(diagdds) ## Forest vs. Grassland not mentioned, but still possible

## let's see how the results look:
res <- results(diagdds, contrast=c("Land.type","Forest","Arableland"))

## edit down to highly significant results:

alpha = 0.1 ## significance cutoff
res <- res[complete.cases(res),] ## necessary because not all ASVs in all landtypes? 
## but then are those ASVs that are present only in one or two land types excluded here?
## these would be among the most important, I would think...think about this later
sigtab = res[(res$padj < alpha), ] ## cut off below the alpha using adjusted pvalues
## add taxonomy:
sigtab = cbind(as(sigtab, "data.frame"), as(tax_table(psLandCont)[rownames(sigtab), ], "matrix"))

## to check ASVs associated with respiration?

R

library('phyloseq')
library('DESeq2')
load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

## just like before make a duplicate phyloseq obj to play with
## and get rid of controls while we're at it
psNoControl = prune_samples(!(rownames(sample_data(ps)) %in% c("C1.1","C1.2","C2.1","C2.2")), ps)

## get rid of NAs in basalResp
basalRespNotNA <- !is.na(sample_data(psNoControl)$soil.respiration)
psNoControl = prune_samples(basalRespNotNA, psNoControl)
## okay, same old code as before:
diagdds = phyloseq_to_deseq2(psNoControl, ~ soil.respiration) ## set "treatment" of interest
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")
resultsNames(diagdds)
res <- results(diagdds)

alpha = 0.01 ## significance cutoff
res <- res[complete.cases(res),] ## necessary because not all ASVs in all landtypes
sigtab = res[(res$padj < alpha), ] ## cut off below the alpha using adjusted pvalues

sigtab$log2FoldChange

View(sigtab)



write.csv(sigtab, file="respDiffSeq.csv")

## these are all the strongly positively associated ASVs with resp:

highResp <- tax_table(ps)[c('ASV5', 'ASV13', 'ASV85', 'ASV110', 'ASV371', 'ASV462', 'ASV621',
       'ASV1089', 'ASV1419', 'ASV1795', 'ASV1831', 'ASV1905', 'ASV2058',
       'ASV2184', 'ASV2773')]

write.csv(highResp, file="hiRespTax.csv")


## there is a geobacter (ASV371) in there, maybe two (ASV2058). Weird.

ASV1831

## let's look at the rep sequence of these weird ones when we have time.


## since our "gene length" is exactly the same for all reads, I think we 
## can interpret baseMean as abundance 

## is this true? for instance, ASV2 has baseMean of 590. Seems like this 
## would be the abundance of ASV among the samples of lowest respiration values.

 
sample_data(psNoControl)$Basal.respiration

sample_variables(psNoControl)

## find some of these ASVs associated with increasing respiration - are they co-occurring? Are they where we expect them to be?

##### diffseq with land type as covariate #####


## if we want to check using covariate of land type, something like this?:

#diagdds = phyloseq_to_deseq2(psNoControl, ~ soil.respiration + Land.type) ## set "treatment" of interest
## ^ I think order matters here, covariate of interest should be last? 
## as per https://support.bioconductor.org/p/100828/

diagdds = phyloseq_to_deseq2(psNoControl, ~ Land.type + soil.respiration) ## set "treatment" of interest
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(diagdds), 1, gm_mean)
diagdds = estimateSizeFactors(diagdds, geoMeans = geoMeans)
diagdds = DESeq(diagdds, fitType="local")
resultsNames(diagdds)
res <- results(diagdds)

alpha = 0.01 ## significance cutoff
res <- res[complete.cases(res),] ## necessary because not all ASVs in all landtypes
sigtab = res[(res$padj < alpha), ] ## cut off below the alpha using adjusted pvalues
sigtab$log2FoldChange

## these should ASVs that are changing with respiration 
## regardless of land type...

aa <- sigtab[ sigtab$log2FoldChange > 0, ]
bb <- aa[ aa$baseMean > 1, ]

tax_table(ps)[row.names(bb)]

View(sigtab)

## with this approach, the asvs that come across as most important here are:

ASV1164 Chloroflexi
ASV1734 acidobacteriota, Bryobacter
ASV2626 alphaprot, Esterales 
ASV4902 Actinbacteriota, Solirubrobacteraceae
ASV6938 Alphaproteo, Sphingomonas

## what habitat do these microbes prefer?
## plotted below, seems like only ASV1164 is useful new info

## to be sure, maybe subset (split) by land type and run? We lose
## statistical power, but could be interesting.

## we need a way to rapidly map ASVs.

## we also need to figure out if these are cooccurring

## we have a bunch of old, crude tools...start with these, and think about 
## a proper bayesian model for finding which species are responding

## the bracod paper seems very promising, even though it has zero citations...
## hasn't been updated in a year but could be worse. Interesting, it 
## focuses on the species as predictors of a host trait. That fits our 
## setup just fine, with species as predictors of respiration. Can't
## tell what the backend is. 


## the other option seems to be BORAL. This has been cited a bunch, in nature 
## etc. Looks like a good package to know about. More generalized, etc.
## but also looks like we do 
## uses old gibbs samplers, I think. Weird. 

## let's give bracod a run tomorrow, see if it is easy to set up and use

## in the meantime, how do we map the observations of an ASV? 

## should be do-able in python with the data we have:


sulariEnvCSV="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
envData = pd.read_csv(sulariEnvCSV, index_col='SampleID')
comData = pd.read_csv("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/comdat.csv", 
                      index_col=0) 
controls=[ 'C1.1', 'C1.2', 'C2.1', 'C2.2']
envData.drop(controls, axis='rows', inplace=True)
comData.drop(controls, axis='rows', inplace=True)
plotPoints = gpd.points_from_xy( envData.Longitude, envData.Latitude, crs="EPSG:4326" )
sulariPlotsDF = gpd.GeoDataFrame(envData[[ 'PlotID', 'soil_respiration',
                    'MBC', 'season', 'Land_type', 'pH', 'N', 'C']], geometry=plotPoints)
sulariPlotsDF.to_crs('EPSG:32633', inplace=True)
landColorDict = {
'Arableland':'#862d2d',
'Forest'    :'#006600',
'Grassland' :'#FF7F00'
}
fichtelMap = rasterio.open("studyAreaClipped_UTM.tif")

fig,ax = plt.subplots()

## how can we generalize this so it can handle multiple ASVs? 

def mapOneASV(asv, ax=None, color="b", jitter=0, showLand=False):
    if ax is None: ax = plt.gca()
    jitX = sulariPlotsDF['geometry'].x.apply(lambda x: x + np.random.normal(loc=0, scale=jitter))
    jitY = sulariPlotsDF['geometry'].y.apply(lambda x: x + np.random.normal(loc=0, scale=jitter))
    asvPlotPoints = gpd.points_from_xy( jitX, jitY, crs="EPSG:32633" )
    asvGEO = gpd.GeoDataFrame(pd.concat([comData[asv], sulariPlotsDF['Land_type']], axis=1), geometry=asvPlotPoints, crs="EPSG:32633")
    if showLand: 
        asvGEO['landCols'] = [ landColorDict[i] for i in asvGEO['Land_type'] ]
    else: 
        asvGEO['landCols'] = "k"
    rasterio.plot.show(fichtelMap, ax=ax)
    asvGEO.plot(
        marker="o",
        ax=ax,
        linewidth=1,
        edgecolor=asvGEO['landCols'],
        facecolor=color,
        markersize=asvGEO[asv]*10000)
    return(asvGEO)

plt.close('all')

## then we can make maps as we see fit, with 0-inf otus.

mapOneASV("ASV2", color="b", jitter=0, showLand=True)

asv1 = mapOneASV("ASV1", color="r", jitter=0)

fig,ax = plt.subplots()

mapOneASV("ASV2", color="b", jitter=0)

mapOneASV("ASV1", color="r", jitter=0)

mapOneASV("ASV3", color="k", jitter=0)

## side note, ASV1 seems to be in a lot of plots...
## I thought this was E. coli from our mock community. Is this index bleed?

## how to check...

## this would mean diving back into the phyloseq pipeline and tracking 
## the prevalence of these MC otus...ugh, what a pain. 

## anyway, fairly low levels. If we are sticking to community level
## questions, probably not important.

## let's map some of the ASVs that look important for 

ASV1831 "Gemmataceae"         "Zavarzinella"
ASV371  "Geobacteraceae"      "Geobacter"

aa = mapOneASV("ASV1831", color="b", jitter=0)

aa = mapOneASV("ASV371", color="y", jitter=100)

aa[aa["ASV371"] > 0].index.values

envData.loc(bb)

envData.loc["S36"]
envData.loc["S70"]

## let's map the five most plentiful 
respASV = pd.read_csv("respDiffSeq.csv", index_col=0)

respASVtax = pd.read_csv("hiRespTax.csv", index_col=0)


respASV.head()

respASV.index


respASV.query("log2FoldChange > 1" ).index


## these are not so rare:

respASV.query("log2FoldChange > 1 & baseMean > 7")

## look at udeabacter


udea = mapOneASV("ASV621", color="b", jitter=0, showLand=True)

## interesting, but are any of these non-grassland species?
## subset to just forest sites, then run the deseq
## check tomorrow, have to work on some other things...

## here are some species that still appeared sensitive to 
## respiration, after accounting for land type, using deseq2 above

## ASV1164 Chloroflexi  forest
## ASV1734 acidobacteriota, Bryobacter
## ASV2626 alphaprot, Esterales 
## ASV4902 Actinbacteriota, Solirubrobacteraceae
## ASV6938 Alphaproteo, Sphingomonas

fig,ax = plt.subplots()

plt.close('all')

mapOneASV("ASV1164", color="b", jitter=0, showLand=True) ## definitely forest associated

aa = mapOneASV("ASV1734", color="b", jitter=0, showLand=True) ## definitely forest associated
aa = mapOneASV("ASV2626", color="b", jitter=0, showLand=True) ## only found in one plot above thresholds? So this probably disappears after our transformations
aa = mapOneASV("ASV4902", color="b", jitter=0, showLand=True) ## same: only found in one plot above thresholds? So this probably disappears after our transformations
aa = mapOneASV("ASV6938", color="b", jitter=0, showLand=True) ## also only one point. Oh jeez. 

(aa.iloc[:,0] > 0).sum()

## so the only the Chloroflexi from forests seems informative here.
## let's try subsetting by landtype:

## I think we need some multivariate approaches here. Networks and bayesian lms, like maybe bradco

## not sure if it will work with our current conda env?:

pip install BRACoD ## nope

## try a new env:

conda deactivate

conda create -n BRACoD python=3.6

conda activate BRACoD

pip install BRACoD

## theano not working...needs this?
conda install mkl-service

## that seems to have installed okay...test it out with sample data

## following https://github.com/ajverster/BRACoD

python

import BRACoD
import numpy as np

sim_counts, sim_y, contributions = BRACoD.simulate_microbiome_counts(BRACoD.df_counts_obesity)

sim_y ## our response variable 

contributions.shape

sim_relab = BRACoD.scale_counts(sim_counts)

help(BRACoD.run_bracod)

trace = BRACoD.run_bracod(sim_relab, sim_y, n_sample = 1000, n_burn=1000, njobs=4)

## too many cores? try defaults?
trace = BRACoD.run_bracod(sim_relab, sim_y, n_sample = 1000, n_burn=1000)


## whoah, this is slow...why?
## not working, freezes up. No real use of memory/cores


BRACoD.convergence_tests(trace, sim_relab)

conda env export > my_bracod_env.yaml


## that appears to be dead. 

## okay, that rules out bracod for the moment. Let's pick a cooccurence network 
## analysis method, try an install:

install.packages("devtools")

install.packages("BiocManager")

BiocManager::install("limma")

# Install NetCoMi
devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))


## try some sample features:

library(NetCoMi)

data("amgut1.filt")

amgut1.filt[0:10,0:10]


data("amgut2.filt.phy")

amgut2.filt.phy ## phyloseq obj


net_spring <- netConstruct(amgut1.filt,
                           filtTax = "highestFreq",
                           filtTaxPar = list(highestFreq = 50),
                           filtSamp = "totalReads",
                           filtSampPar = list(totalReads = 1000),
                           measure = "spring",
                           measurePar = list(nlambda=10, 
                                             rep.num=10,
                                             Rmethod = "approx"),
                           normMethod = "none", 
                           zeroMethod = "none",
                           sparsMethod = "none", 
                           dissFunc = "signed",
                           verbose = 2,
                           seed = 123456)



props_spring <- netAnalyze(net_spring, 
                           centrLCC = TRUE,
                           clustMethod = "cluster_fast_greedy",
                           hubPar = "eigenvector",
                           weightDeg = FALSE, normDeg = FALSE)


#?summary.microNetProps


summary(props_spring, numbNodes = 5L)

p <- plot(props_spring, 
          nodeColor = "cluster", 
          nodeSize = "eigenvector",
          title1 = "Network on OTU level with SPRING associations", 
          showTitle = TRUE,
          cexTitle = 2.3)

legend(0.7, 1.1, cex = 2.2, title = "estimated association:",
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"), 
       bty = "n", horiz = TRUE)

## or with pearson:

net_pears <- netConstruct(amgut2.filt.phy,  
                          measure = "pearson",
                          normMethod = "clr",
                          zeroMethod = "multRepl",
                          sparsMethod = "threshold",
                          thresh = 0.3,
                          verbose = 3)

## let's try it on our data.

## we can use a centered log transformation, pearson correlation as per the examples 
## on the NetCoMi github

load("/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sulariData/sulariPhyloseqObject.rda")

net_pears <- netConstruct(ps,  
                          measure = "pearson",
                          normMethod = "clr",
                          zeroMethod = "multRepl", ## don't understand totally...
                          sparsMethod = "threshold", ## don't understand totally...
                          thresh = 0.3,
                          verbose = 3)

## killed. To much memory required?


props_pears <- netAnalyze(net_pears, 
                          clustMethod = "cluster_fast_greedy")

plot(props_pears, 
     nodeColor = "cluster", 
     nodeSize = "eigenvector",
     title1 = "Network on OTU level with Pearson correlations", 
     showTitle = TRUE,
     cexTitle = 2.3)

legend(0.7, 1.1, cex = 2.2, title = "estimated correlation:", 
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"), 
       bty = "n", horiz = TRUE)


## can we port this over to the lab computer?

nanoComp

## let's try using the house R install:

## make a working directory:

cd /media/vol1/daniel/sulariArne/soilAnalysis


#library('DESeq2')

if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install('phyloseq')

library('phyloseq')

setwd("/media/vol1/daniel/sulariArne/soilAnalysis")

download.file("https://github.com/danchurch/fichtelgebirgeSoils/raw/main/sulariData/sulariPhyloseqObject.rda", destfile="sulariPhyloseqObject.rda")

load("sulariPhyloseqObject.rda")

ps ## looks ok


BiocManager::install("limma")

BiocManager::install("zCompositions")

install.packages("devtools") 

devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))


## dev.tools failed. 
## the following packages failed. Work on it tomorrow. 

install.packages("systemfonts")
## which needs...
sudo apt install libfontconfig1-dev

install.packages(       "xml2")
## which needs...
sudo apt install libxml2-dev

install.packages("textshaping")
## which needs...
sudo apt install libharfbuzz-dev libfribidi-dev

install.packages(  "rversions") ## easy
install.packages( "urlchecker") ## easy

install.packages(    "openssl")
## which needs
sudo apt install libssl-dev

install.packages(       "ragg")
## which needs
sudo apt install libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev

install.packages("credentials") ## easy
install.packages(      "httr2") ## easy
install.packages(       "httr") ## easy
install.packages(       "gert") ## easy
install.packages(         "gh") ## easy
install.packages(    "usethis") ## easy
install.packages(    "pkgdown") ## easy
install.packages(   "roxygen2") ## easy

## and...
install.packages(   "devtools")

devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))

## failed again. Lots of failed dependencies:
‘SpiecEasi’, ‘mixedCCA’, ‘qgraph’, ‘SPRING’, ‘WGCNA’ are not available for package ‘NetCoMi’


library(devtools)
install_github("zdk123/SpiecEasi")

## which needs:

## which needs fortran ??
sudo apt install gfortran
## and the following are not there:
ld -llapack --verbose
ld -lblas --verbose
sudo apt install liblapack-dev libopenblas-dev

install.packages("mixedCCA")
install.packages("qgraph") 
install.packages("SPRING")
install.packages("WGCNA")

## try again 
devtools::install_github("stefpeschel/NetCoMi", 
                         dependencies = c("Depends", "Imports", "LinkingTo"),
                         repos = c("https://cloud.r-project.org/",
                                   BiocManager::repositories()))

library('NetCoMi') ## no errors...finally...

R

library('phyloseq')
library('NetCoMi')
setwd("/media/vol1/daniel/sulariArne/soilAnalysis")

## does the plotter work on x11 forwarding?

plot(1) ## looks okay, for base plotter

#download.file("https://github.com/danchurch/fichtelgebirgeSoils/raw/main/sulariData/sulariPhyloseqObject.rda", destfile="sulariPhyloseqObject.rda")

load("sulariPhyloseqObject.rda")

## to run our data, we want a pretty rigorous minimum abundance threshold. 
## for all of our vegan stuff, we use a minimum abundance of 50 reads 
## per observation. 

## to be clear, we want all sample counts below 50 to be come zero

dropLow <- function(x) {
                           if (x < 0) {x = 0}
                          }

dropLow <- function(x) {
                           x <- x - 50
                           if (x < 0) {x = 0}
                           x
                          }

dropLow(100)
ps.atLeast50 <- transform_sample_counts(ps.filter.fam, dropLow)

## nope, can't handle if statements. Let' pull out the matrix and work 
## on it directly:


setwd("/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis")

load("../sulariData/sulariPhyloseqObject.rda")
library('vegan')
library('phyloseq')
library('NetCoMi')

ps.atLeast50 <- ps
aa <- otu_table(ps)
bb <- aa - 50 
bb[bb < 0] <- 0
otu_table(ps.atLeast50) <- bb
## can we trim out empty otus now? 
ps.atLeast50 <- prune_taxa( taxa_sums(ps.atLeast50) > 0, ps.atLeast50 )

otu_table(ps)[0:10,0:10]

otu_table(ps.atLeast50)[0:10,0:10]

dim(otu_table(ps)) 
dim(otu_table(ps.atLeast50)) ## down to just 4280 taxa, out of 36140

## ok, looks right
## now pass that to network software:

## just curious, can it handle the full ps? want to see how this changes the network.

#net_pears_fullPS <- netConstruct(ps,  
#                          measure = "pearson",
#                          normMethod = "clr",
#                          zeroMethod = "multRepl", ## don't understand totally...
#                          sparsMethod = "threshold", ## don't understand totally...
#                          thresh = 0.3,
#                          verbose = 3)

## nope, dies

## try smaller object:

net_pears_PSatleast50 <- netConstruct(ps.atLeast50,  
                          measure = "pearson",
                          normMethod = "clr",
                          zeroMethod = "multRepl", ## don't understand totally...
                          sparsMethod = "threshold", ## don't understand totally...
                          thresh = 0.3,
                          verbose = 3)

## that was computationally expensive:
save(net_pears_PSatleast50, file = "net_pears_PSatleast50.rda") ## 139 mb, pretty darn big..

props_pears <- netAnalyze(net_pears_PSatleast50, 
                          clustMethod = "cluster_fast_greedy")


?netConstruct

## huh, we can use a custom count matrix

## which means we can add columns. 
## so let's make a matrix that includes land-type, and 

?netAnalyze

## not run
plot(props_pears, 
     nodeColor = "cluster", 
     nodeSize = "eigenvector",
     title1 = "Network on OTU level with Pearson correlations", 
     showTitle = TRUE,
     cexTitle = 2.3)

legend(0.7, 1.1, cex = 2.2, title = "estimated correlation:", 
       legend = c("+","-"), lty = 1, lwd = 3, col = c("#009900","red"), 
       bty = "n", horiz = TRUE)

## plan for the day:

## rerun network analysis with environment variables land-use and respiration

## if it works, update notebook with it:



### on another note, we need to run jupyter on the lab computer...

## means 
## - install jupyter
## - clone repo to lab comp
##  maybe best to do this as we did on my own computer, within a conda env

## can we use our current spatialDirt yaml for this?

conda env export > spatialDirt_15.4.24.yaml

## update repo, clone onto lab
## on the lab comp, try to use it to make a new environment:

cd /media/vol1/daniel/sulariArne/soilAnalysis

wget https://raw.githubusercontent.com/danchurch/fichtelgebirgeSoils/main/spatialDirt_15.4.24.yaml 

conda update -n base conda

conda install -n base conda-libmamba-solver

conda config --set solver libmamba

## get the standard channels

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

conda env create --name spatialDirt --file=spatialDirt_15.4.24.yaml

## lots of errors...let's see...installing kernels as above

## is the R kernel on there?

## try: 
## https://stackoverflow.com/questions/44056164/jupyter-client-has-to-be-installed-but-jupyter-kernelspec-version-exited-wit

## in a sudo R sesh on lab comp:

install.packages('IRkernel')

system.file('kernelspec', package = 'IRkernel')

## gives us:
"/usr/local/lib/R/site-library/IRkernel/kernelspec"


## which we can give to our notebook
jupyter kernelspec install "/usr/local/lib/R/site-library/IRkernel/kernelspec" \
  --name "R" \
  --user 

jupyter kernelspec list

cd /media/vol1/daniel/miniconda3/envs/spatialDirt



## as per this site:
https://stackoverflow.com/questions/69244218/how-to-run-a-jupyter-notebook-through-a-remote-server-on-local-machine

jupyter notebook --no-browser --port=8080

ssh -L 8080:localhost:8080 test@132.180.112.115

## almost works. wants passwords and tokens and stuff:
https://jupyter-server.readthedocs.io/en/latest/operators/public-server.html

## for now use tokens...


## we need to synch up with git

## on the nanocomp

## as per above, the nanocomp computer repo needs 
## to be given permissions, etc.

cd /media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis

git config --global user.email "danchurchthomas@gmail.com"

git config --global user.name "danchurch"

git remote add origin https://github.com/danchurch/fichtelgebirgeSoils.git
git branch -M main
git remote set-url origin git@github.com:danchurch/fichtelgebirgeSoils.git

git push -u origin main

## nope, don't think I put keys for lab comp on there

cd ~/.ssh
ssh-keygen -t rsa -f nanoComp2git

## and as per here:
https://stackoverflow.com/questions/13509293/git-fatal-could-not-read-from-remote-repository
## the ssh-agent had to be started up and informed about the new key:

eval `ssh-agent -s`
ssh-add ~/.ssh/nanoComp2git

## and this had to be added to the bashrc. weird. never had to do that before...

## anyway, github seems synched. 
## and does that mean we can carefully work remotely with the jupyter notebook now?

## when we actually get comfortable with this, we can get back to editing this
## text file local. For now, both vim and jupyter are on the nanocomp computer:


cd /media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis

conda activate spatialDirt 

jupyter notebook --no-browser --port=8080

## run this to activate the tcp forwarding
ssh -L 8080:localhost:8080 test@132.180.112.115

## it's going to be confusing working on both computers. 
## just slow it down, save/pull/push.

## calculating the network statistics takes for ever with netcommi

## can we multithread? its only using one core...


## I think we need to revisit the network software 

## we need some how a vector of the ASVs most responsive to 
## respiration, added to a cooccurrence matrix.

## we need to mkae our own adjacency matrix, I guess. 

## anyway, to get the candidates for responsiveness to 
## respiration...seems like we can't avoid a full multivariate
## treatment of the community matrix?

## let's take a look at the BORAL tool...



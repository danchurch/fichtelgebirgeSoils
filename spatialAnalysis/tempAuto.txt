## let's develop a strategy for check seasonal autocorrelation

## we'll merge this later with the main text

## the most basic strategy would be to put the abundances of the 
## previous season as predictor for the current
## but hurts my head - use multiple community matrices 
## as a predictor for the current one? 

## another possibility that seems more likely is that HMSC can 
## treat time as a "spatial" variable

## in addition to or instead of spatial patterns? It looks 
## likes we can include as many "coordinates" as we want with 
## our "spatial" random level.

## can their be two spatial random effects? As in, a true spatial
## and also a time random effect?

## first step would be to identify just those sites that have 
## repeated sampling. We did this before...

import pandas as pd
import natsort as ns
import numpy as np

sulariEnvCSV="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/spatialAnalysis/sulariEnv.csv"
sulariUTM="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/spatialAnalysis/sulariSpatial.csv"
envData = (pd.read_csv(sulariEnvCSV, index_col='SampleID')
               .drop(['Latitude', 'Longitude'], axis='columns'))
spatData = pd.read_csv(sulariUTM, index_col='SampleID')
repFilt = envData['PlotID'].duplicated(keep=False)
repeatedPlots = envData[repFilt].sort_values(by="PlotID")
repeatedPlots = repeatedPlots.merge(spatData, how='inner', left_index=True, right_index=True)
## drop the P0228, not actually repeated:
repeatedPlots.drop(['S89','S102'], axis='rows', inplace=True)
## use natsort to clean up sample name order
repeatedPlots = repeatedPlots.loc[ ns.natsorted(repeatedPlots.index)]
repeatedPlots['Date'] = pd.to_datetime(repeatedPlots['Date'], yearfirst=True, errors='coerce')

## how many different sampling times do each have?
repeatedPlots[["PlotID","Date"]].groupby("PlotID").nunique()

## I think the temporal unit should be season. 
## we need to bin the dates then

repeatedPlots['Date'].duplicated().any() ##  no repeats, can be an index:

repeatedPlots = (repeatedPlots
                    .sort_values(by='Date')
                    .reset_index()
                    .set_index('Date')
)

## we'll use a "meteorological" definition

repeatedPlots['season'] = 0
## so winter of 2022 
repeatedPlots.loc['2020-01-01':'2022-02-18', 'season'] = 1
## spring of 2022
repeatedPlots.loc['2022-03-01':'2022-05-31', 'season'] = 2
## summer of 2022
repeatedPlots.loc['2022-06-01':'2022-09-30', 'season'] = 3
## fall of 2022
repeatedPlots.loc['2022-10-01':'2022-12-31', 'season'] = 4

repeatedPlots['season']

repeatedPlots.groupby('season').size() #actually fairly even distribution

## clean up rownames:
repeatedPlots = repeatedPlots.loc[ ns.natsorted(repeatedPlots.index)]

## save out for R
repeatedPlots.to_csv("repeatedPlotsEnv.csv")

## so we can use this dataframe in R to subset the phyloseq object
## down to size

## when reloading, date format is lost

repeatedPlots = pd.read_csv("repeatedPlotsEnv.csv", index_col='SampleID')
repeatedPlots['Date'] = pd.to_datetime(repeatedPlots['Date'], yearfirst=True, errors='coerce')

#### R , hmsc spatiotemproal model ####

## now, how to insert the temporal effects here?
## we could try testing temporal effects only, or in combination
## with space
## I think the point of using this approach is to incorporate 
## all data, or as much as possible, so let's throw it all in there...
## but how?

## two options - convert to season, or do number-days
## seasonality is intriguing, because of all the corellated 
## predictors, temperature moisture, etc

## will they let us do two separated auto-correlated random effects?

library(Hmsc)
library(phyloseq)
library(ape)

rm(list=ls())
envData <- read.csv("repeatedPlotsEnv.csv", row.names="SampleID")
load("psCleanedUp.rda")
## trim down to repeated sites, removed zeroed species
repeatedSitesPS = prune_samples(rownames(envData), psCleanedUp)
repeatedSitesPS = prune_taxa( colSums(otu_table(repeatedSitesPS)) > 0, repeatedSitesPS)
bactRaw <- otu_table(repeatedSitesPS)@.Data
bact.pa<-ifelse(bactRaw>0,1,0)
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat
threshold.prev = 0.05 ## present in at least n% of samples
threshold.abu = 0.002 ## only OTU that reaches at least n% abundance of one sample
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples?
cond2=apply(bact.rel,2,max)>=threshold.abu
bactData <- bactRaw[, cond1 & cond2 ]
sum(bactData > 0)
sum(bact.pa)
dim(bactData)
dim(bact.pa)
Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
varsOfInterest = c("PlotID","soil_respiration","Land_type","pH","N","C","Temperature","waterCont","season","xx","yy")
XData <- data.frame(envData[,varsOfInterest])
XData$Land_type <- as.factor(XData$Land_type)
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about.
Ypa = 1*(Y>0)
ccases <- complete.cases(XData) ## we only lose 1, down to 33 samples
XData <- XData[ccases,]
Y = Y[ccases,]
Ypa = Ypa[ccases,]
Yabu = Yabu[ccases,]
Ycombo = cbind(Ypa,Yabu)
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))
sample.id = row.names(Y)
samplePlotid = XData$PlotID
studyDesign = data.frame("sample"=as.factor(sample.id), 
                         "site_spatial"=as.factor(samplePlotid),
                         "season"=as.integer(XData$season)
                          )

plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID
plot_coords$PlotID <- NULL

plot_coords <- XData[,c("PlotID","xx","yy")]

varsOfmodel <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont","season")
XData <- XData[,c(varsOfmodel)]
XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature
rL.sample = HmscRandomLevel(units = sample.id)

## try two "spatial" random effects?
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) ## try 3. Not sure how to optimize this. But there may multiple spatial scales.

rL.site_season = HmscRandomLevel(sData = plot_coords)

rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=1) ## I can imagine that such a simple predictor needs more?


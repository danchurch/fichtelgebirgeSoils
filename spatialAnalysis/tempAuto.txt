## let's develop a strategy for check seasonal autocorrelation

## we'll merge this later with the main text

## the most basic strategy would be to put the abundances of the 
## previous season as predictor for the current
## but hurts my head - use multiple community matrices 
## as a predictor for the current one? 

## another possibility that seems more likely is that HMSC can 
## treat time as a "spatial" variable

## in addition to or instead of spatial patterns? It looks 
## likes we can include as many "coordinates" as we want with 
## our "spatial" random level.

## can their be two spatial random effects? As in, a true spatial
## and also a time random effect?

## first step would be to identify just those sites that have 
## repeated sampling. We did this before...

import pandas as pd
import natsort as ns
import numpy as np

sulariEnvCSV="sulariEnv.csv"
sulariUTM="sulariSpatial.csv"
envData = (pd.read_csv(sulariEnvCSV, index_col='SampleID')
               .drop(['Latitude', 'Longitude'], axis='columns'))
spatData = pd.read_csv(sulariUTM, index_col='SampleID')
repFilt = envData['PlotID'].duplicated(keep=False)
repeatedPlots = envData[repFilt].sort_values(by="PlotID")
repeatedPlots = repeatedPlots.merge(spatData, how='inner', left_index=True, right_index=True)
## drop the P0228, not actually repeated:
repeatedPlots.drop(['S89','S102'], axis='rows', inplace=True)
## use natsort to clean up sample name order
repeatedPlots = repeatedPlots.loc[ ns.natsorted(repeatedPlots.index)]
repeatedPlots['Date'] = pd.to_datetime(repeatedPlots['Date'], yearfirst=True, errors='coerce')
## how many different sampling times do each have?
repeatedPlots[["PlotID","Date"]].groupby("PlotID").nunique()
## I think the temporal unit should be season. 
## we need to bin the dates then
repeatedPlots['Date'].duplicated().any() ##  no repeats, can be an index:
repeatedPlots.sort_values(by='Date', inplace=True)
repeatedPlots = (repeatedPlots
                    .reset_index()
                    .set_index('Date')
)
## we'll use a "meteorological" definition
repeatedPlots['season'] = 'zoop'
## so winter of 2022 
repeatedPlots.loc['2020-01-01':'2022-02-18', 'season'] = 'winter'
## spring of 2022
repeatedPlots.loc['2022-03-01':'2022-05-31', 'season'] = 'spring'
## summer of 2022
repeatedPlots.loc['2022-06-01':'2022-09-30', 'season'] = 'summer'
## fall of 2022
repeatedPlots.loc['2022-10-01':'2022-12-31', 'season'] = 'fall'
## clean up rownames:
repeatedPlots = (repeatedPlots
                    .reset_index()
                    .set_index('SampleID')
)
repeatedPlots = repeatedPlots.loc[ ns.natsorted(repeatedPlots.index)]
## save out for R
#repeatedPlots.to_csv("repeatedPlotsEnv.csv")

## so we can use this dataframe in R to subset the phyloseq object
## down to size

## when reloading, date format is lost:
repeatedPlots = pd.read_csv("repeatedPlotsEnv.csv", index_col='SampleID')
repeatedPlots['Date'] = pd.to_datetime(repeatedPlots['Date'], yearfirst=True, errors='coerce')

#### R , hmsc spatiotemproal model ####

## now, how to insert the temporal effects here?
## we could try testing temporal effects only, or in combination
## with space
## I think the point of using this approach is to incorporate 
## all data, or as much as possible, so let's throw it all in there...
## but how?

## two options - convert to season, or do number-days
## seasonality is intriguing, because of all the corellated 
## predictors, temperature moisture, etc

## will they let us do two separated auto-correlated random effects?

library(Hmsc)
library(phyloseq)
library(ape)
rm(list=ls())
envData <- read.csv("repeatedPlotsEnv.csv", row.names="SampleID")
load("psCleanedUp.rda")
## trim down to repeated sites, removed zeroed species
repeatedSitesPS = prune_samples(rownames(envData), psCleanedUp)
repeatedSitesPS = prune_taxa( colSums(otu_table(repeatedSitesPS)) > 0, repeatedSitesPS)
bactRaw <- otu_table(repeatedSitesPS)@.Data
bact.pa<-ifelse(bactRaw>0,1,0)
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat
## real model:
#threshold.prev = 0.05 ## present in at least n% of samples
#threshold.abu = 0.002 ## only OTU that reaches at least n% abundance of one sample
#####
## toy model, greatly simplified for testing:
#threshold.prev = 0.05 ## present in at least n% of samples
threshold.abu = 0.005 ## only OTU that reaches at least n% abundance of one sample
#####
ny = dim(bactRaw)[1]
#cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples?
cond2=apply(bact.rel,2,max)>=threshold.abu
bactData <- bactRaw[, cond2 ]
#bactData <- bactRaw[, cond1 & cond2 ]
sum(bactData > 0)
sum(bact.pa)
dim(bactData)
dim(bact.pa)
Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
varsOfInterest = c("PlotID","season","soil_respiration","Land_type","pH","N","C","Temperature","waterCont","xx","yy")
XData <- data.frame(envData[,varsOfInterest])
XData$Land_type <- as.factor(XData$Land_type)
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about.
Ypa = 1*(Y>0)
ccases <- complete.cases(XData) ## we only lose 1, down to 33 samples
XData <- XData[ccases,]
Y = Y[ccases,]
Ypa = Ypa[ccases,]
Yabu = Yabu[ccases,]
Ycombo = cbind(Ypa,Yabu)
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))
samplePlotid = XData$PlotID
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID
plot_coords$PlotID <- NULL
## use this for our spatial random effects
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) ## try 3. Not sure how to optimize this. But there may multiple spatial scales.
## random effects by sample is simple:
sample.id = row.names(Y)
rL.sample = HmscRandomLevel(units = sample.id)
## now, how do we implement our temporal variable, season?:
## acts on individual samples, not plots, necessarily
## I think we need a subtable that links to a column in our experimental design df 
## one column would be "season" and the other would be the integer value 
seasonDF = data.frame(row.names=c( "winter", "spring", "summer" , "fall" ), seasNumber=1:4)
rL.site_season = HmscRandomLevel(sData = seasonDF)
rL.site_season = setPriors(rL.site_season,nfMin=1,nfMax=1) ## I can imagine that such a simple predictor needs more?
## get the general ecological predictors ("fixed effects"), and formula
varsOfmodel <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont")
XData0 <- XData[,c(varsOfmodel)] ## clean up to just these for model below
XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature
## I think this temporal effect acts on the individual sample level
## this issue is pertinent: https://github.com/hmsc-r/HMSC/issues/103
## we need a study design that has one row for each date-plot combo,
studyDesign = data.frame("sample"=as.factor(sample.id), 
                         "site_spatial"=as.factor(samplePlotid),
                         "season"=as.factor(XData$season)
                          )
m_SpTmpSmall = Hmsc(Y=Ycombo,
                   XData = XData0,  XFormula = XFormula0,
                   distr={"probit"} ,
                   studyDesign=studyDesign,
                   ranLevels={list("sample"=rL.sample, 
                                   "site_spatial" = rL.site_spatial,
                                   "season" = rL.site_season
                                  )})
save(m_SpTmpSmall, file="m_SpTmpSmall.rda")


## sanity check for random effects, these pairs of numbers should match
rL.sample
length(levels(as.factor(studyDesign$sample)))

rL.site_spatial
length(levels(as.factor(studyDesign$site_spatial)))

rL.site_season
length(levels(as.factor(studyDesign$season)))


## try sampling this. cutoff is 0.5% of abundances of at least one sample
## gives us 258 species
## synch up the lab computer, then run:
## 

## sample_m_SpTmpSmall.r
#########################################################
library(Hmsc)
print(paste("start time is", Sys.time()))
load("m_SpTmpSmall.rda")
thin = 50 ## modified from 5 previously
samples = 1000
nChains = 2
nP = 2
m_SpTmpSmall = sampleMcmc(m_SpTmpSmall,
                     samples = samples,
                     thin = thin,
                     transient = ceiling(0.5*samples*thin),
                     nChains = nChains,
                     nParallel = nP,
                     verbose=1)
save(m_SpTmpSmall, file="m_SpTmpSmall_sampled_thin50.rda")
print(paste("finish time is", Sys.time()))
#########################################################
## with the 50 thin, this took ~4 hours:
## "start time is 2024-05-29 15:16:56.161548"
## "finish time is 2024-05-29 19:12:01.006627"

## oops, deleted our unsampled model, might need it:

git restore --source  80af950f5c3d6dc517aab46bdeb09584ae90366a m_SpTmpSmall.rda

nohup Rscript sample_m_SpTmpSmall.r &> sample_m_SpTmpSmall.log & ## 


soil
conda activate spatialDirt

R

library(Hmsc)

load("m_SpTmpSmall_sampled.rda")

mpost = convertToCodaObject(m_SpTmpSmall) 

sink(file="summary_m_SpTmpSmall_sampled.txt")
summary(mpost$Beta) 
sink()

## plotTrace_m_SpTmpSmall.r 
####### script for printing plots #####

library(Hmsc)
load("m_SpTmpSmall_sampled.rda")
mpost = convertToCodaObject(m_SpTmpSmall) 
pdf(file="traces_m_SpTmpSmall_sampled.pdf")
plot(mpost$Beta)
dev.off()

#######################################

nohup Rscript plotTrace_m_SpTmpSmall.r

## evince won't forward, get it:

getFile="/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis/traces_m_SpTmpSmall_sampled.pdf"
putItHere="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/files2big4git"
scp test@132.180.112.115:$getFile $putItHere

## large, gives the trace of every asv against every variable in the model

effectiveSize(mpost$Beta)

sink(file="effectiveSize_m_SpTmpSmall_sampled.txt")
effectiveSize(mpost$Beta)
sink()

## the effective size for these is really small.
## not sure why. Given that the model only ran for 
## 20 min, I guess we can afford to run it longer tonight
## and maybe increase the number of species?

es.beta <- effectiveSize(mpost$Beta)
ge.beta <- gelman.diag(mpost$Beta,multivariate=FALSE)$psrf

par(mfrow=c(1,2)) 
hist(es.beta, main="effective sample size for beta") 
hist(ge.beta, main="potential scale reduction factor for beta)")

## yup, looks like we didn't sample nearly enough
## set up another sample event, longer...modified above
## should take ~10 times longer

es.gamma = effectiveSize(mpost$Gamma)
ge.gamma = gelman.diag(mpost$Gamma,multivariate=FALSE)$psrf

par(mfrow=c(1,2)) 
hist(es.gamma, main="effective sample size for gamma") 
hist(ge.gamma, main="potential scale reduction factor for gamma)")
## oh yeah, this is shit, ess is down below 100 for all, near zero for a lot

mpost$Omega

mpost$Omega[[1]]

mpost$temp = mpost$Omega[[1]]
for(i in 1:length(mpost$temp)){
  mpost$temp[[i]] = mpost$temp[[i]][,1:1000]
}

es.omega1 = effectiveSize(effectiveSize(mpost$Omega[[1]]))
ge.omega1 = gelman.diag(mpost$Omega[[1]], multivariate=FALSE)

hist(es.omega1, main="ess(omega)") 
hist(ge.omega1$psrf, main="psrf(omega)")

## okay, so it's shit. but for now, keep moving through the analysis
## the goal is to get a skeleton of an analysis ready for when the 
## real data hits

## yeah, this is taking forever. Let's make some scripts, following 
## the fungal paper:

## checkConvergenceSpTmpSmall.r
########## script to check convergences ###########
library(Hmsc)
load("m_SpTmpSmall_sampled_thin50.rda")
m = m_SpTmpSmall; rm(m_SpTmpSmall)
mpost = convertToCodaObject(m)
es.beta = effectiveSize(mpost$Beta)
ge.beta = gelman.diag(mpost$Beta,multivariate=FALSE)$psrf
png(file="essBetaSpTmpSmall.png")
par(mfrow=c(1,2)) 
hist(es.beta, main="effective sample size for beta") 
hist(ge.beta, main="potential scale reduction factor for beta)")
dev.off()
es.gamma = effectiveSize(mpost$Gamma)
ge.gamma = gelman.diag(mpost$Gamma,multivariate=FALSE)$psrf
png(file="essGammaSpTmpSmall.png")
par(mfrow=c(1,2)) 
hist(es.gamma, main="effective sample size for gamma") 
hist(ge.gamma, main="potential scale reduction factor for gamma)")
dev.off()
es.V = effectiveSize(mpost$V)
ge.V = gelman.diag(mpost$V,multivariate=FALSE)$psrf
png(file="essGammaSpTmpSmall.png")
par(mfrow=c(1,2)) 
hist(es.V, main="effective sample size for V") 
hist(ge.V, main="potential scale reduction factor for V)")
dev.off()
es.omega = effectiveSize(mpost$Omega[[1]])
ge.omega = gelman.diag(mpost$Omega[[1]], multivariate=FALSE)$psrf
png(file="essGammaSpTmpSmall.png")
par(mfrow=c(1,2)) 
hist(effectiveSize(, main="ess(omega)") 
hist(gelman.diag(, main="psrf(omega)")
dev.off()
mixing = list(es.beta=es.beta, ge.beta=ge.beta,
              es.gamma=es.gamma, ge.gamma=ge.gamma,
              es.V=es.V, ge.V=ge.V,
              es.omega=es.omega, ge.omega=ge.omega)
save(mixing,file="m_SpTmpSmall_mixingStats.rda")
 
########## ^^script to check convergences^^ ###########

nohup Rscript checkConvergenceSpTmpSmall.r &> checkConvergenceSpTmpSmall.log & 


?evaluateModelFit

m_SpTmpSmall

aa <- evaluateModelFit(hM = m_SpTmpSmall, predY = preds)

aa

## can also check the predictive power

postBeta = getPostEstimate(m, parName = "Beta") 

plotBeta(m, post = postBeta, param = "Support", supportLevel = 0.95)


############## check species-species interactions #########

OmegaCor = computeAssociations(m) 
supportLevel = 0.95 
toPlot = ((OmegaCor[[1]]$support>supportLevel) 
           + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean 
corrplot(toPlot, method = "color", 
         col = colorRampPalette(c("blue","white","red"))(200), 
         title = paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))




## let's develop a strategy for check seasonal autocorrelation

## we'll merge this later with the main text

## the most basic strategy would be to put the abundances of the 
## previous season as predictor for the current
## but hurts my head - use multiple community matrices 
## as a predictor for the current one? 

## another possibility that seems more likely is that HMSC can 
## treat time as a "spatial" variable

## in addition to or instead of spatial patterns? It looks 
## likes we can include as many "coordinates" as we want with 
## our "spatial" random level.

## can their be two spatial random effects? As in, a true spatial
## and also a time random effect?

## first step would be to identify just those sites that have 
## repeated sampling. We did this before...

import pandas as pd
import natsort as ns
import numpy as np

sulariEnvCSV="sulariEnv.csv"
sulariUTM="sulariSpatial.csv"
envData = (pd.read_csv(sulariEnvCSV, index_col='SampleID')
               .drop(['Latitude', 'Longitude'], axis='columns'))
spatData = pd.read_csv(sulariUTM, index_col='SampleID')
repFilt = envData['PlotID'].duplicated(keep=False)
repeatedPlots = envData[repFilt].sort_values(by="PlotID")
repeatedPlots = repeatedPlots.merge(spatData, how='inner', left_index=True, right_index=True)
## drop the P0228, not actually repeated:
repeatedPlots.drop(['S89','S102'], axis='rows', inplace=True)
## use natsort to clean up sample name order
repeatedPlots = repeatedPlots.loc[ ns.natsorted(repeatedPlots.index)]
repeatedPlots['Date'] = pd.to_datetime(repeatedPlots['Date'], yearfirst=True, errors='coerce')
## how many different sampling times do each have?
repeatedPlots[["PlotID","Date"]].groupby("PlotID").nunique()
## I think the temporal unit should be season. 
## we need to bin the dates then
repeatedPlots['Date'].duplicated().any() ##  no repeats, can be an index:
repeatedPlots.sort_values(by='Date', inplace=True)
repeatedPlots = (repeatedPlots
                    .reset_index()
                    .set_index('Date')
)
## we'll use a "meteorological" definition
repeatedPlots['season'] = 'zoop'
## so winter of 2022 
repeatedPlots.loc['2020-01-01':'2022-02-18', 'season'] = 'winter'
## spring of 2022
repeatedPlots.loc['2022-03-01':'2022-05-31', 'season'] = 'spring'
## summer of 2022
repeatedPlots.loc['2022-06-01':'2022-09-30', 'season'] = 'summer'
## fall of 2022
repeatedPlots.loc['2022-10-01':'2022-12-31', 'season'] = 'fall'
## clean up rownames:
repeatedPlots = (repeatedPlots
                    .reset_index()
                    .set_index('SampleID')
)
repeatedPlots = repeatedPlots.loc[ ns.natsorted(repeatedPlots.index)]

## save out for R
#repeatedPlots.to_csv("repeatedPlotsEnv.csv")

repeatedPlots.head()

repeatedPlots.PlotID.unique()

## so we can use this dataframe in R to subset the phyloseq object
## down to size

## when reloading, date format is lost:
repeatedPlots = pd.read_csv("repeatedPlotsEnv.csv", index_col='SampleID')
repeatedPlots['Date'] = pd.to_datetime(repeatedPlots['Date'], yearfirst=True, errors='coerce')

#### R , hmsc spatiotemproal model ####

## now, how to insert the temporal effects here?
## we could try testing temporal effects only, or in combination
## with space
## I think the point of using this approach is to incorporate 
## all data, or as much as possible, so let's throw it all in there...
## but how?

## two options - convert to season, or do number-days
## seasonality is intriguing, because of all the corellated 
## predictors, temperature moisture, etc

## will they let us do two separated auto-correlated random effects?

library(Hmsc)
library(phyloseq)
library(ape)
rm(list=ls())
envData <- read.csv("repeatedPlotsEnv.csv", row.names="SampleID")
load("psCleanedUp.rda")
## trim down to repeated sites, removed zeroed species
repeatedSitesPS = prune_samples(rownames(envData), psCleanedUp)
repeatedSitesPS = prune_taxa( colSums(otu_table(repeatedSitesPS)) > 0, repeatedSitesPS)
bactRaw <- otu_table(repeatedSitesPS)@.Data
bact.pa<-ifelse(bactRaw>0,1,0)
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat
## real model:
#threshold.prev = 0.05 ## present in at least n% of samples
#threshold.abu = 0.002 ## only OTU that reaches at least n% abundance of one sample
#####
## toy model, greatly simplified for testing:
#threshold.prev = 0.05 ## present in at least n% of samples
threshold.abu = 0.005 ## only OTU that reaches at least n% abundance of one sample
#####
ny = dim(bactRaw)[1]
#cond1=!(colSums(bact.pa)<=threshold.prev*ny) ## present in at least n% of samples?
cond2=apply(bact.rel,2,max)>=threshold.abu
bactData <- bactRaw[, cond2 ]
#bactData <- bactRaw[, cond1 & cond2 ]
sum(bactData > 0)
sum(bact.pa)
dim(bactData)
dim(bact.pa)
Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
varsOfInterest = c("PlotID","season","soil_respiration","Land_type","pH","N","C","Temperature","waterCont","xx","yy")
XData <- data.frame(envData[,varsOfInterest])
XData$Land_type <- as.factor(XData$Land_type)
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about.
Ypa = 1*(Y>0)
ccases <- complete.cases(XData) ## we only lose 1, down to 33 samples
XData <- XData[ccases,]
Y = Y[ccases,]
Ypa = Ypa[ccases,]
Yabu = Yabu[ccases,]
Ycombo = cbind(Ypa,Yabu)
ny = dim(Y)[1]
ns = dim(Y)[2]/2
all(rownames(XData) == rownames(Y))
samplePlotid = XData$PlotID
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID
plot_coords$PlotID <- NULL
## use this for our spatial random effects
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) ## try 3. Not sure how to optimize this. But there may multiple spatial scales.
## random effects by sample is simple:
sample.id = row.names(Y)
rL.sample = HmscRandomLevel(units = sample.id)
## now, how do we implement our temporal variable, season?:
## acts on individual samples, not plots, necessarily
## I think we need a subtable that links to a column in our experimental design df 
## one column would be "season" and the other would be the integer value 
seasonDF = data.frame(row.names=c( "winter", "spring", "summer" , "fall" ), seasNumber=1:4)
rL.site_season = HmscRandomLevel(sData = seasonDF)
rL.site_season = setPriors(rL.site_season,nfMin=1,nfMax=1) ## I can imagine that such a simple predictor needs more?
## get the general ecological predictors ("fixed effects"), and formula
varsOfmodel <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont")
XData0 <- XData[,c(varsOfmodel)] ## clean up to just these for model below
XFormula0 = ~ soil_respiration + Land_type + pH + N + C + Temperature
## I think this temporal effect acts on the individual sample level
## this issue is pertinent: https://github.com/hmsc-r/HMSC/issues/103
## we need a study design that has one row for each date-plot combo,
studyDesign = data.frame("sample"=as.factor(sample.id), 
                         "site_spatial"=as.factor(samplePlotid),
                         "season"=as.factor(XData$season)
                          )
m_SpTmpSmall = Hmsc(Y=Ycombo,
                   XData = XData0,  XFormula = XFormula0,
                   distr={"probit"} ,
                   studyDesign=studyDesign,
                   ranLevels={list("sample"=rL.sample, 
                                   "site_spatial" = rL.site_spatial,
                                   "season" = rL.site_season
                                  )})
save(m_SpTmpSmall, file="m_SpTmpSmall.rda")


## sanity check for random effects, these pairs of numbers should match
rL.sample
length(levels(as.factor(studyDesign$sample)))

rL.site_spatial
length(levels(as.factor(studyDesign$site_spatial)))

rL.site_season
length(levels(as.factor(studyDesign$season)))


## try sampling this. cutoff is 0.5% of abundances of at least one sample
## gives us 258 species
## synch up the lab computer, then run:

## sample_m_SpTmpSmall.r
#########################################################
library(Hmsc)
print(paste("start time is", Sys.time()))
load("m_SpTmpSmall.rda")
thin = 300 ## modified from 5,50,150 previously
samples = 1000
nChains = 2
nP = 2
m_SpTmpSmall = sampleMcmc(m_SpTmpSmall,
                     samples = samples,
                     thin = thin,
                     transient = ceiling(0.5*samples*thin),
                     nChains = nChains,
                     nParallel = nP,
                     verbose=1)
#save(m_SpTmpSmall, file="m_SpTmpSmall_sampled_thin300.rda")
save(m_SpTmpSmall, file="m_SpTmpSmall_sampled_thin300_2.rda")
print(paste("finish time is", Sys.time()))
#########################################################

## with the 50 thin, this took ~4 hours
## with the 150 thin, this took ~10 hours
## with the 300 thin, this took ~33 hours

## let's try duplicating this, and adding the sampling 
## together
## oops, deleted our unsampled model, might need it:
#git restore --source  80af950f5c3d6dc517aab46bdeb09584ae90366a m_SpTmpSmall.rda

## try running this twice, and add them together later

soil
conda activate spatialDirt
nohup Rscript sample_m_SpTmpSmall.r &>> m_SpTmpSmall_sampled_thin300.log & 

## this is precious, get these local. Too big for github...
getFile="/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis/m_SpTmpSmall_sampled_thin300.rda"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git/"
scp test@132.180.112.115:$getFile $putItHere


R

library(Hmsc)

load("m_SpTmpSmall_sampled.rda")

mpost = convertToCodaObject(m_SpTmpSmall) 

sink(file="summary_m_SpTmpSmall_sampled.txt")
summary(mpost$Beta) 
sink()

## plotTrace_m_SpTmpSmall.r 
####### script for printing plots #####

library(Hmsc)
load("m_SpTmpSmall_sampled.rda")
mpost = convertToCodaObject(m_SpTmpSmall) 
pdf(file="traces_m_SpTmpSmall_sampled.pdf")
plot(mpost$Beta)
plot(mpost$Beta)
dev.off()

#######################################

nohup Rscript plotTrace_m_SpTmpSmall.r

## evince won't forward, get it:

getFile="/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis/traces_m_SpTmpSmall_sampled.pdf"
putItHere="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/files2big4git"
scp test@132.180.112.115:$getFile $putItHere

## large, gives the trace of every asv against every variable in the model

effectiveSize(mpost$Beta)

sink(file="effectiveSize_m_SpTmpSmall_sampled.txt")
effectiveSize(mpost$Beta)
sink()

## the effective size for these is really small.
## not sure why. Given that the model only ran for 
## 20 min, I guess we can afford to run it longer tonight
## and maybe increase the number of species?

es.beta <- effectiveSize(mpost$Beta)
ge.beta <- gelman.diag(mpost$Beta,multivariate=FALSE)$psrf

par(mfrow=c(1,2)) 
hist(es.beta, main="effective sample size for beta") 
hist(ge.beta, main="potential scale reduction factor for beta)")

## yup, looks like we didn't sample nearly enough
## set up another sample event, longer...modified above
## should take ~10 times longer

es.gamma = effectiveSize(mpost$Gamma)
ge.gamma = gelman.diag(mpost$Gamma,multivariate=FALSE)$psrf

par(mfrow=c(1,2)) 
hist(es.gamma, main="effective sample size for gamma") 
hist(ge.gamma, main="potential scale reduction factor for gamma)")
## oh yeah, this is shit, ess is down below 100 for all, near zero for a lot
## upped the thin to 50, and this improved things quite a bit
## but still need more, try 150? this will be pretty long, but get it 
## started  

mpost$Omega

mpost$Omega[[1]]

mpost$temp = mpost$Omega[[1]]
for(i in 1:length(mpost$temp)){

  mpost$temp[[i]] = mpost$temp[[i]][,1:1000]
}

es.omega1 = effectiveSize(effectiveSize(mpost$Omega[[1]]))
ge.omega1 = gelman.diag(mpost$Omega[[1]], multivariate=FALSE)

hist(es.omega1, main="ess(omega)") 
hist(ge.omega1$psrf, main="psrf(omega)")

## okay, so it's shit. but for now, keep moving through the analysis
## the goal is to get a skeleton of an analysis ready for when the 
## real data hits

## yeah, this is taking forever. Let's make some scripts, following 
## the fungal paper:

library(Hmsc)

## checkConvergenceSpTmpSmall.r
########## script to check convergences ###########

library(Hmsc)
load("m_SpTmpSmall_sampled_thin150.rda")
m = m_SpTmpSmall; rm(m_SpTmpSmall)
mpost = convertToCodaObject(m)

es.beta = effectiveSize(mpost$Beta)
ge.beta = gelman.diag(mpost$Beta,multivariate=FALSE)$psrf
png(file="essBetaSpTmpSmall_thin150.png")
par(mfrow=c(1,2))
hist(es.beta, main="effective sample size for beta")
hist(ge.beta, main="potential scale reduction factor for beta)")
dev.off()

es.gamma = effectiveSize(mpost$Gamma)
ge.gamma = gelman.diag(mpost$Gamma,multivariate=FALSE)$psrf
png(file="essGammaSpTmpSmall_thin150.png")
par(mfrow=c(1,2))
hist(es.gamma, main="effective sample size for gamma")
hist(ge.gamma, main="potential scale reduction factor for gamma)")
dev.off()

es.V = effectiveSize(mpost$V)
ge.V = gelman.diag(mpost$V,multivariate=FALSE)$psrf
png(file="essV_SpTmpSmall_thin150.png")
par(mfrow=c(1,2))
hist(es.V, main="effective sample size for V")
hist(ge.V, main="potential scale reduction factor for V)")
dev.off()

## omega is too large. take the first thousand species/species interactions
mpost$temp = mpost$Omega[[1]]
for(i in 1:length(mpost$temp)){
  mpost$temp[[i]] = mpost$temp[[i]][,1:1000]
}
es.omega1 = effectiveSize(mpost$temp)
ge.omega1 = gelman.diag(mpost$temp,multivariate=FALSE)$psrf
png(file="essOmega1_SpTmpSmall_thin150.png")
par(mfrow=c(1,2))
hist(es.omega1, main="ess(omega)")
hist(ge.omega1, main="psrf(omega)")
dev.off()

mixing = list(es.beta=es.beta, ge.beta=ge.beta,
              es.gamma=es.gamma, ge.gamma=ge.gamma,
              es.V=es.V, ge.V=ge.V,
              es.omega=es.omega1, ge.omega=ge.omega1)
save(mixing, file="m_SpTmpSmall_sampled_thin150_mixingStats.rda")


########################################################

soil
conda activate spatialDirt

nohup Rscript checkConvergenceSpTmpSmall.r &> checkConvergenceSpTmpSmall_thin150.log &

## get a look at the mixing plots in the fully sampled model

R

library(Hmsc)

#load("m_SpTmpSmall_sampled_thin50.rda")

load("m_SpTmpSmall_sampled_thin150.rda")

mpost = convertToCodaObject(m_SpTmpSmall)

pdf(file="traces_m_SpTmpSmall_sampled_thin150.pdf")
plot(mpost$Beta)
dev.off()

## get it local
getFile="/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis/traces_m_SpTmpSmall_sampled_thin150.pdf"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git/"
scp test@132.180.112.115:$getFile $putItHere

#####################################################

## testing model fit 

library(Hmsc)

#load("m_SpTmpSmall_sampled_thin50.rda")
load("m_SpTmpSmall_sampled_thin150.rda")
m = m_SpTmpSmall; rm(m_SpTmpSmall)

?evaluateModelFit

## predictive:

predY.MF = computePredictedValues(m, expected=TRUE)

MF = evaluateModelFit(hM=m, predY=predY.MF)

hist(MF$AUC)

sum(is.na(MF$AUC))

(MF$AUC)

hist(MF$RMSE)

length(MF$RMSE)

(MF$RMSE > 0)

hist(MF$TjurR2)

(MF$TjurR2)

## actually, looks pretty good with the thin50 model, even though mixing wasn't perfect

## can also check the predictive power


## this takes a very long time, probably days, so make a script:

## samplePredictivePower.r
########################################################

library(Hmsc)

print(paste("start time is", Sys.time()))
load("m_SpTmpSmall_sampled_thin300.rda")
m = m_SpTmpSmall; rm(m_SpTmpSmall)

partition=createPartition(hM=m, nfolds=2)
predY.CV = computePredictedValues(m, expected=TRUE, partition=partition, nChains = 1, nParallel = 4)

MF.CV = evaluateModelFit(hM=m, predY=predY.CV)

save(list(predY.CV,MF.CV), file="m_SpTmpSmall_sampled_modelFit_thin300.rda")

postBeta = getPostEstimate(m, parName = "Beta") 

png("postBeta_support_thin300.png")
plotBeta(m, post = postBeta, param = "Support", supportLevel = 0.95)
dev.off()

print(paste("finish time is", Sys.time()))

########################################################



nohup Rscript samplePredictivePower.r &> samplePredictivePower_thin300.log &

############## check species-species interactions #########

soil; conda activate spatialDirt; R


library(Hmsc)
library(corrplot)

load("m_SpTmpSmall_sampled_thin300.rda")

m = m_SpTmpSmall; rm(m_SpTmpSmall)

OmegaCor = computeAssociations(m) 

save(OmegaCor, file='OmegaCor_SpTmpSmall_thin300.rda')

system('ls -lh OmegaCor_SpTmpSmall_thin300.rda')


?computeAssociations

supportLevel = 0.0  ## let's see them all

str(OmegaCor[[1]])


## as found in tutorial
toPlot = ((OmegaCor[[1]]$support>supportLevel) 
           + (OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean 

## I think this gives only posivitive associations?

toPlot = ((OmegaCor[[1]]$support>supportLevel)>0)*OmegaCor[[1]]$mean 


toPlot = ((OmegaCor[[2]]$support>supportLevel) 
           + (OmegaCor[[2]]$support<(1-supportLevel))>0)*OmegaCor[[2]]$mean 

toPlot = ((OmegaCor[[3]]$support>supportLevel) 
           + (OmegaCor[[3]]$support<(1-supportLevel))>0)*OmegaCor[[3]]$mean 

m$rLNames[1]

m$rLNames

dev.new()



aa <- toPlot[1:258,1:258]

corrplot(toPlot, method = "color", 
         col = colorRampPalette(c("blue","white","red"))(200), 
         title = paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0),
         ylim = c(0,258), xlim = c(0,258))

corrplot(aa, method = "color", 
         col = colorRampPalette(c("blue","white","red"))(200), 
         title = paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0),
         ylim = c(0,258), xlim = c(0,258))


## the example in the book (p180) is like this:

## i: 1 
## j 1=readAbund only, 2=full model

for (i in 1:3){
    for (j in 1:2){
        OmegaCor = computeAssociations(models[[i]][[j]])
        supportLevel = 0.95
        toPlot = ((OmegaCor[[1]]$support > supportLevel)
        + (OmegaCor[[1]]$support < (1-supportLevel))
        > 0)
        * OmegaCor[[1]]$mean
        corrplot(toPlot, method = “color”,
        col = c(“grey”,“white”,“black”)
        }
}

## we ran all of ours as one model. We should also run a null model,
## to show the effect of incorporating the covariates on the species
## cooccurrences. This is also useful for ordinations using the
## latent variables 

## it looks to me like we may need separate models to investigate 
## single species associated with high respiration and 
## species associations? Depends on our abundance cutoffs, I think.

## put back in the read Abundances as a variable and build  

## so let's try to follow the book example as closely as possible,
## section 7.9. Split up the hurdle model into two sampling
## events, and add a "null" model. 

## the models have been rewritten on for the full spatial 
## model, maybe do this for this model. Grab the above code and
## cleanup a bit/restructure:

## on nanoComp:


############# spatiotemp, 3 models #######################

soil; conda activate r_env; R

soil; conda activate spatialDirt; R

library(Hmsc)
library(phyloseq)
library(ape)

rm(list=ls())

envData <- read.csv("repeatedPlotsEnv.csv", row.names="SampleID")
load("psCleanedUp.rda")
## trim down to repeated sites, removed zeroed species
repeatedSitesPS = prune_samples(rownames(envData), psCleanedUp)
repeatedSitesPS = prune_taxa( colSums(otu_table(repeatedSitesPS)) > 0, repeatedSitesPS)
varsOfInterest = c("PlotID","soil_respiration","Land_type","pH","N","C","Temperature","waterCont","season","xx","yy")
XData<-data.frame(envData[,varsOfInterest])
XData$Land_type <- as.factor(XData$Land_type)
## we can only use complete cases
ccases <- complete.cases(XData) 
XData = XData[ccases,] ## lose one sample
bactRaw <- otu_table(repeatedSitesPS)@.Data
bactRaw <- bactRaw[ccases,]
XData$Land_type <- as.factor(XData$Land_type)
XData['BacteriaDepth'] <- log(rowSums(bactRaw))
bact.pa<-ifelse(bactRaw>0,1,0)
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat
##################### thresholds!! ###########
## not sure the appropriate thresholds her
## in the case of the full spatial model, 
## ended up subsetting down to ~160 OTUs
threshold.prev = 6/33 ## not a lot of difference between 0 and 6
threshold.abu = 0.01 ## only OTU that reaches at least 1% abundance of one sample
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny)
cond2=apply(bact.rel,2,max)>=threshold.abu
bactData <- bactRaw[, cond1 & cond2 ]
sum(bactData > 0)
sum(bact.pa)
dim(bactData) ## 81 OTUs remain, out of
dim(bact.pa) ## 2903 OTUs
################################################
## 81 OTUs, should go fast
Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes?
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about.
Ypa = 1*(Y>0)
all(rownames(XData) == rownames(Y))
samplePlotid = XData$PlotID
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID
plot_coords$PlotID <- NULL

## use this for our spatial random effects
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) ## try 3. Not sure how to optimize this. But there may multiple spatial scales.
## random effects by sample is simple:
sample.id = row.names(Y)
rL.sample = HmscRandomLevel(units = sample.id)
## now, how do we implement our temporal variable, season?:
## acts on individual samples, not plots, necessarily
## I think we need a subtable that links to a column in our experimental design df 
## one column would be "season" and the other would be the integer value 
seasonDF = data.frame(row.names=c( "winter", "spring", "summer" , "fall" ), seasNumber=1:4)
rL.site_season = HmscRandomLevel(sData = seasonDF)
rL.site_season = setPriors(rL.site_season,nfMin=1,nfMax=1) ## I can imagine that such a simple predictor needs more?
## set the covariates for "null" model:
XData0 <- XData[,"BacteriaDepth",drop=FALSE]
XFormula0 = ~ BacteriaDepth

## set the covariates for hurdle model:
varsOfmodel1 <- c("soil_respiration","Land_type","pH","N","C","Temperature","waterCont")
XData1 <- XData[,c(varsOfmodel1)] ## clean up to just these for model below
XFormula1 = ~ soil_respiration + Land_type + pH + N + C + Temperature
## I think this temporal effect acts on the individual sample level
## this issue is pertinent: https://github.com/hmsc-r/HMSC/issues/103
## we need a study design that has one row for each date-plot combo,
studyDesign = data.frame("sample"=as.factor(sample.id), 
                         "site_spatial"=as.factor(samplePlotid),
                         "season"=as.factor(XData$season)
                          )
## sanity check for random effects, these pairs of numbers should match
rL.sample
length(levels(as.factor(studyDesign$sample)))
rL.site_spatial
length(levels(as.factor(studyDesign$site_spatial)))
rL.site_season
length(levels(as.factor(studyDesign$season)))
## we want three models,
## one that contains only the covariate of read depth
## one that is based on presence absence (probit) and all important env data
## and one that is based on read abundances and all important env data
## for the null abundance model with no environmental covariates, the textbook uses
## a lognormal poisson:
m_spatTempSmall_noEnv = Hmsc(Y=Yabu,
         XData = XData0,  XFormula = XFormula0,
         distr={"lognormal poisson"} ,
         studyDesign=studyDesign,
         ranLevels={list("sample"=rL.sample, 
                         "site_spatial" = rL.site_spatial,
                         "season" = rL.site_season)},
         YScale=TRUE
)
save(m_spatTempSmall_noEnv, file="m_spatTempSmall_noEnv_01percent.rda")
## for the PA model with all environmental covariates:
m_spatTempSmall_PA = Hmsc(Y=Ypa,
         XData = XData1,  XFormula = XFormula1,
         distr={"probit"} ,
         studyDesign=studyDesign,
         ranLevels={list("sample"=rL.sample, 
                         "site_spatial" = rL.site_spatial,
                         "season" = rL.site_season)},
         #YScale=TRUE
)
save(m_spatTempSmall_PA, file="m_spatTempSmall_PA_01percent.rda")
## for the abundance model with all environmental covariates:
m_spatTempSmall_Abu = Hmsc(Y=Yabu,
         XData = XData1,  XFormula = XFormula1,
         distr={"normal"} ,
         studyDesign=studyDesign,
         ranLevels={list("sample"=rL.sample, 
                         "site_spatial" = rL.site_spatial,
                         "season" = rL.site_season)},
         YScale=TRUE
)
save(m_spatTempSmall_Abu, file="m_spatTempSmall_Abu_01percent.rda")

## to sample these posteriors, let's do a script for each. the 1-by-1 
## approach is very slow...

################## sampleHurdleSpatTemp_null_01percent.r #######################

library(Hmsc)

print(paste("start time is", Sys.time()))
load("m_spatTempSmall_noEnv_01percent.rda")
thin = 20
samples = 1000
nChains = 4
nP = 4
m_spatTempSmall_noEnv = sampleMcmc(m_spatTempSmall_noEnv,
                     samples = samples,
                     thin=thin,
                     transient = ceiling(0.5*samples*thin),
                     nChains = nChains,
                     nParallel = nP
                     )
save(m_spatTempSmall_noEnv, file="m_spatTempSmall_noEnv_01percent_sampled.rda")
print(paste("finish time is", Sys.time()))
################################################################################


################## sampleHurdleSpatTemp_PA_01percent.r #######################
library(Hmsc)
print(paste("start time is", Sys.time()))
load("m_spatTempSmall_PA_01percent.rda")
thin = 20
samples = 1000
nChains = 4
nP = 4
m_spatTempSmall_PA = sampleMcmc(m_spatTempSmall_PA,
                     samples = samples,
                     thin=thin,
                     transient = ceiling(0.5*samples*thin),
                     nChains = nChains,
                     nParallel = nP
                     )
save(m_spatTempSmall_PA, file="m_spatTempSmall_PA_01percent.rda_sampled.rda")
print(paste("finish time is", Sys.time()))
################################################################################

################## sampleHurdleSpatTemp_abu_01percent.r #######################
library(Hmsc)
print(paste("start time is", Sys.time()))
load("m_spatTempSmall_Abu_01percent.rda")
thin = 20
samples = 1000
nChains = 4
nP = 4
m_spatTempSmall_Abu = sampleMcmc(m_spatTempSmall_Abu,
                     samples = samples,
                     thin=thin,
                     transient = ceiling(0.5*samples*thin),
                     nChains = nChains,
                     nParallel = nP
                     )
save(m_spatTempSmall_Abu, file="m_spatTempSmall_Abu_01percent_sampled.rda")
print(paste("finish time is", Sys.time()))
################################################################################

## in bash:
nohup Rscript sampleHurdleSpatTemp_PA_01percent.r   &> sampleHurdleSpatTemp_PA_01percent.log &
nohup Rscript sampleHurdleSpatTemp_abu_01percent.r   &> sampleHurdleSpatTemp_abu_01percent.log &
## this failed, rerun
nohup Rscript sampleHurdleSpatTemp_null_01percent.r &> sampleHurdleSpatTemp_null_01percent.log &

## fails again. Can't repeat the error. Maybe due to improper loading of environment?
## possible issue here:
https://github.com/hmsc-r/HMSC/issues/47

load("m_spatTempSmall_noEnv_01percent.rda")

find . -type f -size +20M -exec ls -lh {} \;

##########################

## ok, back after a month, and forgotten everything...

## remember, the goal is to find the species associated with high respiration,
## and maybe predict bacterial presence or abundace with raised temperature or deforestation

## I was running two models - a spatial model, and a spatiotemporal model
## the spatial model had a lot more points, but is simpler (no temporal component)
## the spatial-only model was run on de.nbi, the spatiotemporal on the lab computer 
 
## the spatiotemporal was buggy. Was working on debugging, but maybe focus on the spatial
## model for the moment 

## let's try copying and pasting the old model build process,
## modify to include the new emission data

nanoComp

soil

conda activate spatialDirt

python3

import pandas as pd
import os

## we need to update the repeatedPlots environmental data

repeatedPlotsEnv = pd.read_csv("repeatedPlotsEnv.csv", index_col="SampleID")
envData = pd.read_csv("sulariEnv.csv", index_col="SampleID")
repeatedPlotsEnv = pd.merge(repeatedPlotsEnv, envData["co2flow"], left_index=True, right_index=True)
#repeatedPlotsEnv.to_csv("repeatedPlotsEnv.csv")

## read it back, check:
repeatedPlotsEnv = pd.read_csv("repeatedPlotsEnv.csv", index_col="SampleID")
repeatedPlotsEnv['Date'] = pd.to_datetime(repeatedPlotsEnv['Date'], yearfirst=True, errors='coerce')
repeatedPlotsEnv ## looks okay

## back to R, modify model to use the new CO2 data:

nanoComp

soil; conda activate spatialDirt; R

library(Hmsc)
library(phyloseq)

rm(list=ls())
envData <- read.csv("repeatedPlotsEnv.csv", row.names="SampleID")
load("psCleanedUp.rda")
## trim down to repeated sites, removed zeroed species
repeatedSitesPS = prune_samples(rownames(envData), psCleanedUp)
repeatedSitesPS = prune_taxa( colSums(otu_table(repeatedSitesPS)) > 0, repeatedSitesPS)
## lost ~400 OTUs, means that the vast majority of species are 
## still in our spatiotemporal subsample
varsOfInterest = c("PlotID","soil_respiration","co2flow","Land_type","pH","N","C","Temperature","waterCont","season","xx","yy")
XData<-data.frame(envData[,varsOfInterest])
XData$Land_type <- as.factor(XData$Land_type)
## we can only use complete cases
ccases <- complete.cases(XData) 
XData = XData[ccases,] ## lose one sample
bactRaw <- otu_table(repeatedSitesPS)@.Data
bactRaw <- bactRaw[ccases,]
XData$Land_type <- as.factor(XData$Land_type)
XData['BacteriaDepth'] <- log(rowSums(bactRaw))
bact.pa<-ifelse(bactRaw>0,1,0)
rowsumsInv = 1/rowSums(bactRaw)
nASVs <- dim(bactRaw)[2]
scalarMat = matrix( rep(rowsumsInv, nASVs), ncol = nASVs)
bact.rel <- bactRaw * scalarMat
##################### thresholds!! ###########
## not sure the appropriate thresholds her
## in the case of the full spatial model, 
## ended up subsetting down to ~160 OTUs
threshold.prev = 6/33 ## not a lot of difference between 0 and 6
threshold.abu = 0.008 ## only OTU that reaches at least 0.8% abundance of one sample
ny = dim(bactRaw)[1]
cond1=!(colSums(bact.pa)<=threshold.prev*ny)
cond2=apply(bact.rel,2,max)>=threshold.abu
bactData <- bactRaw[, cond1 & cond2 ]
sum(bactData > 0)
sum(bact.pa)
dim(bactData) ## 123 OTUs remain, out of
dim(bact.pa) ## 2903 OTUs
################################################
Y <- bactData
ny <- dim(Y)[1]
ns <- dim(Y)[2]
Yabu <- Y
Yabu[Y==0] <- NA ## to avoid zeroes
Yabu = log(Yabu) ## take log of abundances. They basically decided on the same transformations as Sulari and I.
Yabu <- scale(Yabu) ## scale, after log? Heavily transformed data. Let's hope they know what they are talking about.
Ypa = 1*(Y>0)
all(rownames(XData) == rownames(Y))
samplePlotid = XData$PlotID
plot_coords <- XData[,c("PlotID","xx","yy")]
plot_coords <- plot_coords[!duplicated(plot_coords),] ## remove duplicated plot rows
rownames(plot_coords) <- plot_coords$PlotID
plot_coords$PlotID <- NULL

## use this for our spatial random effects
rL.site_spatial = HmscRandomLevel(sData = plot_coords)
rL.site_spatial = setPriors(rL.site_spatial,nfMin=1,nfMax=3) ## try 3. Not sure how to optimize this. But there may multiple spatial scales.
## random effects by sample is simple:
sample.id = row.names(Y)
rL.sample = HmscRandomLevel(units = sample.id)
## now, how do we implement our temporal variable, season?:
## acts on individual samples, not plots, necessarily
## I think we need a subtable that links to a column in our experimental design df 
## one column would be "season" and the other would be the integer value 
seasonDF = data.frame(row.names=c( "winter", "spring", "summer" , "fall" ), seasNumber=1:4)
rL.site_season = HmscRandomLevel(sData = seasonDF)
rL.site_season = setPriors(rL.site_season,nfMin=1,nfMax=1) ## I can't imagine that such a simple predictor needs more?
## set the covariates for "null" model:
XData0 <- XData[,"BacteriaDepth",drop=FALSE]
XFormula0 = ~ BacteriaDepth
## set the covariates for hurdle model:
varsOfmodel1 <- c("soil_respiration","co2flow","Land_type","pH","N","C","Temperature","waterCont")
XData1 <- XData[,c(varsOfmodel1)] ## clean up to just these for model below
XFormula1 = ~ soil_respiration + co2flow + Land_type + pH + N + C + Temperature
## I think this temporal effect acts on the individual sample level
## this issue is pertinent: https://github.com/hmsc-r/HMSC/issues/103
## we need a study design that has one row for each date-plot combo,
studyDesign = data.frame("sample"=as.factor(sample.id), 
                         "site_spatial"=as.factor(samplePlotid),
                         "season"=as.factor(XData$season)
                          )
## sanity check for random effects, these pairs of numbers should match
rL.sample
length(levels(as.factor(studyDesign$sample)))
rL.site_spatial
length(levels(as.factor(studyDesign$site_spatial)))
rL.site_season
length(levels(as.factor(studyDesign$season)))

as.factor(XData$season)

## build the models

## spatiotemporal model, but no environmental effects, just read depths
## 1. m_spatTemp_ReadAbundancesOnly
m_spatTemp_ReadAbundancesOnly = Hmsc(Y=Y,
         XData = XData0,  XFormula = XFormula0,
         distr={"lognormal poisson"} ,
         studyDesign=studyDesign,
         ranLevels={list("sample"=rL.sample, 
                         "site_spatial" = rL.site_spatial,
                         "season" = rL.site_season)},
         YScale=TRUE
)
save(m_spatTemp_ReadAbundancesOnly, file="m_spatTemp_ReadAbundancesOnly.rda")

## for comparison, a model with full environmental covariates and spatial patterning, but 
## no temporal effects? To be fully comparable, we need to do the two-model hurdle 
## approach:

## 2.1 m_spatTemp_noTemp_PA
m_spatTemp_noTemp_PA = Hmsc(Y=Ypa,
         XData = XData1,  XFormula = XFormula1,
         distr={"probit"} ,
         studyDesign=studyDesign,
         ranLevels={list("sample"=rL.sample, 
                         "site_spatial" = rL.site_spatial
                         )},
)
save(m_spatTemp_noTemp_PA, file="m_spatTemp_noTemp_PA.rda")

## 2.2 m_spatTemp_noTemp_abu
m_spatTemp_noTemp_abu = Hmsc(Y=Yabu,
         XData = XData1,  XFormula = XFormula1,
         distr={"normal"} ,
         studyDesign=studyDesign,
         ranLevels={list("sample"=rL.sample, 
                         "site_spatial" = rL.site_spatial)},
         YScale=TRUE
                    )
save(m_spatTemp_noTemp_abu, file="m_spatTemp_noTemp_abu.rda")

## the full spatiotemporal, environmental hurdle model:
## 3.1 m_spatTemp_Env_PA
m_spatTemp_Env_PA = Hmsc(Y=Ypa,
         XData = XData1,  XFormula = XFormula1,
         distr={"probit"} ,
         studyDesign=studyDesign,
         ranLevels={list("sample"=rL.sample, 
                         "site_spatial" = rL.site_spatial,
                         "season" = rL.site_season)}
              )
save(m_spatTemp_Env_PA, file="m_spatTemp_Env_PA.rda")

## 3.2 m_spatTemp_Env_abu
m_spatTemp_Env_abu = Hmsc(Y=Yabu,
         XData = XData1,  XFormula = XFormula1,
         distr={"normal"} ,
         studyDesign=studyDesign,
         ranLevels={list("sample"=rL.sample,
                         "site_spatial" = rL.site_spatial,
                         "season" = rL.site_season)},
         YScale=TRUE
)

save(m_spatTemp_Env_abu, file="m_spatTemp_Env_abu.rda")

## let's try running the first three on the lab comp.
## if memory is too high, kill it. If low, add the other models:

m_spatTemp_ReadAbundancesOnly.rda
m_spatTemp_noTemp_PA.rda
m_spatTemp_noTemp_abu.rda
m_spatTemp_Env_PA.rda
m_spatTemp_Env_abu.rda

## 1.  m_spatTemp_ReadAbundancesOnly
#### m_spatTemp_ReadAbundancesOnly.r ####

library(Hmsc)
print("starting model:")
print("m_spatTemp_ReadAbundancesOnly")
print(paste("start time is", Sys.time()))
load("m_spatTemp_ReadAbundancesOnly.rda")
thin = 20
samples = 1000
nChains = 8
nP = 8
m_spatTemp_ReadAbundancesOnly = sampleMcmc(m_spatTemp_ReadAbundancesOnly,
                     samples = samples,
                     thin=thin,
                     transient = ceiling(0.5*samples*thin),
                     nChains = nChains,
                     nParallel = nP
                     )
saveRDS(m_spatTemp_ReadAbundancesOnly, file="m_spatTemp_ReadAbundancesOnly_sampled.rds")
print(paste("finish time is", Sys.time()))

###########################################

## 2.1 m_spatTemp_noTemp_PA
#### m_spatTemp_noTemp_PA.r ####

library(Hmsc)
print("starting model:")
print("m_spatTemp_noTemp_PA")
print(paste("start time is", Sys.time()))
load("m_spatTemp_noTemp_PA.rda")
thin = 20
samples = 1000
nChains = 8
nP = 2
m_spatTemp_noTemp_PA = sampleMcmc(m_spatTemp_noTemp_PA,
                     samples = samples,
                     thin=thin,
                     transient = ceiling(0.5*samples*thin),
                     nChains = nChains,
                     nParallel = nP
                     )
saveRDS(m_spatTemp_noTemp_PA, file="m_spatTemp_noTemp_PA_sampled.rds")
print(paste("finish time is", Sys.time()))

###########################################


## 2.2 m_spatTemp_noTemp_abu
#### m_spatTemp_noTemp_abu.r ####

library(Hmsc)
print("starting model:")
print("m_spatTemp_noTemp_abu")
print(paste("start time is", Sys.time()))
load("m_spatTemp_noTemp_abu.rda")
thin = 20
samples = 1000
nChains = 8
nP = 2
m_spatTemp_noTemp_abu = sampleMcmc(m_spatTemp_noTemp_abu,
                     samples = samples,
                     thin=thin,
                     transient = ceiling(0.5*samples*thin),
                     nChains = nChains,
                     nParallel = nP
                     )
saveRDS(m_spatTemp_noTemp_abu, file="m_spatTemp_noTemp_abu_sampled.rds")
print(paste("finish time is", Sys.time()))

###########################################

## 3.1 m_spatTemp_Env_PA
#### m_spatTemp_Env_PA.r ####

library(Hmsc)
print("starting model:")
print("m_spatTemp_Env_PA")
print(paste("start time is", Sys.time()))
load("m_spatTemp_Env_PA.rda")
thin = 20
samples = 1000
nChains = 8
nP = 2
m_spatTemp_Env_PA = sampleMcmc(m_spatTemp_Env_PA,
                     samples = samples,
                     thin=thin,
                     transient = ceiling(0.5*samples*thin),
                     nChains = nChains,
                     nParallel = nP
                     )
saveRDS(m_spatTemp_Env_PA, file="m_spatTemp_Env_PA_sampled.rds")
print(paste("finish time is", Sys.time()))

###########################################

## 3.2 m_spatTemp_Env_abu
#### m_spatTemp_Env_abu.r ####

library(Hmsc)
print("starting model:")
print("m_spatTemp_Env_abu")
print(paste("start time is", Sys.time()))
load("m_spatTemp_Env_abu.rda")
thin = 20
samples = 1000
nChains = 8
nP = 2
m_spatTemp_Env_abu = sampleMcmc(m_spatTemp_Env_abu,
                     samples = samples,
                     thin=thin,
                     transient = ceiling(0.5*samples*thin),
                     nChains = nChains,
                     nParallel = nP
                     )
saveRDS(m_spatTemp_Env_abu, file="m_spatTemp_Env_abu_sampled.rds")
print(paste("finish time is", Sys.time()))

###########################################

## 1. m_spatTemp_ReadAbundancesOnly
nohup Rscript m_spatTemp_ReadAbundancesOnly.r &>> m_spatTemp_ReadAbundancesOnly.log &

## 2.1 m_spatTemp_noTemp_PA
nohup Rscript m_spatTemp_noTemp_PA.r &>> m_spatTemp_noTemp_PA.log &

## 2.2 m_spatTemp_noTemp_abu
nohup Rscript m_spatTemp_noTemp_abu.r &>> m_spatTemp_noTemp_abu.log &

## 3.1 m_spatTemp_Env_PA
nohup Rscript m_spatTemp_Env_PA.r &>> m_spatTemp_Env_PA.log &

## 3.2 m_spatTemp_Env_abu
nohup Rscript m_spatTemp_Env_abu.r &>> m_spatTemp_Env_abu.log &

## as before, the spatTemp_ReadAbu only model is failing. What is it about this model?
## try it interactively. 
## but worked when rerun. Might be simply that there weren't enough processors with all
## models sampling at the same time.

## given what we saw before, and the fact that the sampling was so quick,
## maybe rerun these, and combine the results

## processors when the other models were started?
## it's very quick (~1/2 hour), so rerun for deeper sampling alone, then run the others
## again together for deeper sampling after this finishes

## and done. 

## how do we combine these? For instance:

soil; conda activate spatialDirt; R

library(Hmsc)

## 1.  m_spatTemp_ReadAbundancesOnly
## actually, this model we are rerunning with 8 chains, all at once
## probably no need for deeper sampling and combining. Onto the others

## 2.1 m_spatTemp_noTemp_PA
load("m_spatTemp_noTemp_PA_sampled.rda")
m_spatTemp_noTemp_PA1 <- m_spatTemp_noTemp_PA 
rm(m_spatTemp_noTemp_PA)
load("m_spatTemp_noTemp_PA_sampled2.rda")
m_spatTemp_noTemp_PA2 <- m_spatTemp_noTemp_PA 
rm(m_spatTemp_noTemp_PA)
m_spatTemp_noTemp_PA_C <- c(m_spatTemp_noTemp_PA1, m_spatTemp_noTemp_PA2)
save(m_spatTemp_noTemp_PA_C, file="m_spatTemp_noTemp_PA_sampled_C.rda")

## 2.2 m_spatTemp_noTemp_abu
load("m_spatTemp_noTemp_abu_sampled.rda")
m_spatTemp_noTemp_abu1 <- m_spatTemp_noTemp_abu 
rm(m_spatTemp_noTemp_abu)
load("m_spatTemp_noTemp_abu_sampled2.rda")
m_spatTemp_noTemp_abu2 <- m_spatTemp_noTemp_abu 
rm(m_spatTemp_noTemp_abu)
m_spatTemp_noTemp_abu_C <- c(m_spatTemp_noTemp_abu1, m_spatTemp_noTemp_abu2)
save(m_spatTemp_noTemp_abu_C, file="m_spatTemp_noTemp_abu_sampled_C.rda")

## 3.1 m_spatTemp_Env_PA
load("m_spatTemp_Env_PA_sampled.rda")
m_spatTemp_Env_PA1 <- m_spatTemp_Env_PA 
rm(m_spatTemp_Env_PA)
load("m_spatTemp_Env_PA_sampled2.rda")
m_spatTemp_Env_PA2 <- m_spatTemp_Env_PA 
rm(m_spatTemp_Env_PA)
m_spatTemp_Env_PA_C <- c(m_spatTemp_Env_PA1, m_spatTemp_Env_PA2)
save(m_spatTemp_Env_PA_C, file="m_spatTemp_Env_PA_sampled_C.rda")

## 3.2 m_spatTemp_Env_abu
load("m_spatTemp_Env_abu_sampled.rda")
m_spatTemp_Env_abu1 <- m_spatTemp_Env_abu 
rm(m_spatTemp_Env_abu)
load("m_spatTemp_Env_abu_sampled2.rda")
m_spatTemp_Env_abu2 <- m_spatTemp_Env_abu 
rm(m_spatTemp_Env_abu)
m_spatTemp_Env_abu_C <- c(m_spatTemp_Env_abu1, m_spatTemp_Env_abu2)
save(m_spatTemp_Env_abu_C, file="m_spatTemp_Env_abu_sampled_C.rda")

## check these:

load("m_spatTemp_ReadAbundancesOnly_sampled.rda")
load("m_spatTemp_noTemp_PA_sampled_C.rda")
load("m_spatTemp_noTemp_abu_sampled_C.rda")
load("m_spatTemp_Env_PA_sampled_C.rda")
load("m_spatTemp_Env_abu_sampled_C.rda")

## tired of all the issues caused by having to use
## the original names of these data objects.
## let's resave as rds files:

saveRDS(m_spatTemp_ReadAbundancesOnly, file="m_spatTemp_ReadAbundancesOnly_sampled.rds")
saveRDS(m_spatTemp_noTemp_PA_C, file="m_spatTemp_noTemp_PA_sampled_C.rds")
saveRDS(m_spatTemp_noTemp_abu_C, file="m_spatTemp_noTemp_abu_sampled_C.rds")
saveRDS(m_spatTemp_Env_PA_C, file="m_spatTemp_Env_PA_sampled_C.rds")
saveRDS(m_spatTemp_Env_abu_C, file="m_spatTemp_Env_abu_sampled_C.rds")

m_spatTemp_ReadAbundancesOnly <- readRDS(file="m_spatTemp_ReadAbundancesOnly_sampled.rds")
m_spatTemp_noTemp_PA_C        <- readRDS(file="m_spatTemp_noTemp_PA_sampled_C.rds")
m_spatTemp_noTemp_abu_C       <- readRDS(file="m_spatTemp_noTemp_abu_sampled_C.rds")
m_spatTemp_Env_PA_C           <- readRDS(file="m_spatTemp_Env_PA_sampled_C.rds")
m_spatTemp_Env_abu_C          <- readRDS(file="m_spatTemp_Env_abu_sampled_C.rds")

m_spatTemp_ReadAbundancesOnly
m_spatTemp_noTemp_PA_C       
m_spatTemp_noTemp_abu_C      
m_spatTemp_Env_PA_C          
m_spatTemp_Env_abu_C         

## in shell, to clean up:
rm "m_spatTemp_ReadAbundancesOnly_sampled.rda"
rm "m_spatTemp_noTemp_PA_sampled_C.rda"
rm "m_spatTemp_noTemp_abu_sampled_C.rda"
rm "m_spatTemp_Env_PA_sampled_C.rda"
rm "m_spatTemp_Env_abu_sampled_C.rda"

## and did that out of order, deleted everything. Great.
## rerunning models, with 8 chains, saving as rds. Code modified above.

## now to load these:

soil; conda activate spatialDirt; R

library(Hmsc)

m_spatTemp_ReadAbundancesOnly <- readRDS("m_spatTemp_ReadAbundancesOnly_sampled.rds")
m_spatTemp_noTemp_PA <- readRDS("m_spatTemp_noTemp_PA_sampled.rds")
m_spatTemp_noTemp_abu <- readRDS("m_spatTemp_noTemp_abu_sampled.rds")
m_spatTemp_Env_PA <- readRDS("m_spatTemp_Env_PA_sampled.rds")
m_spatTemp_Env_abu <- readRDS("m_spatTemp_Env_abu_sampled.rds")

m_spatTemp_ReadAbundancesOnly
m_spatTemp_noTemp_PA
m_spatTemp_noTemp_abu
m_spatTemp_Env_PA
m_spatTemp_Env_abu

## Okay, why do we have all these models again? Head hurts.

## check mixing:

m_spatTemp_Env_PA_coda = convertToCodaObject(m_spatTemp_Env_PA)

m_spatTemp_Env_PA.beta = effectiveSize(m_spatTemp_Env_PA_coda$Beta)

## what happens when we try to plot the mixing plots directly, 
## (prepare for disaster...)

plot(m_spatTemp_Env_PA_coda$Beta)

hist(m_spatTemp_Env_PA.beta, main="effective sample size for beta", breaks=30)
mean(m_spatTemp_Env_PA.beta) ## mean = 4000, okay, that is deep

pdf(file="m_spatTemp_Env_PA_beta_mixing.pdf")
plot(m_spatTemp_Env_PA_coda$Beta)
dev.off() ## get local etc

## get it local
getFile="/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis/m_spatTemp_Env_PA_beta_mixing.pdf"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git"
scp test@132.180.112.115:$getFile $putItHere

## and repeat for the species-species interactions:
m_spatTemp_Env_PA_coda$temp = m_spatTemp_Env_PA_coda$Omega[[1]]
for(i in 1:length(m_spatTemp_Env_PA_coda$temp)){
  m_spatTemp_Env_PA_coda$temp[[i]] = m_spatTemp_Env_PA_coda$temp[[i]][,1:1000]
}

es.omega1 = effectiveSize(m_spatTemp_Env_PA_coda$temp)
ge.omega1 = gelman.diag(m_spatTemp_Env_PA_coda$temp,multivariate=FALSE)$psrf

#png(file="essOmega1_SpTmpSmall_thin150.png")
par(mfrow=c(1,2))
hist(es.omega1, main="ess(omega)")
hist(ge.omega1, main="psrf(omega)")
#dev.off()
mean(es.omega1) ## 3335, pretty good.

pdf(file="m_spatTemp_Env_PA_Omega_mixing.pdf")
plot(m_spatTemp_Env_PA_coda$temp)
dev.off() ## get local etc

## get it local
getFile="/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis/m_spatTemp_Env_PA_Omega_mixing.pdf"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git"
scp test@132.180.112.115:$getFile $putItHere

## okay, the beta and the omega are well sampling on the Presence/Absence of the hurdle model. 
## how about the abundance-based model that is conditioned on the PA?:
  
#m_spatTemp_Env_abu
m_spatTemp_Env_abu_coda = convertToCodaObject(m_spatTemp_Env_abu)
m_spatTemp_Env_abu.beta = effectiveSize(m_spatTemp_Env_abu_coda$Beta)

hist(m_spatTemp_Env_abu.beta, main="effective sample size for beta", breaks=30) ## wow, 
mean(m_spatTemp_Env_abu.beta) ## mean = 7177, okay, that is really deep


m_spatTemp_Env_abu_coda$temp = m_spatTemp_Env_abu_coda$Omega[[1]]
for(i in 1:length(m_spatTemp_Env_abu_coda$temp)){
  m_spatTemp_Env_abu_coda$temp[[i]] = m_spatTemp_Env_abu_coda$temp[[i]][,1:1000]
}

es.omega1 = effectiveSize(m_spatTemp_Env_abu_coda$temp)
ge.omega1 = gelman.diag(m_spatTemp_Env_abu_coda$temp,multivariate=FALSE)$psrf

par(mfrow=c(1,2))
hist(es.omega1, main="ess(omega)")
hist(ge.omega1, main="psrf(omega)")
#dev.off()
mean(es.omega1) ## 3054, pretty good.

## check the mixing
pdf(file="m_spatTemp_Env_abu_beta_mixing.pdf")
plot(m_spatTemp_Env_abu_coda$Beta)
dev.off() ## get local etc

pdf(file="m_spatTemp_Env_abu_Omega_mixing.pdf")
plot(m_spatTemp_Env_abu_coda$temp)
dev.off() ## get local etc

summary(m_spatTemp_Env_abu_coda$Beta)

## get them local
getFile="/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis/m_spatTemp_Env_abu_beta_mixing.pdf"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git"
scp test@132.180.112.115:$getFile $putItHere

getFile="/media/vol1/daniel/sulariArne/soilAnalysis/fichtelgebirgeSoils/spatialAnalysis/m_spatTemp_Env_abu_Omega_mixing.pdf"
putItHere="/home/daniel/Documents/projects/fichtelSoils/files2big4git"
scp test@132.180.112.115:$getFile $putItHere

### compare variance explained

## let's go ahead and set up the code for comparisons of R2 from the 
## various models, and restart the notebook showing this.

m_spatTemp_ReadAbundancesOnly
m_spatTemp_noTemp_PA
m_spatTemp_noTemp_abu
m_spatTemp_Env_PA
m_spatTemp_Env_abu

## explanatory

preds = computePredictedValues(m_spat_noEnv)
MF = evaluateModelFit(hM=m_spat_noEnv, predY=preds)
postBeta = getPostEstimate(m_spat_noEnv, parName="Beta")
hist(MF$RMSE)
hist(MF$C.RMSE)

## to get one RMSE value:

preds = computePredictedValues(m_spatTemp_ReadAbundancesOnly)
m_spatTemp_ReadAbundancesOnly_MF = evaluateModelFit(hM=m_spatTemp_ReadAbundancesOnly, predY=preds)

m_spatTemp_ReadAbundancesOnly_MF$RMSE



str(m_spatTemp_ReadAbundancesOnly)

models = list(
m_spatTemp_ReadAbundancesOnly,
m_spatTemp_noTemp_PA,
m_spatTemp_noTemp_abu,
m_spatTemp_Env_PA,
m_spatTemp_Env_abu
)


predlist <- list()
modelFitList <- list()
for (i in 1:length(models)) { 
  print(models[[i]])
  preds = computePredictedValues(models[[i]])
  MF = evaluateModelFit(hM=models[[i]], predY=preds)
  predlist[[i]] <- preds
  modelFitList[[i]] <- MF
}
modelNames=c("m_spatTemp_ReadAbundancesOnly", "m_spatTemp_noTemp_PA", "m_spatTemp_noTemp_abu", "m_spatTemp_Env_PA", "m_spatTemp_Env_abu")
names(modelFitList) <- modelNames

modelFitList$m_spatTemp_ReadAbundancesOnly

## we want a dataframe for each. For instance, for RMSE across all models:

?evaluateModelFit

## so this data is now available:
print("m_spatTemp_ReadAbundancesOnly")
str(modelFitList[[1]])

print("m_spatTemp_noTemp_PA")
str(modelFitList[[2]])

print("m_spatTemp_noTemp_abu")
str(modelFitList[[3]])

print("m_spatTemp_Env_PA")
str(modelFitList[[4]])

print("m_spatTemp_Env_abu")
str(modelFitList[[5]])


nSpecies <- length(modelFitList[[1]][[1]])

RMSEmat <- matrix(ncol=length(modelFitList), nrow=nSpecies)
for (i in 1:length(modelFitList)){
  RMSEmat[,i] <- modelFitList[[i]]$RMSE
}

modelNames=c("m_spatTemp_ReadAbundancesOnly", "m_spatTemp_noTemp_PA", "m_spatTemp_noTemp_abu", "m_spatTemp_Env_PA", "m_spatTemp_Env_abu")
RMSEdf <- as.data.frame(RMSEmat)
colnames(RMSEdf) = modelNames

head(RMSEdf)

## to plot it:

boxplot(RMSEdf)

## but the RMSE isn't really comparable across the models, we y-scaled our yAbu and yPA, but 
## not the raw Y values that the read abundance model is based on 
## matrices for the hurdle models.

## maybe best here to compare the PA models of the hurdles, see how 
## incorporating time (season) changed the explanatory power of the 
## model, if at all.

## we can include the read-abundance Poisson "observation" fits, because
## this is conveniently the fit of this model to PA-transformed
## y data, done automatically 

## in the case of the PA
nSpecies <- length(modelFitList$m_spatTemp_noTemp_PA[[1]])
RMSEmat <- matrix(ncol=2, nrow=nSpecies)
RMSEmat[,1] <- modelFitList$m_spatTemp_noTemp_PA$RMSE
RMSEmat[,2] <- modelFitList$m_spatTemp_Env_PA$RMSE
RMSEdf <- as.data.frame(RMSEmat)
colnames(RMSEdf)=c("m_spatTemp_noTemp_PA", "m_spatTemp_Env_PA")
boxplot(RMSEdf, main=("mean RMSE of P/A component of Hurdles, without and with season") )

## doesn't look different. Maybe a slight improvment by 
## considering season.

## other measures of model fit available for the PA are AUC and TjurR2

## can we plot these together for comparison?
nSpecies <- length(modelFitList$m_spatTemp_noTemp_PA[[1]])
paGFmat <- matrix(ncol=6, nrow=nSpecies)
paGFmat[,1] <- modelFitList$m_spatTemp_ReadAbundancesOnly$O.AUC
paGFmat[,3] <- modelFitList$m_spatTemp_noTemp_PA$AUC
paGFmat[,2] <- modelFitList$m_spatTemp_Env_PA$AUC
paGFmat[,4] <- modelFitList$m_spatTemp_ReadAbundancesOnly$O.TjurR2
paGFmat[,5] <- modelFitList$m_spatTemp_noTemp_PA$TjurR2
paGFmat[,6] <- modelFitList$m_spatTemp_Env_PA$TjurR2
paGFdf <- as.data.frame(paGFmat)
colnames(paGFdf) <- c("readAbundanceOnlyAUC","noTempAUC","spTempAUC","readAbundanceOnlyTjurR2","noTempTjurR2","spTempTjurR2")

par(cex.axis = .75, mar=c(10,4,4,2))
boxplot(paGFdf, 
main=("model fit measures of P/A component of Hurdles,\n without and with season"),
names=c(
  "ReadAbundances AUC",
  "noSeason AUC",
  "fullModel AUC",
  "ReadAbundances TjurR2",
  "noSeason TjurR2",
  "fullModel TjurR2"
),
las=2
)

## following the Abrego Ovaskainen 2023 paper, the TjurR2 is best for 
## judging the model's "positive" predictive power: how often does
## the model properly predict when a species will be present.

## on the other hand, the AUC is perhaps more useful for for 
## discovering when a species will not be present, more the 
## ability for discovering unsuitable sites. 
 
## quite complicated. Let's look at the abundance models, using 

nSpecies <- length(modelFitList$m_spatTemp_noTemp_PA[[1]])
paGFmat <- matrix(ncol=3, nrow=nSpecies)
paGFmat[,1] <- modelFitList$m_spatTemp_ReadAbundancesOnly$C.SR2
paGFmat[,3] <- modelFitList$m_spatTemp_noTemp_abu$R2
paGFmat[,2] <- modelFitList$m_spatTemp_Env_abu$R2
paGFdf <- as.data.frame(paGFmat)
colnames(paGFdf) <- c("readAbundanceOnlyR2","noTempAbuR2","spTempAbuR2")

par(cex.axis = .75, mar=c(10,4,4,2))
boxplot(paGFdf, 
main=("model fit measures of P/A component of Hurdles,\n without and with season"),
names=c(
  "ReadAbundances R2",
  "noSeason R2",
  "fullModel R2"
),
las=2
)

## let's get the cross validations of these models going,
## then update the notebook

## cross-validation:

m_spatTemp_ReadAbundancesOnly
m_spatTemp_noTemp_PA
m_spatTemp_noTemp_abu
m_spatTemp_Env_PA
m_spatTemp_Env_abu

## we need scripts for each, because parallelizing.

conda activate spatialDirt

################ getCrossValFit.r ###################

library(Hmsc)
modelfile = commandArgs(trailingOnly=TRUE)[1]
print(paste("starting",modelfile))
m = readRDS(modelfile)
partition = createPartition(m, nfolds = 2, column = "sample")
preds = computePredictedValues(m, partition=partition, nParallel = 2)
MF = evaluateModelFit(hM=m, predY=preds)
saveRDS(preds, file=sub("_sampled.rds", "_crossValPreds.rds",modelfile))
saveRDS(MF, file=sub("_sampled.rds", "_crossValModelFit.rds",modelfile))
print(paste("finish time is", Sys.time()))

#####################################################

## we need to get this going, because this will take forever
nohup Rscript getCrossValFit.r "m_spatTemp_ReadAbundancesOnly_sampled.rds"  &> m_spatTemp_ReadAbundancesOnly_crossval.log &
nohup Rscript getCrossValFit.r "m_spatTemp_noTemp_PA_sampled.rds" &> m_spatTemp_noTemp_PA_crossval.log &
nohup Rscript getCrossValFit.r "m_spatTemp_noTemp_abu_sampled.rds" &> m_spatTemp_noTemp_abu_crossval.log &
nohup Rscript getCrossValFit.r "m_spatTemp_Env_abu_sampled.rds" &> m_spatTemp_Env_abu_crossval.log &
nohup Rscript getCrossValFit.r "m_spatTemp_Env_PA_sampled.rds" &> m_spatTemp_Env_PA_crossval.log &

## these were all started July17, 11:20am

## okay, hope that runs well

## time to update the notebook, if for no other reason to remember 
## what the hell we are doing.

## from local computer:
## run this to activate the tcp forwarding

ssh -L 8080:localhost:8080 test@132.180.112.115

## using this, on nanocomp
conda activate spatialDirt
jupyter notebook --no-browser --port=8080 spatialAnalysisSulariData.ipynb

## then open browser to:
http://localhost:8080/notebooks/spatialAnalysisSulariData.ipynb
 
## oops get that notebook back to the latest commit 
git checkout dea37cba0f1f22b51f10fdcea46a3e62298d8395 spatialAnalysisSulariData.ipynb

